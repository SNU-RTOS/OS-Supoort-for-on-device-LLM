=== /home/rtos/workspace/ghpark/export/llama-3.2-3b-it/llama_f32_ekv1280.tflite ===

Your TFLite model has '57' subgraph(s). In the subgraph description below,
T# represents the Tensor numbers. For example, in Subgraph#0, the RESHAPE op takes
tensor #39 and tensor #276 as input and produces tensor #284 as output.

Subgraph#0 decode(T#0, T#1, T#2, T#3, T#4, T#5, T#6, T#7, T#8, T#9, T#10, T#11, T#12, T#13, T#14, T#15, T#16, T#17, T#18, T#19, T#20, T#21, T#22, T#23, T#24, T#25, T#26, T#27, T#28, T#29, T#30, T#31, T#32, T#33, T#34, T#35, T#36, T#37, T#38, T#39, T#40, T#41, T#42, T#43, T#44, T#45, T#46, T#47, T#48, T#49, T#50, T#51, T#52, T#53, T#54, T#55, T#56, T#57) -> [T#1745, T#503, T#448, T#1204, T#772, T#1691, T#1529, T#880, T#1636, T#1825, T#1258, T#665, T#1799, T#1528, T#556, T#1420, T#826, T#1583, T#502, T#341, T#988, T#1313, T#935, T#611, T#664, T#1312, T#1744, T#1367, T#718, T#881, T#1205, T#989, T#1582, T#1096, T#1151, T#1042, T#719, T#1475, T#395, T#1259, T#1043, T#340, T#1474, T#1637, T#1150, T#827, T#449, T#394, T#773, T#1690, T#557, T#934, T#1366, T#610, T#1421, T#1798, T#1097]
  Op#0 RESHAPE(T#39, T#276) -> [T#284]
  Op#1 EMBEDDING_LOOKUP(T#284, T#283) -> [T#285]
  Op#2 RESHAPE(T#285, T#274) -> [T#286]
  Op#3 CAST(T#5) -> [T#287]
  Op#4 RESHAPE(T#287, T#273) -> [T#288]
  Op#5 MUL(T#288, T#272) -> [T#289]
  Op#6 COS(T#289) -> [T#290]
  Op#7 SIN(T#289) -> [T#291]
  Op#8 LESS(T#5, T#277) -> [T#292]
  Op#9 ADD(T#5, T#278) -> [T#293]
  Op#10 SELECT(T#292, T#293, T#5) -> [T#294]
  Op#11 RESHAPE(T#294, T#273) -> [T#295]
  Op#12 CAST(T#295) -> [T#296]
  Op#13 GREATER_EQUAL(T#296, T#279) -> [T#297]
  Op#14 LESS_EQUAL(T#296, T#280) -> [T#298]
  Op#15 LOGICAL_AND(T#297, T#298) -> [T#299]
  Op#16 REDUCE_ALL(T#299, T#276) -> [T#300]
  Op#17 GATHER_ND(T#264, T#296) -> [T#301]
  Op#18 RESHAPE(T#300, T#275) -> [T#302]
  Op#19 SELECT_V2(T#302, T#301, T#271) -> [T#303]
  Op#20 MUL(T#285, T#286) -> [T#304]
  Op#21 SUM(T#304, T#206) -> [T#305]
  Op#22 MUL(T#305, T#205) -> [T#306]
  Op#23 ADD(T#306, T#281) -> [T#307]
  Op#24 RSQRT(T#307) -> [T#308]
  Op#25 MUL(T#285, T#308) -> [T#309]
  Op#26 MUL(T#309, T#263) -> [T#310]
  Op#27 FULLY_CONNECTED(T#310, T#197, T#-1) -> [T#311]
  Op#28 RESHAPE(T#311, T#270) -> [T#312]
  Op#29 SLICE(T#312, T#204, T#203) -> [T#313]
  Op#30 SLICE(T#312, T#202, T#266) -> [T#314]
  Op#31 SLICE(T#312, T#201, T#266) -> [T#315]
  Op#32 RESHAPE(T#313, T#269) -> [T#316]
  Op#33 SLICE(T#316, T#204, T#200) -> [T#317]
  Op#34 SLICE(T#316, T#199, T#200) -> [T#318]
  Op#35 MUL(T#317, T#290) -> [T#319]
  Op#36 MUL(T#318, T#291) -> [T#320]
  Op#37 SUB(T#319, T#320) -> [T#321]
  Op#38 MUL(T#318, T#290) -> [T#322]
  Op#39 MUL(T#317, T#291) -> [T#323]
  Op#40 ADD(T#322, T#323) -> [T#324]
  Op#41 CONCATENATION(T#321, T#324) -> [T#325]
  Op#42 RESHAPE(T#325, T#268) -> [T#326]
  Op#43 RESHAPE(T#314, T#267) -> [T#327]
  Op#44 SLICE(T#327, T#204, T#198) -> [T#328]
  Op#45 SLICE(T#327, T#199, T#198) -> [T#329]
  Op#46 MUL(T#328, T#290) -> [T#330]
  Op#47 MUL(T#329, T#291) -> [T#331]
  Op#48 SUB(T#330, T#331) -> [T#332]
  Op#49 MUL(T#329, T#290) -> [T#333]
  Op#50 MUL(T#328, T#291) -> [T#334]
  Op#51 ADD(T#333, T#334) -> [T#335]
  Op#52 CONCATENATION(T#332, T#335) -> [T#336]
  Op#53 RESHAPE(T#336, T#266) -> [T#337]
  Op#54 RESHAPE(T#5, T#265) -> [T#338]
  Op#55 PACK(T#282, T#338, T#282, T#282) -> [T#339]
  Op#56 DYNAMIC_UPDATE_SLICE(T#42, T#337, T#339) -> [T#340]
  Op#57 DYNAMIC_UPDATE_SLICE(T#19, T#315, T#339) -> [T#341]
  Op#58 STABLEHLO_COMPOSITE(T#326, T#340, T#341, T#303) -> [T#342]
  Op#59 RESHAPE(T#342, T#274) -> [T#343]
  Op#60 FULLY_CONNECTED(T#343, T#196, T#-1) -> [T#344]
  Op#61 ADD(T#285, T#344) -> [T#345]
  Op#62 MUL(T#345, T#345) -> [T#346]
  Op#63 SUM(T#346, T#206) -> [T#347]
  Op#64 MUL(T#347, T#205) -> [T#348]
  Op#65 ADD(T#348, T#281) -> [T#349]
  Op#66 RSQRT(T#349) -> [T#350]
  Op#67 MUL(T#345, T#350) -> [T#351]
  Op#68 MUL(T#351, T#262) -> [T#352]
  Op#69 FULLY_CONNECTED(T#352, T#195, T#-1) -> [T#353]
  Op#70 LOGISTIC(T#353) -> [T#354]
  Op#71 MUL(T#353, T#354) -> [T#355]
  Op#72 FULLY_CONNECTED(T#352, T#194, T#-1) -> [T#356]
  Op#73 MUL(T#355, T#356) -> [T#357]
  Op#74 FULLY_CONNECTED(T#357, T#193, T#-1) -> [T#358]
  Op#75 ADD(T#345, T#358) -> [T#359]
  Op#76 MUL(T#359, T#359) -> [T#360]
  Op#77 SUM(T#360, T#206) -> [T#361]
  Op#78 MUL(T#361, T#205) -> [T#362]
  Op#79 ADD(T#362, T#281) -> [T#363]
  Op#80 RSQRT(T#363) -> [T#364]
  Op#81 MUL(T#359, T#364) -> [T#365]
  Op#82 MUL(T#365, T#261) -> [T#366]
  Op#83 FULLY_CONNECTED(T#366, T#192, T#-1) -> [T#367]
  Op#84 RESHAPE(T#367, T#270) -> [T#368]
  Op#85 SLICE(T#368, T#204, T#203) -> [T#369]
  Op#86 SLICE(T#368, T#202, T#266) -> [T#370]
  Op#87 SLICE(T#368, T#201, T#266) -> [T#371]
  Op#88 RESHAPE(T#369, T#269) -> [T#372]
  Op#89 SLICE(T#372, T#204, T#200) -> [T#373]
  Op#90 SLICE(T#372, T#199, T#200) -> [T#374]
  Op#91 MUL(T#373, T#290) -> [T#375]
  Op#92 MUL(T#374, T#291) -> [T#376]
  Op#93 SUB(T#375, T#376) -> [T#377]
  Op#94 MUL(T#374, T#290) -> [T#378]
  Op#95 MUL(T#373, T#291) -> [T#379]
  Op#96 ADD(T#378, T#379) -> [T#380]
  Op#97 CONCATENATION(T#377, T#380) -> [T#381]
  Op#98 RESHAPE(T#381, T#268) -> [T#382]
  Op#99 RESHAPE(T#370, T#267) -> [T#383]
  Op#100 SLICE(T#383, T#204, T#198) -> [T#384]
  Op#101 SLICE(T#383, T#199, T#198) -> [T#385]
  Op#102 MUL(T#384, T#290) -> [T#386]
  Op#103 MUL(T#385, T#291) -> [T#387]
  Op#104 SUB(T#386, T#387) -> [T#388]
  Op#105 MUL(T#385, T#290) -> [T#389]
  Op#106 MUL(T#384, T#291) -> [T#390]
  Op#107 ADD(T#389, T#390) -> [T#391]
  Op#108 CONCATENATION(T#388, T#391) -> [T#392]
  Op#109 RESHAPE(T#392, T#266) -> [T#393]
  Op#110 DYNAMIC_UPDATE_SLICE(T#48, T#393, T#339) -> [T#394]
  Op#111 DYNAMIC_UPDATE_SLICE(T#38, T#371, T#339) -> [T#395]
  Op#112 STABLEHLO_COMPOSITE(T#382, T#394, T#395, T#303) -> [T#396]
  Op#113 RESHAPE(T#396, T#274) -> [T#397]
  Op#114 FULLY_CONNECTED(T#397, T#191, T#-1) -> [T#398]
  Op#115 ADD(T#359, T#398) -> [T#399]
  Op#116 MUL(T#399, T#399) -> [T#400]
  Op#117 SUM(T#400, T#206) -> [T#401]
  Op#118 MUL(T#401, T#205) -> [T#402]
  Op#119 ADD(T#402, T#281) -> [T#403]
  Op#120 RSQRT(T#403) -> [T#404]
  Op#121 MUL(T#399, T#404) -> [T#405]
  Op#122 MUL(T#405, T#260) -> [T#406]
  Op#123 FULLY_CONNECTED(T#406, T#190, T#-1) -> [T#407]
  Op#124 LOGISTIC(T#407) -> [T#408]
  Op#125 MUL(T#407, T#408) -> [T#409]
  Op#126 FULLY_CONNECTED(T#406, T#189, T#-1) -> [T#410]
  Op#127 MUL(T#409, T#410) -> [T#411]
  Op#128 FULLY_CONNECTED(T#411, T#188, T#-1) -> [T#412]
  Op#129 ADD(T#399, T#412) -> [T#413]
  Op#130 MUL(T#413, T#413) -> [T#414]
  Op#131 SUM(T#414, T#206) -> [T#415]
  Op#132 MUL(T#415, T#205) -> [T#416]
  Op#133 ADD(T#416, T#281) -> [T#417]
  Op#134 RSQRT(T#417) -> [T#418]
  Op#135 MUL(T#413, T#418) -> [T#419]
  Op#136 MUL(T#419, T#259) -> [T#420]
  Op#137 FULLY_CONNECTED(T#420, T#187, T#-1) -> [T#421]
  Op#138 RESHAPE(T#421, T#270) -> [T#422]
  Op#139 SLICE(T#422, T#204, T#203) -> [T#423]
  Op#140 SLICE(T#422, T#202, T#266) -> [T#424]
  Op#141 SLICE(T#422, T#201, T#266) -> [T#425]
  Op#142 RESHAPE(T#423, T#269) -> [T#426]
  Op#143 SLICE(T#426, T#204, T#200) -> [T#427]
  Op#144 SLICE(T#426, T#199, T#200) -> [T#428]
  Op#145 MUL(T#427, T#290) -> [T#429]
  Op#146 MUL(T#428, T#291) -> [T#430]
  Op#147 SUB(T#429, T#430) -> [T#431]
  Op#148 MUL(T#428, T#290) -> [T#432]
  Op#149 MUL(T#427, T#291) -> [T#433]
  Op#150 ADD(T#432, T#433) -> [T#434]
  Op#151 CONCATENATION(T#431, T#434) -> [T#435]
  Op#152 RESHAPE(T#435, T#268) -> [T#436]
  Op#153 RESHAPE(T#424, T#267) -> [T#437]
  Op#154 SLICE(T#437, T#204, T#198) -> [T#438]
  Op#155 SLICE(T#437, T#199, T#198) -> [T#439]
  Op#156 MUL(T#438, T#290) -> [T#440]
  Op#157 MUL(T#439, T#291) -> [T#441]
  Op#158 SUB(T#440, T#441) -> [T#442]
  Op#159 MUL(T#439, T#290) -> [T#443]
  Op#160 MUL(T#438, T#291) -> [T#444]
  Op#161 ADD(T#443, T#444) -> [T#445]
  Op#162 CONCATENATION(T#442, T#445) -> [T#446]
  Op#163 RESHAPE(T#446, T#266) -> [T#447]
  Op#164 DYNAMIC_UPDATE_SLICE(T#2, T#447, T#339) -> [T#448]
  Op#165 DYNAMIC_UPDATE_SLICE(T#47, T#425, T#339) -> [T#449]
  Op#166 STABLEHLO_COMPOSITE(T#436, T#448, T#449, T#303) -> [T#450]
  Op#167 RESHAPE(T#450, T#274) -> [T#451]
  Op#168 FULLY_CONNECTED(T#451, T#186, T#-1) -> [T#452]
  Op#169 ADD(T#413, T#452) -> [T#453]
  Op#170 MUL(T#453, T#453) -> [T#454]
  Op#171 SUM(T#454, T#206) -> [T#455]
  Op#172 MUL(T#455, T#205) -> [T#456]
  Op#173 ADD(T#456, T#281) -> [T#457]
  Op#174 RSQRT(T#457) -> [T#458]
  Op#175 MUL(T#453, T#458) -> [T#459]
  Op#176 MUL(T#459, T#258) -> [T#460]
  Op#177 FULLY_CONNECTED(T#460, T#185, T#-1) -> [T#461]
  Op#178 LOGISTIC(T#461) -> [T#462]
  Op#179 MUL(T#461, T#462) -> [T#463]
  Op#180 FULLY_CONNECTED(T#460, T#184, T#-1) -> [T#464]
  Op#181 MUL(T#463, T#464) -> [T#465]
  Op#182 FULLY_CONNECTED(T#465, T#183, T#-1) -> [T#466]
  Op#183 ADD(T#453, T#466) -> [T#467]
  Op#184 MUL(T#467, T#467) -> [T#468]
  Op#185 SUM(T#468, T#206) -> [T#469]
  Op#186 MUL(T#469, T#205) -> [T#470]
  Op#187 ADD(T#470, T#281) -> [T#471]
  Op#188 RSQRT(T#471) -> [T#472]
  Op#189 MUL(T#467, T#472) -> [T#473]
  Op#190 MUL(T#473, T#257) -> [T#474]
  Op#191 FULLY_CONNECTED(T#474, T#182, T#-1) -> [T#475]
  Op#192 RESHAPE(T#475, T#270) -> [T#476]
  Op#193 SLICE(T#476, T#204, T#203) -> [T#477]
  Op#194 SLICE(T#476, T#202, T#266) -> [T#478]
  Op#195 SLICE(T#476, T#201, T#266) -> [T#479]
  Op#196 RESHAPE(T#477, T#269) -> [T#480]
  Op#197 SLICE(T#480, T#204, T#200) -> [T#481]
  Op#198 SLICE(T#480, T#199, T#200) -> [T#482]
  Op#199 MUL(T#481, T#290) -> [T#483]
  Op#200 MUL(T#482, T#291) -> [T#484]
  Op#201 SUB(T#483, T#484) -> [T#485]
  Op#202 MUL(T#482, T#290) -> [T#486]
  Op#203 MUL(T#481, T#291) -> [T#487]
  Op#204 ADD(T#486, T#487) -> [T#488]
  Op#205 CONCATENATION(T#485, T#488) -> [T#489]
  Op#206 RESHAPE(T#489, T#268) -> [T#490]
  Op#207 RESHAPE(T#478, T#267) -> [T#491]
  Op#208 SLICE(T#491, T#204, T#198) -> [T#492]
  Op#209 SLICE(T#491, T#199, T#198) -> [T#493]
  Op#210 MUL(T#492, T#290) -> [T#494]
  Op#211 MUL(T#493, T#291) -> [T#495]
  Op#212 SUB(T#494, T#495) -> [T#496]
  Op#213 MUL(T#493, T#290) -> [T#497]
  Op#214 MUL(T#492, T#291) -> [T#498]
  Op#215 ADD(T#497, T#498) -> [T#499]
  Op#216 CONCATENATION(T#496, T#499) -> [T#500]
  Op#217 RESHAPE(T#500, T#266) -> [T#501]
  Op#218 DYNAMIC_UPDATE_SLICE(T#18, T#501, T#339) -> [T#502]
  Op#219 DYNAMIC_UPDATE_SLICE(T#1, T#479, T#339) -> [T#503]
  Op#220 STABLEHLO_COMPOSITE(T#490, T#502, T#503, T#303) -> [T#504]
  Op#221 RESHAPE(T#504, T#274) -> [T#505]
  Op#222 FULLY_CONNECTED(T#505, T#181, T#-1) -> [T#506]
  Op#223 ADD(T#467, T#506) -> [T#507]
  Op#224 MUL(T#507, T#507) -> [T#508]
  Op#225 SUM(T#508, T#206) -> [T#509]
  Op#226 MUL(T#509, T#205) -> [T#510]
  Op#227 ADD(T#510, T#281) -> [T#511]
  Op#228 RSQRT(T#511) -> [T#512]
  Op#229 MUL(T#507, T#512) -> [T#513]
  Op#230 MUL(T#513, T#256) -> [T#514]
  Op#231 FULLY_CONNECTED(T#514, T#180, T#-1) -> [T#515]
  Op#232 LOGISTIC(T#515) -> [T#516]
  Op#233 MUL(T#515, T#516) -> [T#517]
  Op#234 FULLY_CONNECTED(T#514, T#179, T#-1) -> [T#518]
  Op#235 MUL(T#517, T#518) -> [T#519]
  Op#236 FULLY_CONNECTED(T#519, T#178, T#-1) -> [T#520]
  Op#237 ADD(T#507, T#520) -> [T#521]
  Op#238 MUL(T#521, T#521) -> [T#522]
  Op#239 SUM(T#522, T#206) -> [T#523]
  Op#240 MUL(T#523, T#205) -> [T#524]
  Op#241 ADD(T#524, T#281) -> [T#525]
  Op#242 RSQRT(T#525) -> [T#526]
  Op#243 MUL(T#521, T#526) -> [T#527]
  Op#244 MUL(T#527, T#255) -> [T#528]
  Op#245 FULLY_CONNECTED(T#528, T#177, T#-1) -> [T#529]
  Op#246 RESHAPE(T#529, T#270) -> [T#530]
  Op#247 SLICE(T#530, T#204, T#203) -> [T#531]
  Op#248 SLICE(T#530, T#202, T#266) -> [T#532]
  Op#249 SLICE(T#530, T#201, T#266) -> [T#533]
  Op#250 RESHAPE(T#531, T#269) -> [T#534]
  Op#251 SLICE(T#534, T#204, T#200) -> [T#535]
  Op#252 SLICE(T#534, T#199, T#200) -> [T#536]
  Op#253 MUL(T#535, T#290) -> [T#537]
  Op#254 MUL(T#536, T#291) -> [T#538]
  Op#255 SUB(T#537, T#538) -> [T#539]
  Op#256 MUL(T#536, T#290) -> [T#540]
  Op#257 MUL(T#535, T#291) -> [T#541]
  Op#258 ADD(T#540, T#541) -> [T#542]
  Op#259 CONCATENATION(T#539, T#542) -> [T#543]
  Op#260 RESHAPE(T#543, T#268) -> [T#544]
  Op#261 RESHAPE(T#532, T#267) -> [T#545]
  Op#262 SLICE(T#545, T#204, T#198) -> [T#546]
  Op#263 SLICE(T#545, T#199, T#198) -> [T#547]
  Op#264 MUL(T#546, T#290) -> [T#548]
  Op#265 MUL(T#547, T#291) -> [T#549]
  Op#266 SUB(T#548, T#549) -> [T#550]
  Op#267 MUL(T#547, T#290) -> [T#551]
  Op#268 MUL(T#546, T#291) -> [T#552]
  Op#269 ADD(T#551, T#552) -> [T#553]
  Op#270 CONCATENATION(T#550, T#553) -> [T#554]
  Op#271 RESHAPE(T#554, T#266) -> [T#555]
  Op#272 DYNAMIC_UPDATE_SLICE(T#14, T#555, T#339) -> [T#556]
  Op#273 DYNAMIC_UPDATE_SLICE(T#51, T#533, T#339) -> [T#557]
  Op#274 STABLEHLO_COMPOSITE(T#544, T#556, T#557, T#303) -> [T#558]
  Op#275 RESHAPE(T#558, T#274) -> [T#559]
  Op#276 FULLY_CONNECTED(T#559, T#176, T#-1) -> [T#560]
  Op#277 ADD(T#521, T#560) -> [T#561]
  Op#278 MUL(T#561, T#561) -> [T#562]
  Op#279 SUM(T#562, T#206) -> [T#563]
  Op#280 MUL(T#563, T#205) -> [T#564]
  Op#281 ADD(T#564, T#281) -> [T#565]
  Op#282 RSQRT(T#565) -> [T#566]
  Op#283 MUL(T#561, T#566) -> [T#567]
  Op#284 MUL(T#567, T#254) -> [T#568]
  Op#285 FULLY_CONNECTED(T#568, T#175, T#-1) -> [T#569]
  Op#286 LOGISTIC(T#569) -> [T#570]
  Op#287 MUL(T#569, T#570) -> [T#571]
  Op#288 FULLY_CONNECTED(T#568, T#174, T#-1) -> [T#572]
  Op#289 MUL(T#571, T#572) -> [T#573]
  Op#290 FULLY_CONNECTED(T#573, T#173, T#-1) -> [T#574]
  Op#291 ADD(T#561, T#574) -> [T#575]
  Op#292 MUL(T#575, T#575) -> [T#576]
  Op#293 SUM(T#576, T#206) -> [T#577]
  Op#294 MUL(T#577, T#205) -> [T#578]
  Op#295 ADD(T#578, T#281) -> [T#579]
  Op#296 RSQRT(T#579) -> [T#580]
  Op#297 MUL(T#575, T#580) -> [T#581]
  Op#298 MUL(T#581, T#253) -> [T#582]
  Op#299 FULLY_CONNECTED(T#582, T#172, T#-1) -> [T#583]
  Op#300 RESHAPE(T#583, T#270) -> [T#584]
  Op#301 SLICE(T#584, T#204, T#203) -> [T#585]
  Op#302 SLICE(T#584, T#202, T#266) -> [T#586]
  Op#303 SLICE(T#584, T#201, T#266) -> [T#587]
  Op#304 RESHAPE(T#585, T#269) -> [T#588]
  Op#305 SLICE(T#588, T#204, T#200) -> [T#589]
  Op#306 SLICE(T#588, T#199, T#200) -> [T#590]
  Op#307 MUL(T#589, T#290) -> [T#591]
  Op#308 MUL(T#590, T#291) -> [T#592]
  Op#309 SUB(T#591, T#592) -> [T#593]
  Op#310 MUL(T#590, T#290) -> [T#594]
  Op#311 MUL(T#589, T#291) -> [T#595]
  Op#312 ADD(T#594, T#595) -> [T#596]
  Op#313 CONCATENATION(T#593, T#596) -> [T#597]
  Op#314 RESHAPE(T#597, T#268) -> [T#598]
  Op#315 RESHAPE(T#586, T#267) -> [T#599]
  Op#316 SLICE(T#599, T#204, T#198) -> [T#600]
  Op#317 SLICE(T#599, T#199, T#198) -> [T#601]
  Op#318 MUL(T#600, T#290) -> [T#602]
  Op#319 MUL(T#601, T#291) -> [T#603]
  Op#320 SUB(T#602, T#603) -> [T#604]
  Op#321 MUL(T#601, T#290) -> [T#605]
  Op#322 MUL(T#600, T#291) -> [T#606]
  Op#323 ADD(T#605, T#606) -> [T#607]
  Op#324 CONCATENATION(T#604, T#607) -> [T#608]
  Op#325 RESHAPE(T#608, T#266) -> [T#609]
  Op#326 DYNAMIC_UPDATE_SLICE(T#54, T#609, T#339) -> [T#610]
  Op#327 DYNAMIC_UPDATE_SLICE(T#23, T#587, T#339) -> [T#611]
  Op#328 STABLEHLO_COMPOSITE(T#598, T#610, T#611, T#303) -> [T#612]
  Op#329 RESHAPE(T#612, T#274) -> [T#613]
  Op#330 FULLY_CONNECTED(T#613, T#171, T#-1) -> [T#614]
  Op#331 ADD(T#575, T#614) -> [T#615]
  Op#332 MUL(T#615, T#615) -> [T#616]
  Op#333 SUM(T#616, T#206) -> [T#617]
  Op#334 MUL(T#617, T#205) -> [T#618]
  Op#335 ADD(T#618, T#281) -> [T#619]
  Op#336 RSQRT(T#619) -> [T#620]
  Op#337 MUL(T#615, T#620) -> [T#621]
  Op#338 MUL(T#621, T#252) -> [T#622]
  Op#339 FULLY_CONNECTED(T#622, T#170, T#-1) -> [T#623]
  Op#340 LOGISTIC(T#623) -> [T#624]
  Op#341 MUL(T#623, T#624) -> [T#625]
  Op#342 FULLY_CONNECTED(T#622, T#169, T#-1) -> [T#626]
  Op#343 MUL(T#625, T#626) -> [T#627]
  Op#344 FULLY_CONNECTED(T#627, T#168, T#-1) -> [T#628]
  Op#345 ADD(T#615, T#628) -> [T#629]
  Op#346 MUL(T#629, T#629) -> [T#630]
  Op#347 SUM(T#630, T#206) -> [T#631]
  Op#348 MUL(T#631, T#205) -> [T#632]
  Op#349 ADD(T#632, T#281) -> [T#633]
  Op#350 RSQRT(T#633) -> [T#634]
  Op#351 MUL(T#629, T#634) -> [T#635]
  Op#352 MUL(T#635, T#251) -> [T#636]
  Op#353 FULLY_CONNECTED(T#636, T#167, T#-1) -> [T#637]
  Op#354 RESHAPE(T#637, T#270) -> [T#638]
  Op#355 SLICE(T#638, T#204, T#203) -> [T#639]
  Op#356 SLICE(T#638, T#202, T#266) -> [T#640]
  Op#357 SLICE(T#638, T#201, T#266) -> [T#641]
  Op#358 RESHAPE(T#639, T#269) -> [T#642]
  Op#359 SLICE(T#642, T#204, T#200) -> [T#643]
  Op#360 SLICE(T#642, T#199, T#200) -> [T#644]
  Op#361 MUL(T#643, T#290) -> [T#645]
  Op#362 MUL(T#644, T#291) -> [T#646]
  Op#363 SUB(T#645, T#646) -> [T#647]
  Op#364 MUL(T#644, T#290) -> [T#648]
  Op#365 MUL(T#643, T#291) -> [T#649]
  Op#366 ADD(T#648, T#649) -> [T#650]
  Op#367 CONCATENATION(T#647, T#650) -> [T#651]
  Op#368 RESHAPE(T#651, T#268) -> [T#652]
  Op#369 RESHAPE(T#640, T#267) -> [T#653]
  Op#370 SLICE(T#653, T#204, T#198) -> [T#654]
  Op#371 SLICE(T#653, T#199, T#198) -> [T#655]
  Op#372 MUL(T#654, T#290) -> [T#656]
  Op#373 MUL(T#655, T#291) -> [T#657]
  Op#374 SUB(T#656, T#657) -> [T#658]
  Op#375 MUL(T#655, T#290) -> [T#659]
  Op#376 MUL(T#654, T#291) -> [T#660]
  Op#377 ADD(T#659, T#660) -> [T#661]
  Op#378 CONCATENATION(T#658, T#661) -> [T#662]
  Op#379 RESHAPE(T#662, T#266) -> [T#663]
  Op#380 DYNAMIC_UPDATE_SLICE(T#24, T#663, T#339) -> [T#664]
  Op#381 DYNAMIC_UPDATE_SLICE(T#11, T#641, T#339) -> [T#665]
  Op#382 STABLEHLO_COMPOSITE(T#652, T#664, T#665, T#303) -> [T#666]
  Op#383 RESHAPE(T#666, T#274) -> [T#667]
  Op#384 FULLY_CONNECTED(T#667, T#166, T#-1) -> [T#668]
  Op#385 ADD(T#629, T#668) -> [T#669]
  Op#386 MUL(T#669, T#669) -> [T#670]
  Op#387 SUM(T#670, T#206) -> [T#671]
  Op#388 MUL(T#671, T#205) -> [T#672]
  Op#389 ADD(T#672, T#281) -> [T#673]
  Op#390 RSQRT(T#673) -> [T#674]
  Op#391 MUL(T#669, T#674) -> [T#675]
  Op#392 MUL(T#675, T#250) -> [T#676]
  Op#393 FULLY_CONNECTED(T#676, T#165, T#-1) -> [T#677]
  Op#394 LOGISTIC(T#677) -> [T#678]
  Op#395 MUL(T#677, T#678) -> [T#679]
  Op#396 FULLY_CONNECTED(T#676, T#164, T#-1) -> [T#680]
  Op#397 MUL(T#679, T#680) -> [T#681]
  Op#398 FULLY_CONNECTED(T#681, T#163, T#-1) -> [T#682]
  Op#399 ADD(T#669, T#682) -> [T#683]
  Op#400 MUL(T#683, T#683) -> [T#684]
  Op#401 SUM(T#684, T#206) -> [T#685]
  Op#402 MUL(T#685, T#205) -> [T#686]
  Op#403 ADD(T#686, T#281) -> [T#687]
  Op#404 RSQRT(T#687) -> [T#688]
  Op#405 MUL(T#683, T#688) -> [T#689]
  Op#406 MUL(T#689, T#249) -> [T#690]
  Op#407 FULLY_CONNECTED(T#690, T#162, T#-1) -> [T#691]
  Op#408 RESHAPE(T#691, T#270) -> [T#692]
  Op#409 SLICE(T#692, T#204, T#203) -> [T#693]
  Op#410 SLICE(T#692, T#202, T#266) -> [T#694]
  Op#411 SLICE(T#692, T#201, T#266) -> [T#695]
  Op#412 RESHAPE(T#693, T#269) -> [T#696]
  Op#413 SLICE(T#696, T#204, T#200) -> [T#697]
  Op#414 SLICE(T#696, T#199, T#200) -> [T#698]
  Op#415 MUL(T#697, T#290) -> [T#699]
  Op#416 MUL(T#698, T#291) -> [T#700]
  Op#417 SUB(T#699, T#700) -> [T#701]
  Op#418 MUL(T#698, T#290) -> [T#702]
  Op#419 MUL(T#697, T#291) -> [T#703]
  Op#420 ADD(T#702, T#703) -> [T#704]
  Op#421 CONCATENATION(T#701, T#704) -> [T#705]
  Op#422 RESHAPE(T#705, T#268) -> [T#706]
  Op#423 RESHAPE(T#694, T#267) -> [T#707]
  Op#424 SLICE(T#707, T#204, T#198) -> [T#708]
  Op#425 SLICE(T#707, T#199, T#198) -> [T#709]
  Op#426 MUL(T#708, T#290) -> [T#710]
  Op#427 MUL(T#709, T#291) -> [T#711]
  Op#428 SUB(T#710, T#711) -> [T#712]
  Op#429 MUL(T#709, T#290) -> [T#713]
  Op#430 MUL(T#708, T#291) -> [T#714]
  Op#431 ADD(T#713, T#714) -> [T#715]
  Op#432 CONCATENATION(T#712, T#715) -> [T#716]
  Op#433 RESHAPE(T#716, T#266) -> [T#717]
  Op#434 DYNAMIC_UPDATE_SLICE(T#28, T#717, T#339) -> [T#718]
  Op#435 DYNAMIC_UPDATE_SLICE(T#36, T#695, T#339) -> [T#719]
  Op#436 STABLEHLO_COMPOSITE(T#706, T#718, T#719, T#303) -> [T#720]
  Op#437 RESHAPE(T#720, T#274) -> [T#721]
  Op#438 FULLY_CONNECTED(T#721, T#161, T#-1) -> [T#722]
  Op#439 ADD(T#683, T#722) -> [T#723]
  Op#440 MUL(T#723, T#723) -> [T#724]
  Op#441 SUM(T#724, T#206) -> [T#725]
  Op#442 MUL(T#725, T#205) -> [T#726]
  Op#443 ADD(T#726, T#281) -> [T#727]
  Op#444 RSQRT(T#727) -> [T#728]
  Op#445 MUL(T#723, T#728) -> [T#729]
  Op#446 MUL(T#729, T#248) -> [T#730]
  Op#447 FULLY_CONNECTED(T#730, T#160, T#-1) -> [T#731]
  Op#448 LOGISTIC(T#731) -> [T#732]
  Op#449 MUL(T#731, T#732) -> [T#733]
  Op#450 FULLY_CONNECTED(T#730, T#159, T#-1) -> [T#734]
  Op#451 MUL(T#733, T#734) -> [T#735]
  Op#452 FULLY_CONNECTED(T#735, T#158, T#-1) -> [T#736]
  Op#453 ADD(T#723, T#736) -> [T#737]
  Op#454 MUL(T#737, T#737) -> [T#738]
  Op#455 SUM(T#738, T#206) -> [T#739]
  Op#456 MUL(T#739, T#205) -> [T#740]
  Op#457 ADD(T#740, T#281) -> [T#741]
  Op#458 RSQRT(T#741) -> [T#742]
  Op#459 MUL(T#737, T#742) -> [T#743]
  Op#460 MUL(T#743, T#247) -> [T#744]
  Op#461 FULLY_CONNECTED(T#744, T#157, T#-1) -> [T#745]
  Op#462 RESHAPE(T#745, T#270) -> [T#746]
  Op#463 SLICE(T#746, T#204, T#203) -> [T#747]
  Op#464 SLICE(T#746, T#202, T#266) -> [T#748]
  Op#465 SLICE(T#746, T#201, T#266) -> [T#749]
  Op#466 RESHAPE(T#747, T#269) -> [T#750]
  Op#467 SLICE(T#750, T#204, T#200) -> [T#751]
  Op#468 SLICE(T#750, T#199, T#200) -> [T#752]
  Op#469 MUL(T#751, T#290) -> [T#753]
  Op#470 MUL(T#752, T#291) -> [T#754]
  Op#471 SUB(T#753, T#754) -> [T#755]
  Op#472 MUL(T#752, T#290) -> [T#756]
  Op#473 MUL(T#751, T#291) -> [T#757]
  Op#474 ADD(T#756, T#757) -> [T#758]
  Op#475 CONCATENATION(T#755, T#758) -> [T#759]
  Op#476 RESHAPE(T#759, T#268) -> [T#760]
  Op#477 RESHAPE(T#748, T#267) -> [T#761]
  Op#478 SLICE(T#761, T#204, T#198) -> [T#762]
  Op#479 SLICE(T#761, T#199, T#198) -> [T#763]
  Op#480 MUL(T#762, T#290) -> [T#764]
  Op#481 MUL(T#763, T#291) -> [T#765]
  Op#482 SUB(T#764, T#765) -> [T#766]
  Op#483 MUL(T#763, T#290) -> [T#767]
  Op#484 MUL(T#762, T#291) -> [T#768]
  Op#485 ADD(T#767, T#768) -> [T#769]
  Op#486 CONCATENATION(T#766, T#769) -> [T#770]
  Op#487 RESHAPE(T#770, T#266) -> [T#771]
  Op#488 DYNAMIC_UPDATE_SLICE(T#4, T#771, T#339) -> [T#772]
  Op#489 DYNAMIC_UPDATE_SLICE(T#49, T#749, T#339) -> [T#773]
  Op#490 STABLEHLO_COMPOSITE(T#760, T#772, T#773, T#303) -> [T#774]
  Op#491 RESHAPE(T#774, T#274) -> [T#775]
  Op#492 FULLY_CONNECTED(T#775, T#156, T#-1) -> [T#776]
  Op#493 ADD(T#737, T#776) -> [T#777]
  Op#494 MUL(T#777, T#777) -> [T#778]
  Op#495 SUM(T#778, T#206) -> [T#779]
  Op#496 MUL(T#779, T#205) -> [T#780]
  Op#497 ADD(T#780, T#281) -> [T#781]
  Op#498 RSQRT(T#781) -> [T#782]
  Op#499 MUL(T#777, T#782) -> [T#783]
  Op#500 MUL(T#783, T#246) -> [T#784]
  Op#501 FULLY_CONNECTED(T#784, T#155, T#-1) -> [T#785]
  Op#502 LOGISTIC(T#785) -> [T#786]
  Op#503 MUL(T#785, T#786) -> [T#787]
  Op#504 FULLY_CONNECTED(T#784, T#154, T#-1) -> [T#788]
  Op#505 MUL(T#787, T#788) -> [T#789]
  Op#506 FULLY_CONNECTED(T#789, T#153, T#-1) -> [T#790]
  Op#507 ADD(T#777, T#790) -> [T#791]
  Op#508 MUL(T#791, T#791) -> [T#792]
  Op#509 SUM(T#792, T#206) -> [T#793]
  Op#510 MUL(T#793, T#205) -> [T#794]
  Op#511 ADD(T#794, T#281) -> [T#795]
  Op#512 RSQRT(T#795) -> [T#796]
  Op#513 MUL(T#791, T#796) -> [T#797]
  Op#514 MUL(T#797, T#245) -> [T#798]
  Op#515 FULLY_CONNECTED(T#798, T#152, T#-1) -> [T#799]
  Op#516 RESHAPE(T#799, T#270) -> [T#800]
  Op#517 SLICE(T#800, T#204, T#203) -> [T#801]
  Op#518 SLICE(T#800, T#202, T#266) -> [T#802]
  Op#519 SLICE(T#800, T#201, T#266) -> [T#803]
  Op#520 RESHAPE(T#801, T#269) -> [T#804]
  Op#521 SLICE(T#804, T#204, T#200) -> [T#805]
  Op#522 SLICE(T#804, T#199, T#200) -> [T#806]
  Op#523 MUL(T#805, T#290) -> [T#807]
  Op#524 MUL(T#806, T#291) -> [T#808]
  Op#525 SUB(T#807, T#808) -> [T#809]
  Op#526 MUL(T#806, T#290) -> [T#810]
  Op#527 MUL(T#805, T#291) -> [T#811]
  Op#528 ADD(T#810, T#811) -> [T#812]
  Op#529 CONCATENATION(T#809, T#812) -> [T#813]
  Op#530 RESHAPE(T#813, T#268) -> [T#814]
  Op#531 RESHAPE(T#802, T#267) -> [T#815]
  Op#532 SLICE(T#815, T#204, T#198) -> [T#816]
  Op#533 SLICE(T#815, T#199, T#198) -> [T#817]
  Op#534 MUL(T#816, T#290) -> [T#818]
  Op#535 MUL(T#817, T#291) -> [T#819]
  Op#536 SUB(T#818, T#819) -> [T#820]
  Op#537 MUL(T#817, T#290) -> [T#821]
  Op#538 MUL(T#816, T#291) -> [T#822]
  Op#539 ADD(T#821, T#822) -> [T#823]
  Op#540 CONCATENATION(T#820, T#823) -> [T#824]
  Op#541 RESHAPE(T#824, T#266) -> [T#825]
  Op#542 DYNAMIC_UPDATE_SLICE(T#16, T#825, T#339) -> [T#826]
  Op#543 DYNAMIC_UPDATE_SLICE(T#46, T#803, T#339) -> [T#827]
  Op#544 STABLEHLO_COMPOSITE(T#814, T#826, T#827, T#303) -> [T#828]
  Op#545 RESHAPE(T#828, T#274) -> [T#829]
  Op#546 FULLY_CONNECTED(T#829, T#151, T#-1) -> [T#830]
  Op#547 ADD(T#791, T#830) -> [T#831]
  Op#548 MUL(T#831, T#831) -> [T#832]
  Op#549 SUM(T#832, T#206) -> [T#833]
  Op#550 MUL(T#833, T#205) -> [T#834]
  Op#551 ADD(T#834, T#281) -> [T#835]
  Op#552 RSQRT(T#835) -> [T#836]
  Op#553 MUL(T#831, T#836) -> [T#837]
  Op#554 MUL(T#837, T#244) -> [T#838]
  Op#555 FULLY_CONNECTED(T#838, T#150, T#-1) -> [T#839]
  Op#556 LOGISTIC(T#839) -> [T#840]
  Op#557 MUL(T#839, T#840) -> [T#841]
  Op#558 FULLY_CONNECTED(T#838, T#149, T#-1) -> [T#842]
  Op#559 MUL(T#841, T#842) -> [T#843]
  Op#560 FULLY_CONNECTED(T#843, T#148, T#-1) -> [T#844]
  Op#561 ADD(T#831, T#844) -> [T#845]
  Op#562 MUL(T#845, T#845) -> [T#846]
  Op#563 SUM(T#846, T#206) -> [T#847]
  Op#564 MUL(T#847, T#205) -> [T#848]
  Op#565 ADD(T#848, T#281) -> [T#849]
  Op#566 RSQRT(T#849) -> [T#850]
  Op#567 MUL(T#845, T#850) -> [T#851]
  Op#568 MUL(T#851, T#243) -> [T#852]
  Op#569 FULLY_CONNECTED(T#852, T#147, T#-1) -> [T#853]
  Op#570 RESHAPE(T#853, T#270) -> [T#854]
  Op#571 SLICE(T#854, T#204, T#203) -> [T#855]
  Op#572 SLICE(T#854, T#202, T#266) -> [T#856]
  Op#573 SLICE(T#854, T#201, T#266) -> [T#857]
  Op#574 RESHAPE(T#855, T#269) -> [T#858]
  Op#575 SLICE(T#858, T#204, T#200) -> [T#859]
  Op#576 SLICE(T#858, T#199, T#200) -> [T#860]
  Op#577 MUL(T#859, T#290) -> [T#861]
  Op#578 MUL(T#860, T#291) -> [T#862]
  Op#579 SUB(T#861, T#862) -> [T#863]
  Op#580 MUL(T#860, T#290) -> [T#864]
  Op#581 MUL(T#859, T#291) -> [T#865]
  Op#582 ADD(T#864, T#865) -> [T#866]
  Op#583 CONCATENATION(T#863, T#866) -> [T#867]
  Op#584 RESHAPE(T#867, T#268) -> [T#868]
  Op#585 RESHAPE(T#856, T#267) -> [T#869]
  Op#586 SLICE(T#869, T#204, T#198) -> [T#870]
  Op#587 SLICE(T#869, T#199, T#198) -> [T#871]
  Op#588 MUL(T#870, T#290) -> [T#872]
  Op#589 MUL(T#871, T#291) -> [T#873]
  Op#590 SUB(T#872, T#873) -> [T#874]
  Op#591 MUL(T#871, T#290) -> [T#875]
  Op#592 MUL(T#870, T#291) -> [T#876]
  Op#593 ADD(T#875, T#876) -> [T#877]
  Op#594 CONCATENATION(T#874, T#877) -> [T#878]
  Op#595 RESHAPE(T#878, T#266) -> [T#879]
  Op#596 DYNAMIC_UPDATE_SLICE(T#8, T#879, T#339) -> [T#880]
  Op#597 DYNAMIC_UPDATE_SLICE(T#29, T#857, T#339) -> [T#881]
  Op#598 STABLEHLO_COMPOSITE(T#868, T#880, T#881, T#303) -> [T#882]
  Op#599 RESHAPE(T#882, T#274) -> [T#883]
  Op#600 FULLY_CONNECTED(T#883, T#146, T#-1) -> [T#884]
  Op#601 ADD(T#845, T#884) -> [T#885]
  Op#602 MUL(T#885, T#885) -> [T#886]
  Op#603 SUM(T#886, T#206) -> [T#887]
  Op#604 MUL(T#887, T#205) -> [T#888]
  Op#605 ADD(T#888, T#281) -> [T#889]
  Op#606 RSQRT(T#889) -> [T#890]
  Op#607 MUL(T#885, T#890) -> [T#891]
  Op#608 MUL(T#891, T#242) -> [T#892]
  Op#609 FULLY_CONNECTED(T#892, T#145, T#-1) -> [T#893]
  Op#610 LOGISTIC(T#893) -> [T#894]
  Op#611 MUL(T#893, T#894) -> [T#895]
  Op#612 FULLY_CONNECTED(T#892, T#144, T#-1) -> [T#896]
  Op#613 MUL(T#895, T#896) -> [T#897]
  Op#614 FULLY_CONNECTED(T#897, T#143, T#-1) -> [T#898]
  Op#615 ADD(T#885, T#898) -> [T#899]
  Op#616 MUL(T#899, T#899) -> [T#900]
  Op#617 SUM(T#900, T#206) -> [T#901]
  Op#618 MUL(T#901, T#205) -> [T#902]
  Op#619 ADD(T#902, T#281) -> [T#903]
  Op#620 RSQRT(T#903) -> [T#904]
  Op#621 MUL(T#899, T#904) -> [T#905]
  Op#622 MUL(T#905, T#241) -> [T#906]
  Op#623 FULLY_CONNECTED(T#906, T#142, T#-1) -> [T#907]
  Op#624 RESHAPE(T#907, T#270) -> [T#908]
  Op#625 SLICE(T#908, T#204, T#203) -> [T#909]
  Op#626 SLICE(T#908, T#202, T#266) -> [T#910]
  Op#627 SLICE(T#908, T#201, T#266) -> [T#911]
  Op#628 RESHAPE(T#909, T#269) -> [T#912]
  Op#629 SLICE(T#912, T#204, T#200) -> [T#913]
  Op#630 SLICE(T#912, T#199, T#200) -> [T#914]
  Op#631 MUL(T#913, T#290) -> [T#915]
  Op#632 MUL(T#914, T#291) -> [T#916]
  Op#633 SUB(T#915, T#916) -> [T#917]
  Op#634 MUL(T#914, T#290) -> [T#918]
  Op#635 MUL(T#913, T#291) -> [T#919]
  Op#636 ADD(T#918, T#919) -> [T#920]
  Op#637 CONCATENATION(T#917, T#920) -> [T#921]
  Op#638 RESHAPE(T#921, T#268) -> [T#922]
  Op#639 RESHAPE(T#910, T#267) -> [T#923]
  Op#640 SLICE(T#923, T#204, T#198) -> [T#924]
  Op#641 SLICE(T#923, T#199, T#198) -> [T#925]
  Op#642 MUL(T#924, T#290) -> [T#926]
  Op#643 MUL(T#925, T#291) -> [T#927]
  Op#644 SUB(T#926, T#927) -> [T#928]
  Op#645 MUL(T#925, T#290) -> [T#929]
  Op#646 MUL(T#924, T#291) -> [T#930]
  Op#647 ADD(T#929, T#930) -> [T#931]
  Op#648 CONCATENATION(T#928, T#931) -> [T#932]
  Op#649 RESHAPE(T#932, T#266) -> [T#933]
  Op#650 DYNAMIC_UPDATE_SLICE(T#52, T#933, T#339) -> [T#934]
  Op#651 DYNAMIC_UPDATE_SLICE(T#22, T#911, T#339) -> [T#935]
  Op#652 STABLEHLO_COMPOSITE(T#922, T#934, T#935, T#303) -> [T#936]
  Op#653 RESHAPE(T#936, T#274) -> [T#937]
  Op#654 FULLY_CONNECTED(T#937, T#141, T#-1) -> [T#938]
  Op#655 ADD(T#899, T#938) -> [T#939]
  Op#656 MUL(T#939, T#939) -> [T#940]
  Op#657 SUM(T#940, T#206) -> [T#941]
  Op#658 MUL(T#941, T#205) -> [T#942]
  Op#659 ADD(T#942, T#281) -> [T#943]
  Op#660 RSQRT(T#943) -> [T#944]
  Op#661 MUL(T#939, T#944) -> [T#945]
  Op#662 MUL(T#945, T#240) -> [T#946]
  Op#663 FULLY_CONNECTED(T#946, T#140, T#-1) -> [T#947]
  Op#664 LOGISTIC(T#947) -> [T#948]
  Op#665 MUL(T#947, T#948) -> [T#949]
  Op#666 FULLY_CONNECTED(T#946, T#139, T#-1) -> [T#950]
  Op#667 MUL(T#949, T#950) -> [T#951]
  Op#668 FULLY_CONNECTED(T#951, T#138, T#-1) -> [T#952]
  Op#669 ADD(T#939, T#952) -> [T#953]
  Op#670 MUL(T#953, T#953) -> [T#954]
  Op#671 SUM(T#954, T#206) -> [T#955]
  Op#672 MUL(T#955, T#205) -> [T#956]
  Op#673 ADD(T#956, T#281) -> [T#957]
  Op#674 RSQRT(T#957) -> [T#958]
  Op#675 MUL(T#953, T#958) -> [T#959]
  Op#676 MUL(T#959, T#239) -> [T#960]
  Op#677 FULLY_CONNECTED(T#960, T#137, T#-1) -> [T#961]
  Op#678 RESHAPE(T#961, T#270) -> [T#962]
  Op#679 SLICE(T#962, T#204, T#203) -> [T#963]
  Op#680 SLICE(T#962, T#202, T#266) -> [T#964]
  Op#681 SLICE(T#962, T#201, T#266) -> [T#965]
  Op#682 RESHAPE(T#963, T#269) -> [T#966]
  Op#683 SLICE(T#966, T#204, T#200) -> [T#967]
  Op#684 SLICE(T#966, T#199, T#200) -> [T#968]
  Op#685 MUL(T#967, T#290) -> [T#969]
  Op#686 MUL(T#968, T#291) -> [T#970]
  Op#687 SUB(T#969, T#970) -> [T#971]
  Op#688 MUL(T#968, T#290) -> [T#972]
  Op#689 MUL(T#967, T#291) -> [T#973]
  Op#690 ADD(T#972, T#973) -> [T#974]
  Op#691 CONCATENATION(T#971, T#974) -> [T#975]
  Op#692 RESHAPE(T#975, T#268) -> [T#976]
  Op#693 RESHAPE(T#964, T#267) -> [T#977]
  Op#694 SLICE(T#977, T#204, T#198) -> [T#978]
  Op#695 SLICE(T#977, T#199, T#198) -> [T#979]
  Op#696 MUL(T#978, T#290) -> [T#980]
  Op#697 MUL(T#979, T#291) -> [T#981]
  Op#698 SUB(T#980, T#981) -> [T#982]
  Op#699 MUL(T#979, T#290) -> [T#983]
  Op#700 MUL(T#978, T#291) -> [T#984]
  Op#701 ADD(T#983, T#984) -> [T#985]
  Op#702 CONCATENATION(T#982, T#985) -> [T#986]
  Op#703 RESHAPE(T#986, T#266) -> [T#987]
  Op#704 DYNAMIC_UPDATE_SLICE(T#20, T#987, T#339) -> [T#988]
  Op#705 DYNAMIC_UPDATE_SLICE(T#31, T#965, T#339) -> [T#989]
  Op#706 STABLEHLO_COMPOSITE(T#976, T#988, T#989, T#303) -> [T#990]
  Op#707 RESHAPE(T#990, T#274) -> [T#991]
  Op#708 FULLY_CONNECTED(T#991, T#136, T#-1) -> [T#992]
  Op#709 ADD(T#953, T#992) -> [T#993]
  Op#710 MUL(T#993, T#993) -> [T#994]
  Op#711 SUM(T#994, T#206) -> [T#995]
  Op#712 MUL(T#995, T#205) -> [T#996]
  Op#713 ADD(T#996, T#281) -> [T#997]
  Op#714 RSQRT(T#997) -> [T#998]
  Op#715 MUL(T#993, T#998) -> [T#999]
  Op#716 MUL(T#999, T#238) -> [T#1000]
  Op#717 FULLY_CONNECTED(T#1000, T#135, T#-1) -> [T#1001]
  Op#718 LOGISTIC(T#1001) -> [T#1002]
  Op#719 MUL(T#1001, T#1002) -> [T#1003]
  Op#720 FULLY_CONNECTED(T#1000, T#134, T#-1) -> [T#1004]
  Op#721 MUL(T#1003, T#1004) -> [T#1005]
  Op#722 FULLY_CONNECTED(T#1005, T#133, T#-1) -> [T#1006]
  Op#723 ADD(T#993, T#1006) -> [T#1007]
  Op#724 MUL(T#1007, T#1007) -> [T#1008]
  Op#725 SUM(T#1008, T#206) -> [T#1009]
  Op#726 MUL(T#1009, T#205) -> [T#1010]
  Op#727 ADD(T#1010, T#281) -> [T#1011]
  Op#728 RSQRT(T#1011) -> [T#1012]
  Op#729 MUL(T#1007, T#1012) -> [T#1013]
  Op#730 MUL(T#1013, T#237) -> [T#1014]
  Op#731 FULLY_CONNECTED(T#1014, T#132, T#-1) -> [T#1015]
  Op#732 RESHAPE(T#1015, T#270) -> [T#1016]
  Op#733 SLICE(T#1016, T#204, T#203) -> [T#1017]
  Op#734 SLICE(T#1016, T#202, T#266) -> [T#1018]
  Op#735 SLICE(T#1016, T#201, T#266) -> [T#1019]
  Op#736 RESHAPE(T#1017, T#269) -> [T#1020]
  Op#737 SLICE(T#1020, T#204, T#200) -> [T#1021]
  Op#738 SLICE(T#1020, T#199, T#200) -> [T#1022]
  Op#739 MUL(T#1021, T#290) -> [T#1023]
  Op#740 MUL(T#1022, T#291) -> [T#1024]
  Op#741 SUB(T#1023, T#1024) -> [T#1025]
  Op#742 MUL(T#1022, T#290) -> [T#1026]
  Op#743 MUL(T#1021, T#291) -> [T#1027]
  Op#744 ADD(T#1026, T#1027) -> [T#1028]
  Op#745 CONCATENATION(T#1025, T#1028) -> [T#1029]
  Op#746 RESHAPE(T#1029, T#268) -> [T#1030]
  Op#747 RESHAPE(T#1018, T#267) -> [T#1031]
  Op#748 SLICE(T#1031, T#204, T#198) -> [T#1032]
  Op#749 SLICE(T#1031, T#199, T#198) -> [T#1033]
  Op#750 MUL(T#1032, T#290) -> [T#1034]
  Op#751 MUL(T#1033, T#291) -> [T#1035]
  Op#752 SUB(T#1034, T#1035) -> [T#1036]
  Op#753 MUL(T#1033, T#290) -> [T#1037]
  Op#754 MUL(T#1032, T#291) -> [T#1038]
  Op#755 ADD(T#1037, T#1038) -> [T#1039]
  Op#756 CONCATENATION(T#1036, T#1039) -> [T#1040]
  Op#757 RESHAPE(T#1040, T#266) -> [T#1041]
  Op#758 DYNAMIC_UPDATE_SLICE(T#35, T#1041, T#339) -> [T#1042]
  Op#759 DYNAMIC_UPDATE_SLICE(T#41, T#1019, T#339) -> [T#1043]
  Op#760 STABLEHLO_COMPOSITE(T#1030, T#1042, T#1043, T#303) -> [T#1044]
  Op#761 RESHAPE(T#1044, T#274) -> [T#1045]
  Op#762 FULLY_CONNECTED(T#1045, T#131, T#-1) -> [T#1046]
  Op#763 ADD(T#1007, T#1046) -> [T#1047]
  Op#764 MUL(T#1047, T#1047) -> [T#1048]
  Op#765 SUM(T#1048, T#206) -> [T#1049]
  Op#766 MUL(T#1049, T#205) -> [T#1050]
  Op#767 ADD(T#1050, T#281) -> [T#1051]
  Op#768 RSQRT(T#1051) -> [T#1052]
  Op#769 MUL(T#1047, T#1052) -> [T#1053]
  Op#770 MUL(T#1053, T#236) -> [T#1054]
  Op#771 FULLY_CONNECTED(T#1054, T#130, T#-1) -> [T#1055]
  Op#772 LOGISTIC(T#1055) -> [T#1056]
  Op#773 MUL(T#1055, T#1056) -> [T#1057]
  Op#774 FULLY_CONNECTED(T#1054, T#129, T#-1) -> [T#1058]
  Op#775 MUL(T#1057, T#1058) -> [T#1059]
  Op#776 FULLY_CONNECTED(T#1059, T#128, T#-1) -> [T#1060]
  Op#777 ADD(T#1047, T#1060) -> [T#1061]
  Op#778 MUL(T#1061, T#1061) -> [T#1062]
  Op#779 SUM(T#1062, T#206) -> [T#1063]
  Op#780 MUL(T#1063, T#205) -> [T#1064]
  Op#781 ADD(T#1064, T#281) -> [T#1065]
  Op#782 RSQRT(T#1065) -> [T#1066]
  Op#783 MUL(T#1061, T#1066) -> [T#1067]
  Op#784 MUL(T#1067, T#235) -> [T#1068]
  Op#785 FULLY_CONNECTED(T#1068, T#127, T#-1) -> [T#1069]
  Op#786 RESHAPE(T#1069, T#270) -> [T#1070]
  Op#787 SLICE(T#1070, T#204, T#203) -> [T#1071]
  Op#788 SLICE(T#1070, T#202, T#266) -> [T#1072]
  Op#789 SLICE(T#1070, T#201, T#266) -> [T#1073]
  Op#790 RESHAPE(T#1071, T#269) -> [T#1074]
  Op#791 SLICE(T#1074, T#204, T#200) -> [T#1075]
  Op#792 SLICE(T#1074, T#199, T#200) -> [T#1076]
  Op#793 MUL(T#1075, T#290) -> [T#1077]
  Op#794 MUL(T#1076, T#291) -> [T#1078]
  Op#795 SUB(T#1077, T#1078) -> [T#1079]
  Op#796 MUL(T#1076, T#290) -> [T#1080]
  Op#797 MUL(T#1075, T#291) -> [T#1081]
  Op#798 ADD(T#1080, T#1081) -> [T#1082]
  Op#799 CONCATENATION(T#1079, T#1082) -> [T#1083]
  Op#800 RESHAPE(T#1083, T#268) -> [T#1084]
  Op#801 RESHAPE(T#1072, T#267) -> [T#1085]
  Op#802 SLICE(T#1085, T#204, T#198) -> [T#1086]
  Op#803 SLICE(T#1085, T#199, T#198) -> [T#1087]
  Op#804 MUL(T#1086, T#290) -> [T#1088]
  Op#805 MUL(T#1087, T#291) -> [T#1089]
  Op#806 SUB(T#1088, T#1089) -> [T#1090]
  Op#807 MUL(T#1087, T#290) -> [T#1091]
  Op#808 MUL(T#1086, T#291) -> [T#1092]
  Op#809 ADD(T#1091, T#1092) -> [T#1093]
  Op#810 CONCATENATION(T#1090, T#1093) -> [T#1094]
  Op#811 RESHAPE(T#1094, T#266) -> [T#1095]
  Op#812 DYNAMIC_UPDATE_SLICE(T#33, T#1095, T#339) -> [T#1096]
  Op#813 DYNAMIC_UPDATE_SLICE(T#57, T#1073, T#339) -> [T#1097]
  Op#814 STABLEHLO_COMPOSITE(T#1084, T#1096, T#1097, T#303) -> [T#1098]
  Op#815 RESHAPE(T#1098, T#274) -> [T#1099]
  Op#816 FULLY_CONNECTED(T#1099, T#126, T#-1) -> [T#1100]
  Op#817 ADD(T#1061, T#1100) -> [T#1101]
  Op#818 MUL(T#1101, T#1101) -> [T#1102]
  Op#819 SUM(T#1102, T#206) -> [T#1103]
  Op#820 MUL(T#1103, T#205) -> [T#1104]
  Op#821 ADD(T#1104, T#281) -> [T#1105]
  Op#822 RSQRT(T#1105) -> [T#1106]
  Op#823 MUL(T#1101, T#1106) -> [T#1107]
  Op#824 MUL(T#1107, T#234) -> [T#1108]
  Op#825 FULLY_CONNECTED(T#1108, T#125, T#-1) -> [T#1109]
  Op#826 LOGISTIC(T#1109) -> [T#1110]
  Op#827 MUL(T#1109, T#1110) -> [T#1111]
  Op#828 FULLY_CONNECTED(T#1108, T#124, T#-1) -> [T#1112]
  Op#829 MUL(T#1111, T#1112) -> [T#1113]
  Op#830 FULLY_CONNECTED(T#1113, T#123, T#-1) -> [T#1114]
  Op#831 ADD(T#1101, T#1114) -> [T#1115]
  Op#832 MUL(T#1115, T#1115) -> [T#1116]
  Op#833 SUM(T#1116, T#206) -> [T#1117]
  Op#834 MUL(T#1117, T#205) -> [T#1118]
  Op#835 ADD(T#1118, T#281) -> [T#1119]
  Op#836 RSQRT(T#1119) -> [T#1120]
  Op#837 MUL(T#1115, T#1120) -> [T#1121]
  Op#838 MUL(T#1121, T#233) -> [T#1122]
  Op#839 FULLY_CONNECTED(T#1122, T#122, T#-1) -> [T#1123]
  Op#840 RESHAPE(T#1123, T#270) -> [T#1124]
  Op#841 SLICE(T#1124, T#204, T#203) -> [T#1125]
  Op#842 SLICE(T#1124, T#202, T#266) -> [T#1126]
  Op#843 SLICE(T#1124, T#201, T#266) -> [T#1127]
  Op#844 RESHAPE(T#1125, T#269) -> [T#1128]
  Op#845 SLICE(T#1128, T#204, T#200) -> [T#1129]
  Op#846 SLICE(T#1128, T#199, T#200) -> [T#1130]
  Op#847 MUL(T#1129, T#290) -> [T#1131]
  Op#848 MUL(T#1130, T#291) -> [T#1132]
  Op#849 SUB(T#1131, T#1132) -> [T#1133]
  Op#850 MUL(T#1130, T#290) -> [T#1134]
  Op#851 MUL(T#1129, T#291) -> [T#1135]
  Op#852 ADD(T#1134, T#1135) -> [T#1136]
  Op#853 CONCATENATION(T#1133, T#1136) -> [T#1137]
  Op#854 RESHAPE(T#1137, T#268) -> [T#1138]
  Op#855 RESHAPE(T#1126, T#267) -> [T#1139]
  Op#856 SLICE(T#1139, T#204, T#198) -> [T#1140]
  Op#857 SLICE(T#1139, T#199, T#198) -> [T#1141]
  Op#858 MUL(T#1140, T#290) -> [T#1142]
  Op#859 MUL(T#1141, T#291) -> [T#1143]
  Op#860 SUB(T#1142, T#1143) -> [T#1144]
  Op#861 MUL(T#1141, T#290) -> [T#1145]
  Op#862 MUL(T#1140, T#291) -> [T#1146]
  Op#863 ADD(T#1145, T#1146) -> [T#1147]
  Op#864 CONCATENATION(T#1144, T#1147) -> [T#1148]
  Op#865 RESHAPE(T#1148, T#266) -> [T#1149]
  Op#866 DYNAMIC_UPDATE_SLICE(T#45, T#1149, T#339) -> [T#1150]
  Op#867 DYNAMIC_UPDATE_SLICE(T#34, T#1127, T#339) -> [T#1151]
  Op#868 STABLEHLO_COMPOSITE(T#1138, T#1150, T#1151, T#303) -> [T#1152]
  Op#869 RESHAPE(T#1152, T#274) -> [T#1153]
  Op#870 FULLY_CONNECTED(T#1153, T#121, T#-1) -> [T#1154]
  Op#871 ADD(T#1115, T#1154) -> [T#1155]
  Op#872 MUL(T#1155, T#1155) -> [T#1156]
  Op#873 SUM(T#1156, T#206) -> [T#1157]
  Op#874 MUL(T#1157, T#205) -> [T#1158]
  Op#875 ADD(T#1158, T#281) -> [T#1159]
  Op#876 RSQRT(T#1159) -> [T#1160]
  Op#877 MUL(T#1155, T#1160) -> [T#1161]
  Op#878 MUL(T#1161, T#232) -> [T#1162]
  Op#879 FULLY_CONNECTED(T#1162, T#120, T#-1) -> [T#1163]
  Op#880 LOGISTIC(T#1163) -> [T#1164]
  Op#881 MUL(T#1163, T#1164) -> [T#1165]
  Op#882 FULLY_CONNECTED(T#1162, T#119, T#-1) -> [T#1166]
  Op#883 MUL(T#1165, T#1166) -> [T#1167]
  Op#884 FULLY_CONNECTED(T#1167, T#118, T#-1) -> [T#1168]
  Op#885 ADD(T#1155, T#1168) -> [T#1169]
  Op#886 MUL(T#1169, T#1169) -> [T#1170]
  Op#887 SUM(T#1170, T#206) -> [T#1171]
  Op#888 MUL(T#1171, T#205) -> [T#1172]
  Op#889 ADD(T#1172, T#281) -> [T#1173]
  Op#890 RSQRT(T#1173) -> [T#1174]
  Op#891 MUL(T#1169, T#1174) -> [T#1175]
  Op#892 MUL(T#1175, T#231) -> [T#1176]
  Op#893 FULLY_CONNECTED(T#1176, T#117, T#-1) -> [T#1177]
  Op#894 RESHAPE(T#1177, T#270) -> [T#1178]
  Op#895 SLICE(T#1178, T#204, T#203) -> [T#1179]
  Op#896 SLICE(T#1178, T#202, T#266) -> [T#1180]
  Op#897 SLICE(T#1178, T#201, T#266) -> [T#1181]
  Op#898 RESHAPE(T#1179, T#269) -> [T#1182]
  Op#899 SLICE(T#1182, T#204, T#200) -> [T#1183]
  Op#900 SLICE(T#1182, T#199, T#200) -> [T#1184]
  Op#901 MUL(T#1183, T#290) -> [T#1185]
  Op#902 MUL(T#1184, T#291) -> [T#1186]
  Op#903 SUB(T#1185, T#1186) -> [T#1187]
  Op#904 MUL(T#1184, T#290) -> [T#1188]
  Op#905 MUL(T#1183, T#291) -> [T#1189]
  Op#906 ADD(T#1188, T#1189) -> [T#1190]
  Op#907 CONCATENATION(T#1187, T#1190) -> [T#1191]
  Op#908 RESHAPE(T#1191, T#268) -> [T#1192]
  Op#909 RESHAPE(T#1180, T#267) -> [T#1193]
  Op#910 SLICE(T#1193, T#204, T#198) -> [T#1194]
  Op#911 SLICE(T#1193, T#199, T#198) -> [T#1195]
  Op#912 MUL(T#1194, T#290) -> [T#1196]
  Op#913 MUL(T#1195, T#291) -> [T#1197]
  Op#914 SUB(T#1196, T#1197) -> [T#1198]
  Op#915 MUL(T#1195, T#290) -> [T#1199]
  Op#916 MUL(T#1194, T#291) -> [T#1200]
  Op#917 ADD(T#1199, T#1200) -> [T#1201]
  Op#918 CONCATENATION(T#1198, T#1201) -> [T#1202]
  Op#919 RESHAPE(T#1202, T#266) -> [T#1203]
  Op#920 DYNAMIC_UPDATE_SLICE(T#3, T#1203, T#339) -> [T#1204]
  Op#921 DYNAMIC_UPDATE_SLICE(T#30, T#1181, T#339) -> [T#1205]
  Op#922 STABLEHLO_COMPOSITE(T#1192, T#1204, T#1205, T#303) -> [T#1206]
  Op#923 RESHAPE(T#1206, T#274) -> [T#1207]
  Op#924 FULLY_CONNECTED(T#1207, T#116, T#-1) -> [T#1208]
  Op#925 ADD(T#1169, T#1208) -> [T#1209]
  Op#926 MUL(T#1209, T#1209) -> [T#1210]
  Op#927 SUM(T#1210, T#206) -> [T#1211]
  Op#928 MUL(T#1211, T#205) -> [T#1212]
  Op#929 ADD(T#1212, T#281) -> [T#1213]
  Op#930 RSQRT(T#1213) -> [T#1214]
  Op#931 MUL(T#1209, T#1214) -> [T#1215]
  Op#932 MUL(T#1215, T#230) -> [T#1216]
  Op#933 FULLY_CONNECTED(T#1216, T#115, T#-1) -> [T#1217]
  Op#934 LOGISTIC(T#1217) -> [T#1218]
  Op#935 MUL(T#1217, T#1218) -> [T#1219]
  Op#936 FULLY_CONNECTED(T#1216, T#114, T#-1) -> [T#1220]
  Op#937 MUL(T#1219, T#1220) -> [T#1221]
  Op#938 FULLY_CONNECTED(T#1221, T#113, T#-1) -> [T#1222]
  Op#939 ADD(T#1209, T#1222) -> [T#1223]
  Op#940 MUL(T#1223, T#1223) -> [T#1224]
  Op#941 SUM(T#1224, T#206) -> [T#1225]
  Op#942 MUL(T#1225, T#205) -> [T#1226]
  Op#943 ADD(T#1226, T#281) -> [T#1227]
  Op#944 RSQRT(T#1227) -> [T#1228]
  Op#945 MUL(T#1223, T#1228) -> [T#1229]
  Op#946 MUL(T#1229, T#229) -> [T#1230]
  Op#947 FULLY_CONNECTED(T#1230, T#112, T#-1) -> [T#1231]
  Op#948 RESHAPE(T#1231, T#270) -> [T#1232]
  Op#949 SLICE(T#1232, T#204, T#203) -> [T#1233]
  Op#950 SLICE(T#1232, T#202, T#266) -> [T#1234]
  Op#951 SLICE(T#1232, T#201, T#266) -> [T#1235]
  Op#952 RESHAPE(T#1233, T#269) -> [T#1236]
  Op#953 SLICE(T#1236, T#204, T#200) -> [T#1237]
  Op#954 SLICE(T#1236, T#199, T#200) -> [T#1238]
  Op#955 MUL(T#1237, T#290) -> [T#1239]
  Op#956 MUL(T#1238, T#291) -> [T#1240]
  Op#957 SUB(T#1239, T#1240) -> [T#1241]
  Op#958 MUL(T#1238, T#290) -> [T#1242]
  Op#959 MUL(T#1237, T#291) -> [T#1243]
  Op#960 ADD(T#1242, T#1243) -> [T#1244]
  Op#961 CONCATENATION(T#1241, T#1244) -> [T#1245]
  Op#962 RESHAPE(T#1245, T#268) -> [T#1246]
  Op#963 RESHAPE(T#1234, T#267) -> [T#1247]
  Op#964 SLICE(T#1247, T#204, T#198) -> [T#1248]
  Op#965 SLICE(T#1247, T#199, T#198) -> [T#1249]
  Op#966 MUL(T#1248, T#290) -> [T#1250]
  Op#967 MUL(T#1249, T#291) -> [T#1251]
  Op#968 SUB(T#1250, T#1251) -> [T#1252]
  Op#969 MUL(T#1249, T#290) -> [T#1253]
  Op#970 MUL(T#1248, T#291) -> [T#1254]
  Op#971 ADD(T#1253, T#1254) -> [T#1255]
  Op#972 CONCATENATION(T#1252, T#1255) -> [T#1256]
  Op#973 RESHAPE(T#1256, T#266) -> [T#1257]
  Op#974 DYNAMIC_UPDATE_SLICE(T#10, T#1257, T#339) -> [T#1258]
  Op#975 DYNAMIC_UPDATE_SLICE(T#40, T#1235, T#339) -> [T#1259]
  Op#976 STABLEHLO_COMPOSITE(T#1246, T#1258, T#1259, T#303) -> [T#1260]
  Op#977 RESHAPE(T#1260, T#274) -> [T#1261]
  Op#978 FULLY_CONNECTED(T#1261, T#111, T#-1) -> [T#1262]
  Op#979 ADD(T#1223, T#1262) -> [T#1263]
  Op#980 MUL(T#1263, T#1263) -> [T#1264]
  Op#981 SUM(T#1264, T#206) -> [T#1265]
  Op#982 MUL(T#1265, T#205) -> [T#1266]
  Op#983 ADD(T#1266, T#281) -> [T#1267]
  Op#984 RSQRT(T#1267) -> [T#1268]
  Op#985 MUL(T#1263, T#1268) -> [T#1269]
  Op#986 MUL(T#1269, T#228) -> [T#1270]
  Op#987 FULLY_CONNECTED(T#1270, T#110, T#-1) -> [T#1271]
  Op#988 LOGISTIC(T#1271) -> [T#1272]
  Op#989 MUL(T#1271, T#1272) -> [T#1273]
  Op#990 FULLY_CONNECTED(T#1270, T#109, T#-1) -> [T#1274]
  Op#991 MUL(T#1273, T#1274) -> [T#1275]
  Op#992 FULLY_CONNECTED(T#1275, T#108, T#-1) -> [T#1276]
  Op#993 ADD(T#1263, T#1276) -> [T#1277]
  Op#994 MUL(T#1277, T#1277) -> [T#1278]
  Op#995 SUM(T#1278, T#206) -> [T#1279]
  Op#996 MUL(T#1279, T#205) -> [T#1280]
  Op#997 ADD(T#1280, T#281) -> [T#1281]
  Op#998 RSQRT(T#1281) -> [T#1282]
  Op#999 MUL(T#1277, T#1282) -> [T#1283]
  Op#1000 MUL(T#1283, T#227) -> [T#1284]
  Op#1001 FULLY_CONNECTED(T#1284, T#107, T#-1) -> [T#1285]
  Op#1002 RESHAPE(T#1285, T#270) -> [T#1286]
  Op#1003 SLICE(T#1286, T#204, T#203) -> [T#1287]
  Op#1004 SLICE(T#1286, T#202, T#266) -> [T#1288]
  Op#1005 SLICE(T#1286, T#201, T#266) -> [T#1289]
  Op#1006 RESHAPE(T#1287, T#269) -> [T#1290]
  Op#1007 SLICE(T#1290, T#204, T#200) -> [T#1291]
  Op#1008 SLICE(T#1290, T#199, T#200) -> [T#1292]
  Op#1009 MUL(T#1291, T#290) -> [T#1293]
  Op#1010 MUL(T#1292, T#291) -> [T#1294]
  Op#1011 SUB(T#1293, T#1294) -> [T#1295]
  Op#1012 MUL(T#1292, T#290) -> [T#1296]
  Op#1013 MUL(T#1291, T#291) -> [T#1297]
  Op#1014 ADD(T#1296, T#1297) -> [T#1298]
  Op#1015 CONCATENATION(T#1295, T#1298) -> [T#1299]
  Op#1016 RESHAPE(T#1299, T#268) -> [T#1300]
  Op#1017 RESHAPE(T#1288, T#267) -> [T#1301]
  Op#1018 SLICE(T#1301, T#204, T#198) -> [T#1302]
  Op#1019 SLICE(T#1301, T#199, T#198) -> [T#1303]
  Op#1020 MUL(T#1302, T#290) -> [T#1304]
  Op#1021 MUL(T#1303, T#291) -> [T#1305]
  Op#1022 SUB(T#1304, T#1305) -> [T#1306]
  Op#1023 MUL(T#1303, T#290) -> [T#1307]
  Op#1024 MUL(T#1302, T#291) -> [T#1308]
  Op#1025 ADD(T#1307, T#1308) -> [T#1309]
  Op#1026 CONCATENATION(T#1306, T#1309) -> [T#1310]
  Op#1027 RESHAPE(T#1310, T#266) -> [T#1311]
  Op#1028 DYNAMIC_UPDATE_SLICE(T#25, T#1311, T#339) -> [T#1312]
  Op#1029 DYNAMIC_UPDATE_SLICE(T#21, T#1289, T#339) -> [T#1313]
  Op#1030 STABLEHLO_COMPOSITE(T#1300, T#1312, T#1313, T#303) -> [T#1314]
  Op#1031 RESHAPE(T#1314, T#274) -> [T#1315]
  Op#1032 FULLY_CONNECTED(T#1315, T#106, T#-1) -> [T#1316]
  Op#1033 ADD(T#1277, T#1316) -> [T#1317]
  Op#1034 MUL(T#1317, T#1317) -> [T#1318]
  Op#1035 SUM(T#1318, T#206) -> [T#1319]
  Op#1036 MUL(T#1319, T#205) -> [T#1320]
  Op#1037 ADD(T#1320, T#281) -> [T#1321]
  Op#1038 RSQRT(T#1321) -> [T#1322]
  Op#1039 MUL(T#1317, T#1322) -> [T#1323]
  Op#1040 MUL(T#1323, T#226) -> [T#1324]
  Op#1041 FULLY_CONNECTED(T#1324, T#105, T#-1) -> [T#1325]
  Op#1042 LOGISTIC(T#1325) -> [T#1326]
  Op#1043 MUL(T#1325, T#1326) -> [T#1327]
  Op#1044 FULLY_CONNECTED(T#1324, T#104, T#-1) -> [T#1328]
  Op#1045 MUL(T#1327, T#1328) -> [T#1329]
  Op#1046 FULLY_CONNECTED(T#1329, T#103, T#-1) -> [T#1330]
  Op#1047 ADD(T#1317, T#1330) -> [T#1331]
  Op#1048 MUL(T#1331, T#1331) -> [T#1332]
  Op#1049 SUM(T#1332, T#206) -> [T#1333]
  Op#1050 MUL(T#1333, T#205) -> [T#1334]
  Op#1051 ADD(T#1334, T#281) -> [T#1335]
  Op#1052 RSQRT(T#1335) -> [T#1336]
  Op#1053 MUL(T#1331, T#1336) -> [T#1337]
  Op#1054 MUL(T#1337, T#225) -> [T#1338]
  Op#1055 FULLY_CONNECTED(T#1338, T#102, T#-1) -> [T#1339]
  Op#1056 RESHAPE(T#1339, T#270) -> [T#1340]
  Op#1057 SLICE(T#1340, T#204, T#203) -> [T#1341]
  Op#1058 SLICE(T#1340, T#202, T#266) -> [T#1342]
  Op#1059 SLICE(T#1340, T#201, T#266) -> [T#1343]
  Op#1060 RESHAPE(T#1341, T#269) -> [T#1344]
  Op#1061 SLICE(T#1344, T#204, T#200) -> [T#1345]
  Op#1062 SLICE(T#1344, T#199, T#200) -> [T#1346]
  Op#1063 MUL(T#1345, T#290) -> [T#1347]
  Op#1064 MUL(T#1346, T#291) -> [T#1348]
  Op#1065 SUB(T#1347, T#1348) -> [T#1349]
  Op#1066 MUL(T#1346, T#290) -> [T#1350]
  Op#1067 MUL(T#1345, T#291) -> [T#1351]
  Op#1068 ADD(T#1350, T#1351) -> [T#1352]
  Op#1069 CONCATENATION(T#1349, T#1352) -> [T#1353]
  Op#1070 RESHAPE(T#1353, T#268) -> [T#1354]
  Op#1071 RESHAPE(T#1342, T#267) -> [T#1355]
  Op#1072 SLICE(T#1355, T#204, T#198) -> [T#1356]
  Op#1073 SLICE(T#1355, T#199, T#198) -> [T#1357]
  Op#1074 MUL(T#1356, T#290) -> [T#1358]
  Op#1075 MUL(T#1357, T#291) -> [T#1359]
  Op#1076 SUB(T#1358, T#1359) -> [T#1360]
  Op#1077 MUL(T#1357, T#290) -> [T#1361]
  Op#1078 MUL(T#1356, T#291) -> [T#1362]
  Op#1079 ADD(T#1361, T#1362) -> [T#1363]
  Op#1080 CONCATENATION(T#1360, T#1363) -> [T#1364]
  Op#1081 RESHAPE(T#1364, T#266) -> [T#1365]
  Op#1082 DYNAMIC_UPDATE_SLICE(T#53, T#1365, T#339) -> [T#1366]
  Op#1083 DYNAMIC_UPDATE_SLICE(T#27, T#1343, T#339) -> [T#1367]
  Op#1084 STABLEHLO_COMPOSITE(T#1354, T#1366, T#1367, T#303) -> [T#1368]
  Op#1085 RESHAPE(T#1368, T#274) -> [T#1369]
  Op#1086 FULLY_CONNECTED(T#1369, T#101, T#-1) -> [T#1370]
  Op#1087 ADD(T#1331, T#1370) -> [T#1371]
  Op#1088 MUL(T#1371, T#1371) -> [T#1372]
  Op#1089 SUM(T#1372, T#206) -> [T#1373]
  Op#1090 MUL(T#1373, T#205) -> [T#1374]
  Op#1091 ADD(T#1374, T#281) -> [T#1375]
  Op#1092 RSQRT(T#1375) -> [T#1376]
  Op#1093 MUL(T#1371, T#1376) -> [T#1377]
  Op#1094 MUL(T#1377, T#224) -> [T#1378]
  Op#1095 FULLY_CONNECTED(T#1378, T#100, T#-1) -> [T#1379]
  Op#1096 LOGISTIC(T#1379) -> [T#1380]
  Op#1097 MUL(T#1379, T#1380) -> [T#1381]
  Op#1098 FULLY_CONNECTED(T#1378, T#99, T#-1) -> [T#1382]
  Op#1099 MUL(T#1381, T#1382) -> [T#1383]
  Op#1100 FULLY_CONNECTED(T#1383, T#98, T#-1) -> [T#1384]
  Op#1101 ADD(T#1371, T#1384) -> [T#1385]
  Op#1102 MUL(T#1385, T#1385) -> [T#1386]
  Op#1103 SUM(T#1386, T#206) -> [T#1387]
  Op#1104 MUL(T#1387, T#205) -> [T#1388]
  Op#1105 ADD(T#1388, T#281) -> [T#1389]
  Op#1106 RSQRT(T#1389) -> [T#1390]
  Op#1107 MUL(T#1385, T#1390) -> [T#1391]
  Op#1108 MUL(T#1391, T#223) -> [T#1392]
  Op#1109 FULLY_CONNECTED(T#1392, T#97, T#-1) -> [T#1393]
  Op#1110 RESHAPE(T#1393, T#270) -> [T#1394]
  Op#1111 SLICE(T#1394, T#204, T#203) -> [T#1395]
  Op#1112 SLICE(T#1394, T#202, T#266) -> [T#1396]
  Op#1113 SLICE(T#1394, T#201, T#266) -> [T#1397]
  Op#1114 RESHAPE(T#1395, T#269) -> [T#1398]
  Op#1115 SLICE(T#1398, T#204, T#200) -> [T#1399]
  Op#1116 SLICE(T#1398, T#199, T#200) -> [T#1400]
  Op#1117 MUL(T#1399, T#290) -> [T#1401]
  Op#1118 MUL(T#1400, T#291) -> [T#1402]
  Op#1119 SUB(T#1401, T#1402) -> [T#1403]
  Op#1120 MUL(T#1400, T#290) -> [T#1404]
  Op#1121 MUL(T#1399, T#291) -> [T#1405]
  Op#1122 ADD(T#1404, T#1405) -> [T#1406]
  Op#1123 CONCATENATION(T#1403, T#1406) -> [T#1407]
  Op#1124 RESHAPE(T#1407, T#268) -> [T#1408]
  Op#1125 RESHAPE(T#1396, T#267) -> [T#1409]
  Op#1126 SLICE(T#1409, T#204, T#198) -> [T#1410]
  Op#1127 SLICE(T#1409, T#199, T#198) -> [T#1411]
  Op#1128 MUL(T#1410, T#290) -> [T#1412]
  Op#1129 MUL(T#1411, T#291) -> [T#1413]
  Op#1130 SUB(T#1412, T#1413) -> [T#1414]
  Op#1131 MUL(T#1411, T#290) -> [T#1415]
  Op#1132 MUL(T#1410, T#291) -> [T#1416]
  Op#1133 ADD(T#1415, T#1416) -> [T#1417]
  Op#1134 CONCATENATION(T#1414, T#1417) -> [T#1418]
  Op#1135 RESHAPE(T#1418, T#266) -> [T#1419]
  Op#1136 DYNAMIC_UPDATE_SLICE(T#15, T#1419, T#339) -> [T#1420]
  Op#1137 DYNAMIC_UPDATE_SLICE(T#55, T#1397, T#339) -> [T#1421]
  Op#1138 STABLEHLO_COMPOSITE(T#1408, T#1420, T#1421, T#303) -> [T#1422]
  Op#1139 RESHAPE(T#1422, T#274) -> [T#1423]
  Op#1140 FULLY_CONNECTED(T#1423, T#96, T#-1) -> [T#1424]
  Op#1141 ADD(T#1385, T#1424) -> [T#1425]
  Op#1142 MUL(T#1425, T#1425) -> [T#1426]
  Op#1143 SUM(T#1426, T#206) -> [T#1427]
  Op#1144 MUL(T#1427, T#205) -> [T#1428]
  Op#1145 ADD(T#1428, T#281) -> [T#1429]
  Op#1146 RSQRT(T#1429) -> [T#1430]
  Op#1147 MUL(T#1425, T#1430) -> [T#1431]
  Op#1148 MUL(T#1431, T#222) -> [T#1432]
  Op#1149 FULLY_CONNECTED(T#1432, T#95, T#-1) -> [T#1433]
  Op#1150 LOGISTIC(T#1433) -> [T#1434]
  Op#1151 MUL(T#1433, T#1434) -> [T#1435]
  Op#1152 FULLY_CONNECTED(T#1432, T#94, T#-1) -> [T#1436]
  Op#1153 MUL(T#1435, T#1436) -> [T#1437]
  Op#1154 FULLY_CONNECTED(T#1437, T#93, T#-1) -> [T#1438]
  Op#1155 ADD(T#1425, T#1438) -> [T#1439]
  Op#1156 MUL(T#1439, T#1439) -> [T#1440]
  Op#1157 SUM(T#1440, T#206) -> [T#1441]
  Op#1158 MUL(T#1441, T#205) -> [T#1442]
  Op#1159 ADD(T#1442, T#281) -> [T#1443]
  Op#1160 RSQRT(T#1443) -> [T#1444]
  Op#1161 MUL(T#1439, T#1444) -> [T#1445]
  Op#1162 MUL(T#1445, T#221) -> [T#1446]
  Op#1163 FULLY_CONNECTED(T#1446, T#92, T#-1) -> [T#1447]
  Op#1164 RESHAPE(T#1447, T#270) -> [T#1448]
  Op#1165 SLICE(T#1448, T#204, T#203) -> [T#1449]
  Op#1166 SLICE(T#1448, T#202, T#266) -> [T#1450]
  Op#1167 SLICE(T#1448, T#201, T#266) -> [T#1451]
  Op#1168 RESHAPE(T#1449, T#269) -> [T#1452]
  Op#1169 SLICE(T#1452, T#204, T#200) -> [T#1453]
  Op#1170 SLICE(T#1452, T#199, T#200) -> [T#1454]
  Op#1171 MUL(T#1453, T#290) -> [T#1455]
  Op#1172 MUL(T#1454, T#291) -> [T#1456]
  Op#1173 SUB(T#1455, T#1456) -> [T#1457]
  Op#1174 MUL(T#1454, T#290) -> [T#1458]
  Op#1175 MUL(T#1453, T#291) -> [T#1459]
  Op#1176 ADD(T#1458, T#1459) -> [T#1460]
  Op#1177 CONCATENATION(T#1457, T#1460) -> [T#1461]
  Op#1178 RESHAPE(T#1461, T#268) -> [T#1462]
  Op#1179 RESHAPE(T#1450, T#267) -> [T#1463]
  Op#1180 SLICE(T#1463, T#204, T#198) -> [T#1464]
  Op#1181 SLICE(T#1463, T#199, T#198) -> [T#1465]
  Op#1182 MUL(T#1464, T#290) -> [T#1466]
  Op#1183 MUL(T#1465, T#291) -> [T#1467]
  Op#1184 SUB(T#1466, T#1467) -> [T#1468]
  Op#1185 MUL(T#1465, T#290) -> [T#1469]
  Op#1186 MUL(T#1464, T#291) -> [T#1470]
  Op#1187 ADD(T#1469, T#1470) -> [T#1471]
  Op#1188 CONCATENATION(T#1468, T#1471) -> [T#1472]
  Op#1189 RESHAPE(T#1472, T#266) -> [T#1473]
  Op#1190 DYNAMIC_UPDATE_SLICE(T#43, T#1473, T#339) -> [T#1474]
  Op#1191 DYNAMIC_UPDATE_SLICE(T#37, T#1451, T#339) -> [T#1475]
  Op#1192 STABLEHLO_COMPOSITE(T#1462, T#1474, T#1475, T#303) -> [T#1476]
  Op#1193 RESHAPE(T#1476, T#274) -> [T#1477]
  Op#1194 FULLY_CONNECTED(T#1477, T#91, T#-1) -> [T#1478]
  Op#1195 ADD(T#1439, T#1478) -> [T#1479]
  Op#1196 MUL(T#1479, T#1479) -> [T#1480]
  Op#1197 SUM(T#1480, T#206) -> [T#1481]
  Op#1198 MUL(T#1481, T#205) -> [T#1482]
  Op#1199 ADD(T#1482, T#281) -> [T#1483]
  Op#1200 RSQRT(T#1483) -> [T#1484]
  Op#1201 MUL(T#1479, T#1484) -> [T#1485]
  Op#1202 MUL(T#1485, T#220) -> [T#1486]
  Op#1203 FULLY_CONNECTED(T#1486, T#90, T#-1) -> [T#1487]
  Op#1204 LOGISTIC(T#1487) -> [T#1488]
  Op#1205 MUL(T#1487, T#1488) -> [T#1489]
  Op#1206 FULLY_CONNECTED(T#1486, T#89, T#-1) -> [T#1490]
  Op#1207 MUL(T#1489, T#1490) -> [T#1491]
  Op#1208 FULLY_CONNECTED(T#1491, T#88, T#-1) -> [T#1492]
  Op#1209 ADD(T#1479, T#1492) -> [T#1493]
  Op#1210 MUL(T#1493, T#1493) -> [T#1494]
  Op#1211 SUM(T#1494, T#206) -> [T#1495]
  Op#1212 MUL(T#1495, T#205) -> [T#1496]
  Op#1213 ADD(T#1496, T#281) -> [T#1497]
  Op#1214 RSQRT(T#1497) -> [T#1498]
  Op#1215 MUL(T#1493, T#1498) -> [T#1499]
  Op#1216 MUL(T#1499, T#219) -> [T#1500]
  Op#1217 FULLY_CONNECTED(T#1500, T#87, T#-1) -> [T#1501]
  Op#1218 RESHAPE(T#1501, T#270) -> [T#1502]
  Op#1219 SLICE(T#1502, T#204, T#203) -> [T#1503]
  Op#1220 SLICE(T#1502, T#202, T#266) -> [T#1504]
  Op#1221 SLICE(T#1502, T#201, T#266) -> [T#1505]
  Op#1222 RESHAPE(T#1503, T#269) -> [T#1506]
  Op#1223 SLICE(T#1506, T#204, T#200) -> [T#1507]
  Op#1224 SLICE(T#1506, T#199, T#200) -> [T#1508]
  Op#1225 MUL(T#1507, T#290) -> [T#1509]
  Op#1226 MUL(T#1508, T#291) -> [T#1510]
  Op#1227 SUB(T#1509, T#1510) -> [T#1511]
  Op#1228 MUL(T#1508, T#290) -> [T#1512]
  Op#1229 MUL(T#1507, T#291) -> [T#1513]
  Op#1230 ADD(T#1512, T#1513) -> [T#1514]
  Op#1231 CONCATENATION(T#1511, T#1514) -> [T#1515]
  Op#1232 RESHAPE(T#1515, T#268) -> [T#1516]
  Op#1233 RESHAPE(T#1504, T#267) -> [T#1517]
  Op#1234 SLICE(T#1517, T#204, T#198) -> [T#1518]
  Op#1235 SLICE(T#1517, T#199, T#198) -> [T#1519]
  Op#1236 MUL(T#1518, T#290) -> [T#1520]
  Op#1237 MUL(T#1519, T#291) -> [T#1521]
  Op#1238 SUB(T#1520, T#1521) -> [T#1522]
  Op#1239 MUL(T#1519, T#290) -> [T#1523]
  Op#1240 MUL(T#1518, T#291) -> [T#1524]
  Op#1241 ADD(T#1523, T#1524) -> [T#1525]
  Op#1242 CONCATENATION(T#1522, T#1525) -> [T#1526]
  Op#1243 RESHAPE(T#1526, T#266) -> [T#1527]
  Op#1244 DYNAMIC_UPDATE_SLICE(T#13, T#1527, T#339) -> [T#1528]
  Op#1245 DYNAMIC_UPDATE_SLICE(T#7, T#1505, T#339) -> [T#1529]
  Op#1246 STABLEHLO_COMPOSITE(T#1516, T#1528, T#1529, T#303) -> [T#1530]
  Op#1247 RESHAPE(T#1530, T#274) -> [T#1531]
  Op#1248 FULLY_CONNECTED(T#1531, T#86, T#-1) -> [T#1532]
  Op#1249 ADD(T#1493, T#1532) -> [T#1533]
  Op#1250 MUL(T#1533, T#1533) -> [T#1534]
  Op#1251 SUM(T#1534, T#206) -> [T#1535]
  Op#1252 MUL(T#1535, T#205) -> [T#1536]
  Op#1253 ADD(T#1536, T#281) -> [T#1537]
  Op#1254 RSQRT(T#1537) -> [T#1538]
  Op#1255 MUL(T#1533, T#1538) -> [T#1539]
  Op#1256 MUL(T#1539, T#218) -> [T#1540]
  Op#1257 FULLY_CONNECTED(T#1540, T#85, T#-1) -> [T#1541]
  Op#1258 LOGISTIC(T#1541) -> [T#1542]
  Op#1259 MUL(T#1541, T#1542) -> [T#1543]
  Op#1260 FULLY_CONNECTED(T#1540, T#84, T#-1) -> [T#1544]
  Op#1261 MUL(T#1543, T#1544) -> [T#1545]
  Op#1262 FULLY_CONNECTED(T#1545, T#83, T#-1) -> [T#1546]
  Op#1263 ADD(T#1533, T#1546) -> [T#1547]
  Op#1264 MUL(T#1547, T#1547) -> [T#1548]
  Op#1265 SUM(T#1548, T#206) -> [T#1549]
  Op#1266 MUL(T#1549, T#205) -> [T#1550]
  Op#1267 ADD(T#1550, T#281) -> [T#1551]
  Op#1268 RSQRT(T#1551) -> [T#1552]
  Op#1269 MUL(T#1547, T#1552) -> [T#1553]
  Op#1270 MUL(T#1553, T#217) -> [T#1554]
  Op#1271 FULLY_CONNECTED(T#1554, T#82, T#-1) -> [T#1555]
  Op#1272 RESHAPE(T#1555, T#270) -> [T#1556]
  Op#1273 SLICE(T#1556, T#204, T#203) -> [T#1557]
  Op#1274 SLICE(T#1556, T#202, T#266) -> [T#1558]
  Op#1275 SLICE(T#1556, T#201, T#266) -> [T#1559]
  Op#1276 RESHAPE(T#1557, T#269) -> [T#1560]
  Op#1277 SLICE(T#1560, T#204, T#200) -> [T#1561]
  Op#1278 SLICE(T#1560, T#199, T#200) -> [T#1562]
  Op#1279 MUL(T#1561, T#290) -> [T#1563]
  Op#1280 MUL(T#1562, T#291) -> [T#1564]
  Op#1281 SUB(T#1563, T#1564) -> [T#1565]
  Op#1282 MUL(T#1562, T#290) -> [T#1566]
  Op#1283 MUL(T#1561, T#291) -> [T#1567]
  Op#1284 ADD(T#1566, T#1567) -> [T#1568]
  Op#1285 CONCATENATION(T#1565, T#1568) -> [T#1569]
  Op#1286 RESHAPE(T#1569, T#268) -> [T#1570]
  Op#1287 RESHAPE(T#1558, T#267) -> [T#1571]
  Op#1288 SLICE(T#1571, T#204, T#198) -> [T#1572]
  Op#1289 SLICE(T#1571, T#199, T#198) -> [T#1573]
  Op#1290 MUL(T#1572, T#290) -> [T#1574]
  Op#1291 MUL(T#1573, T#291) -> [T#1575]
  Op#1292 SUB(T#1574, T#1575) -> [T#1576]
  Op#1293 MUL(T#1573, T#290) -> [T#1577]
  Op#1294 MUL(T#1572, T#291) -> [T#1578]
  Op#1295 ADD(T#1577, T#1578) -> [T#1579]
  Op#1296 CONCATENATION(T#1576, T#1579) -> [T#1580]
  Op#1297 RESHAPE(T#1580, T#266) -> [T#1581]
  Op#1298 DYNAMIC_UPDATE_SLICE(T#32, T#1581, T#339) -> [T#1582]
  Op#1299 DYNAMIC_UPDATE_SLICE(T#17, T#1559, T#339) -> [T#1583]
  Op#1300 STABLEHLO_COMPOSITE(T#1570, T#1582, T#1583, T#303) -> [T#1584]
  Op#1301 RESHAPE(T#1584, T#274) -> [T#1585]
  Op#1302 FULLY_CONNECTED(T#1585, T#81, T#-1) -> [T#1586]
  Op#1303 ADD(T#1547, T#1586) -> [T#1587]
  Op#1304 MUL(T#1587, T#1587) -> [T#1588]
  Op#1305 SUM(T#1588, T#206) -> [T#1589]
  Op#1306 MUL(T#1589, T#205) -> [T#1590]
  Op#1307 ADD(T#1590, T#281) -> [T#1591]
  Op#1308 RSQRT(T#1591) -> [T#1592]
  Op#1309 MUL(T#1587, T#1592) -> [T#1593]
  Op#1310 MUL(T#1593, T#216) -> [T#1594]
  Op#1311 FULLY_CONNECTED(T#1594, T#80, T#-1) -> [T#1595]
  Op#1312 LOGISTIC(T#1595) -> [T#1596]
  Op#1313 MUL(T#1595, T#1596) -> [T#1597]
  Op#1314 FULLY_CONNECTED(T#1594, T#79, T#-1) -> [T#1598]
  Op#1315 MUL(T#1597, T#1598) -> [T#1599]
  Op#1316 FULLY_CONNECTED(T#1599, T#78, T#-1) -> [T#1600]
  Op#1317 ADD(T#1587, T#1600) -> [T#1601]
  Op#1318 MUL(T#1601, T#1601) -> [T#1602]
  Op#1319 SUM(T#1602, T#206) -> [T#1603]
  Op#1320 MUL(T#1603, T#205) -> [T#1604]
  Op#1321 ADD(T#1604, T#281) -> [T#1605]
  Op#1322 RSQRT(T#1605) -> [T#1606]
  Op#1323 MUL(T#1601, T#1606) -> [T#1607]
  Op#1324 MUL(T#1607, T#215) -> [T#1608]
  Op#1325 FULLY_CONNECTED(T#1608, T#77, T#-1) -> [T#1609]
  Op#1326 RESHAPE(T#1609, T#270) -> [T#1610]
  Op#1327 SLICE(T#1610, T#204, T#203) -> [T#1611]
  Op#1328 SLICE(T#1610, T#202, T#266) -> [T#1612]
  Op#1329 SLICE(T#1610, T#201, T#266) -> [T#1613]
  Op#1330 RESHAPE(T#1611, T#269) -> [T#1614]
  Op#1331 SLICE(T#1614, T#204, T#200) -> [T#1615]
  Op#1332 SLICE(T#1614, T#199, T#200) -> [T#1616]
  Op#1333 MUL(T#1615, T#290) -> [T#1617]
  Op#1334 MUL(T#1616, T#291) -> [T#1618]
  Op#1335 SUB(T#1617, T#1618) -> [T#1619]
  Op#1336 MUL(T#1616, T#290) -> [T#1620]
  Op#1337 MUL(T#1615, T#291) -> [T#1621]
  Op#1338 ADD(T#1620, T#1621) -> [T#1622]
  Op#1339 CONCATENATION(T#1619, T#1622) -> [T#1623]
  Op#1340 RESHAPE(T#1623, T#268) -> [T#1624]
  Op#1341 RESHAPE(T#1612, T#267) -> [T#1625]
  Op#1342 SLICE(T#1625, T#204, T#198) -> [T#1626]
  Op#1343 SLICE(T#1625, T#199, T#198) -> [T#1627]
  Op#1344 MUL(T#1626, T#290) -> [T#1628]
  Op#1345 MUL(T#1627, T#291) -> [T#1629]
  Op#1346 SUB(T#1628, T#1629) -> [T#1630]
  Op#1347 MUL(T#1627, T#290) -> [T#1631]
  Op#1348 MUL(T#1626, T#291) -> [T#1632]
  Op#1349 ADD(T#1631, T#1632) -> [T#1633]
  Op#1350 CONCATENATION(T#1630, T#1633) -> [T#1634]
  Op#1351 RESHAPE(T#1634, T#266) -> [T#1635]
  Op#1352 DYNAMIC_UPDATE_SLICE(T#9, T#1635, T#339) -> [T#1636]
  Op#1353 DYNAMIC_UPDATE_SLICE(T#44, T#1613, T#339) -> [T#1637]
  Op#1354 STABLEHLO_COMPOSITE(T#1624, T#1636, T#1637, T#303) -> [T#1638]
  Op#1355 RESHAPE(T#1638, T#274) -> [T#1639]
  Op#1356 FULLY_CONNECTED(T#1639, T#76, T#-1) -> [T#1640]
  Op#1357 ADD(T#1601, T#1640) -> [T#1641]
  Op#1358 MUL(T#1641, T#1641) -> [T#1642]
  Op#1359 SUM(T#1642, T#206) -> [T#1643]
  Op#1360 MUL(T#1643, T#205) -> [T#1644]
  Op#1361 ADD(T#1644, T#281) -> [T#1645]
  Op#1362 RSQRT(T#1645) -> [T#1646]
  Op#1363 MUL(T#1641, T#1646) -> [T#1647]
  Op#1364 MUL(T#1647, T#214) -> [T#1648]
  Op#1365 FULLY_CONNECTED(T#1648, T#75, T#-1) -> [T#1649]
  Op#1366 LOGISTIC(T#1649) -> [T#1650]
  Op#1367 MUL(T#1649, T#1650) -> [T#1651]
  Op#1368 FULLY_CONNECTED(T#1648, T#74, T#-1) -> [T#1652]
  Op#1369 MUL(T#1651, T#1652) -> [T#1653]
  Op#1370 FULLY_CONNECTED(T#1653, T#73, T#-1) -> [T#1654]
  Op#1371 ADD(T#1641, T#1654) -> [T#1655]
  Op#1372 MUL(T#1655, T#1655) -> [T#1656]
  Op#1373 SUM(T#1656, T#206) -> [T#1657]
  Op#1374 MUL(T#1657, T#205) -> [T#1658]
  Op#1375 ADD(T#1658, T#281) -> [T#1659]
  Op#1376 RSQRT(T#1659) -> [T#1660]
  Op#1377 MUL(T#1655, T#1660) -> [T#1661]
  Op#1378 MUL(T#1661, T#213) -> [T#1662]
  Op#1379 FULLY_CONNECTED(T#1662, T#72, T#-1) -> [T#1663]
  Op#1380 RESHAPE(T#1663, T#270) -> [T#1664]
  Op#1381 SLICE(T#1664, T#204, T#203) -> [T#1665]
  Op#1382 SLICE(T#1664, T#202, T#266) -> [T#1666]
  Op#1383 SLICE(T#1664, T#201, T#266) -> [T#1667]
  Op#1384 RESHAPE(T#1665, T#269) -> [T#1668]
  Op#1385 SLICE(T#1668, T#204, T#200) -> [T#1669]
  Op#1386 SLICE(T#1668, T#199, T#200) -> [T#1670]
  Op#1387 MUL(T#1669, T#290) -> [T#1671]
  Op#1388 MUL(T#1670, T#291) -> [T#1672]
  Op#1389 SUB(T#1671, T#1672) -> [T#1673]
  Op#1390 MUL(T#1670, T#290) -> [T#1674]
  Op#1391 MUL(T#1669, T#291) -> [T#1675]
  Op#1392 ADD(T#1674, T#1675) -> [T#1676]
  Op#1393 CONCATENATION(T#1673, T#1676) -> [T#1677]
  Op#1394 RESHAPE(T#1677, T#268) -> [T#1678]
  Op#1395 RESHAPE(T#1666, T#267) -> [T#1679]
  Op#1396 SLICE(T#1679, T#204, T#198) -> [T#1680]
  Op#1397 SLICE(T#1679, T#199, T#198) -> [T#1681]
  Op#1398 MUL(T#1680, T#290) -> [T#1682]
  Op#1399 MUL(T#1681, T#291) -> [T#1683]
  Op#1400 SUB(T#1682, T#1683) -> [T#1684]
  Op#1401 MUL(T#1681, T#290) -> [T#1685]
  Op#1402 MUL(T#1680, T#291) -> [T#1686]
  Op#1403 ADD(T#1685, T#1686) -> [T#1687]
  Op#1404 CONCATENATION(T#1684, T#1687) -> [T#1688]
  Op#1405 RESHAPE(T#1688, T#266) -> [T#1689]
  Op#1406 DYNAMIC_UPDATE_SLICE(T#50, T#1689, T#339) -> [T#1690]
  Op#1407 DYNAMIC_UPDATE_SLICE(T#6, T#1667, T#339) -> [T#1691]
  Op#1408 STABLEHLO_COMPOSITE(T#1678, T#1690, T#1691, T#303) -> [T#1692]
  Op#1409 RESHAPE(T#1692, T#274) -> [T#1693]
  Op#1410 FULLY_CONNECTED(T#1693, T#71, T#-1) -> [T#1694]
  Op#1411 ADD(T#1655, T#1694) -> [T#1695]
  Op#1412 MUL(T#1695, T#1695) -> [T#1696]
  Op#1413 SUM(T#1696, T#206) -> [T#1697]
  Op#1414 MUL(T#1697, T#205) -> [T#1698]
  Op#1415 ADD(T#1698, T#281) -> [T#1699]
  Op#1416 RSQRT(T#1699) -> [T#1700]
  Op#1417 MUL(T#1695, T#1700) -> [T#1701]
  Op#1418 MUL(T#1701, T#212) -> [T#1702]
  Op#1419 FULLY_CONNECTED(T#1702, T#70, T#-1) -> [T#1703]
  Op#1420 LOGISTIC(T#1703) -> [T#1704]
  Op#1421 MUL(T#1703, T#1704) -> [T#1705]
  Op#1422 FULLY_CONNECTED(T#1702, T#69, T#-1) -> [T#1706]
  Op#1423 MUL(T#1705, T#1706) -> [T#1707]
  Op#1424 FULLY_CONNECTED(T#1707, T#68, T#-1) -> [T#1708]
  Op#1425 ADD(T#1695, T#1708) -> [T#1709]
  Op#1426 MUL(T#1709, T#1709) -> [T#1710]
  Op#1427 SUM(T#1710, T#206) -> [T#1711]
  Op#1428 MUL(T#1711, T#205) -> [T#1712]
  Op#1429 ADD(T#1712, T#281) -> [T#1713]
  Op#1430 RSQRT(T#1713) -> [T#1714]
  Op#1431 MUL(T#1709, T#1714) -> [T#1715]
  Op#1432 MUL(T#1715, T#211) -> [T#1716]
  Op#1433 FULLY_CONNECTED(T#1716, T#67, T#-1) -> [T#1717]
  Op#1434 RESHAPE(T#1717, T#270) -> [T#1718]
  Op#1435 SLICE(T#1718, T#204, T#203) -> [T#1719]
  Op#1436 SLICE(T#1718, T#202, T#266) -> [T#1720]
  Op#1437 SLICE(T#1718, T#201, T#266) -> [T#1721]
  Op#1438 RESHAPE(T#1719, T#269) -> [T#1722]
  Op#1439 SLICE(T#1722, T#204, T#200) -> [T#1723]
  Op#1440 SLICE(T#1722, T#199, T#200) -> [T#1724]
  Op#1441 MUL(T#1723, T#290) -> [T#1725]
  Op#1442 MUL(T#1724, T#291) -> [T#1726]
  Op#1443 SUB(T#1725, T#1726) -> [T#1727]
  Op#1444 MUL(T#1724, T#290) -> [T#1728]
  Op#1445 MUL(T#1723, T#291) -> [T#1729]
  Op#1446 ADD(T#1728, T#1729) -> [T#1730]
  Op#1447 CONCATENATION(T#1727, T#1730) -> [T#1731]
  Op#1448 RESHAPE(T#1731, T#268) -> [T#1732]
  Op#1449 RESHAPE(T#1720, T#267) -> [T#1733]
  Op#1450 SLICE(T#1733, T#204, T#198) -> [T#1734]
  Op#1451 SLICE(T#1733, T#199, T#198) -> [T#1735]
  Op#1452 MUL(T#1734, T#290) -> [T#1736]
  Op#1453 MUL(T#1735, T#291) -> [T#1737]
  Op#1454 SUB(T#1736, T#1737) -> [T#1738]
  Op#1455 MUL(T#1735, T#290) -> [T#1739]
  Op#1456 MUL(T#1734, T#291) -> [T#1740]
  Op#1457 ADD(T#1739, T#1740) -> [T#1741]
  Op#1458 CONCATENATION(T#1738, T#1741) -> [T#1742]
  Op#1459 RESHAPE(T#1742, T#266) -> [T#1743]
  Op#1460 DYNAMIC_UPDATE_SLICE(T#26, T#1743, T#339) -> [T#1744]
  Op#1461 DYNAMIC_UPDATE_SLICE(T#0, T#1721, T#339) -> [T#1745]
  Op#1462 STABLEHLO_COMPOSITE(T#1732, T#1744, T#1745, T#303) -> [T#1746]
  Op#1463 RESHAPE(T#1746, T#274) -> [T#1747]
  Op#1464 FULLY_CONNECTED(T#1747, T#66, T#-1) -> [T#1748]
  Op#1465 ADD(T#1709, T#1748) -> [T#1749]
  Op#1466 MUL(T#1749, T#1749) -> [T#1750]
  Op#1467 SUM(T#1750, T#206) -> [T#1751]
  Op#1468 MUL(T#1751, T#205) -> [T#1752]
  Op#1469 ADD(T#1752, T#281) -> [T#1753]
  Op#1470 RSQRT(T#1753) -> [T#1754]
  Op#1471 MUL(T#1749, T#1754) -> [T#1755]
  Op#1472 MUL(T#1755, T#210) -> [T#1756]
  Op#1473 FULLY_CONNECTED(T#1756, T#65, T#-1) -> [T#1757]
  Op#1474 LOGISTIC(T#1757) -> [T#1758]
  Op#1475 MUL(T#1757, T#1758) -> [T#1759]
  Op#1476 FULLY_CONNECTED(T#1756, T#64, T#-1) -> [T#1760]
  Op#1477 MUL(T#1759, T#1760) -> [T#1761]
  Op#1478 FULLY_CONNECTED(T#1761, T#63, T#-1) -> [T#1762]
  Op#1479 ADD(T#1749, T#1762) -> [T#1763]
  Op#1480 MUL(T#1763, T#1763) -> [T#1764]
  Op#1481 SUM(T#1764, T#206) -> [T#1765]
  Op#1482 MUL(T#1765, T#205) -> [T#1766]
  Op#1483 ADD(T#1766, T#281) -> [T#1767]
  Op#1484 RSQRT(T#1767) -> [T#1768]
  Op#1485 MUL(T#1763, T#1768) -> [T#1769]
  Op#1486 MUL(T#1769, T#209) -> [T#1770]
  Op#1487 FULLY_CONNECTED(T#1770, T#62, T#-1) -> [T#1771]
  Op#1488 RESHAPE(T#1771, T#270) -> [T#1772]
  Op#1489 SLICE(T#1772, T#204, T#203) -> [T#1773]
  Op#1490 SLICE(T#1772, T#202, T#266) -> [T#1774]
  Op#1491 SLICE(T#1772, T#201, T#266) -> [T#1775]
  Op#1492 RESHAPE(T#1773, T#269) -> [T#1776]
  Op#1493 SLICE(T#1776, T#204, T#200) -> [T#1777]
  Op#1494 SLICE(T#1776, T#199, T#200) -> [T#1778]
  Op#1495 MUL(T#1777, T#290) -> [T#1779]
  Op#1496 MUL(T#1778, T#291) -> [T#1780]
  Op#1497 SUB(T#1779, T#1780) -> [T#1781]
  Op#1498 MUL(T#1778, T#290) -> [T#1782]
  Op#1499 MUL(T#1777, T#291) -> [T#1783]
  Op#1500 ADD(T#1782, T#1783) -> [T#1784]
  Op#1501 CONCATENATION(T#1781, T#1784) -> [T#1785]
  Op#1502 RESHAPE(T#1785, T#268) -> [T#1786]
  Op#1503 RESHAPE(T#1774, T#267) -> [T#1787]
  Op#1504 SLICE(T#1787, T#204, T#198) -> [T#1788]
  Op#1505 SLICE(T#1787, T#199, T#198) -> [T#1789]
  Op#1506 MUL(T#1788, T#290) -> [T#1790]
  Op#1507 MUL(T#1789, T#291) -> [T#1791]
  Op#1508 SUB(T#1790, T#1791) -> [T#1792]
  Op#1509 MUL(T#1789, T#290) -> [T#1793]
  Op#1510 MUL(T#1788, T#291) -> [T#1794]
  Op#1511 ADD(T#1793, T#1794) -> [T#1795]
  Op#1512 CONCATENATION(T#1792, T#1795) -> [T#1796]
  Op#1513 RESHAPE(T#1796, T#266) -> [T#1797]
  Op#1514 DYNAMIC_UPDATE_SLICE(T#56, T#1797, T#339) -> [T#1798]
  Op#1515 DYNAMIC_UPDATE_SLICE(T#12, T#1775, T#339) -> [T#1799]
  Op#1516 STABLEHLO_COMPOSITE(T#1786, T#1798, T#1799, T#303) -> [T#1800]
  Op#1517 RESHAPE(T#1800, T#274) -> [T#1801]
  Op#1518 FULLY_CONNECTED(T#1801, T#61, T#-1) -> [T#1802]
  Op#1519 ADD(T#1763, T#1802) -> [T#1803]
  Op#1520 MUL(T#1803, T#1803) -> [T#1804]
  Op#1521 SUM(T#1804, T#206) -> [T#1805]
  Op#1522 MUL(T#1805, T#205) -> [T#1806]
  Op#1523 ADD(T#1806, T#281) -> [T#1807]
  Op#1524 RSQRT(T#1807) -> [T#1808]
  Op#1525 MUL(T#1803, T#1808) -> [T#1809]
  Op#1526 MUL(T#1809, T#208) -> [T#1810]
  Op#1527 FULLY_CONNECTED(T#1810, T#60, T#-1) -> [T#1811]
  Op#1528 LOGISTIC(T#1811) -> [T#1812]
  Op#1529 MUL(T#1811, T#1812) -> [T#1813]
  Op#1530 FULLY_CONNECTED(T#1810, T#59, T#-1) -> [T#1814]
  Op#1531 MUL(T#1813, T#1814) -> [T#1815]
  Op#1532 FULLY_CONNECTED(T#1815, T#58, T#-1) -> [T#1816]
  Op#1533 ADD(T#1803, T#1816) -> [T#1817]
  Op#1534 MUL(T#1817, T#1817) -> [T#1818]
  Op#1535 SUM(T#1818, T#206) -> [T#1819]
  Op#1536 MUL(T#1819, T#205) -> [T#1820]
  Op#1537 ADD(T#1820, T#281) -> [T#1821]
  Op#1538 RSQRT(T#1821) -> [T#1822]
  Op#1539 MUL(T#1817, T#1822) -> [T#1823]
  Op#1540 MUL(T#1823, T#207) -> [T#1824]
  Op#1541 FULLY_CONNECTED(T#1824, T#283, T#-1) -> [T#1825]

Tensors of Subgraph#0
  T#0(decode_kv_cache_v_26:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1(decode_kv_cache_v_3:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#2(decode_kv_cache_k_2:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#3(decode_kv_cache_k_16:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#4(decode_kv_cache_k_8:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#5(decode_input_pos:0) shape:[1], type:INT32
  T#6(decode_kv_cache_v_25:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#7(decode_kv_cache_v_22:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#8(decode_kv_cache_k_10:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#9(decode_kv_cache_k_24:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#10(decode_kv_cache_k_17:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#11(decode_kv_cache_v_6:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#12(decode_kv_cache_v_27:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#13(decode_kv_cache_k_22:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#14(decode_kv_cache_k_4:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#15(decode_kv_cache_k_20:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#16(decode_kv_cache_k_9:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#17(decode_kv_cache_v_23:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#18(decode_kv_cache_k_3:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#19(decode_kv_cache_v_0:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#20(decode_kv_cache_k_12:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#21(decode_kv_cache_v_18:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#22(decode_kv_cache_v_11:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#23(decode_kv_cache_v_5:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#24(decode_kv_cache_k_6:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#25(decode_kv_cache_k_18:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#26(decode_kv_cache_k_26:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#27(decode_kv_cache_v_19:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#28(decode_kv_cache_k_7:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#29(decode_kv_cache_v_10:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#30(decode_kv_cache_v_16:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#31(decode_kv_cache_v_12:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#32(decode_kv_cache_k_23:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#33(decode_kv_cache_k_14:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#34(decode_kv_cache_v_15:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#35(decode_kv_cache_k_13:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#36(decode_kv_cache_v_7:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#37(decode_kv_cache_v_21:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#38(decode_kv_cache_v_1:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#39(decode_tokens:0) shape:[1, 1], type:INT32
  T#40(decode_kv_cache_v_17:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#41(decode_kv_cache_v_13:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#42(decode_kv_cache_k_0:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#43(decode_kv_cache_k_21:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#44(decode_kv_cache_v_24:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#45(decode_kv_cache_k_15:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#46(decode_kv_cache_v_9:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#47(decode_kv_cache_v_2:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#48(decode_kv_cache_k_1:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#49(decode_kv_cache_v_8:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#50(decode_kv_cache_k_25:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#51(decode_kv_cache_v_4:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#52(decode_kv_cache_k_11:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#53(decode_kv_cache_k_19:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#54(decode_kv_cache_k_5:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#55(decode_kv_cache_v_20:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#56(decode_kv_cache_k_27:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#57(decode_kv_cache_v_14:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#58(arith.constant) shape:[3072, 8192], type:FLOAT32
  T#59(arith.constant1) shape:[8192, 3072], type:FLOAT32
  T#60(arith.constant2) shape:[8192, 3072], type:FLOAT32
  T#61(arith.constant3) shape:[3072, 3072], type:FLOAT32
  T#62(arith.constant4) shape:[5120, 3072], type:FLOAT32
  T#63(arith.constant5) shape:[3072, 8192], type:FLOAT32
  T#64(arith.constant6) shape:[8192, 3072], type:FLOAT32
  T#65(arith.constant7) shape:[8192, 3072], type:FLOAT32
  T#66(arith.constant8) shape:[3072, 3072], type:FLOAT32
  T#67(arith.constant9) shape:[5120, 3072], type:FLOAT32
  T#68(arith.constant10) shape:[3072, 8192], type:FLOAT32
  T#69(arith.constant11) shape:[8192, 3072], type:FLOAT32
  T#70(arith.constant12) shape:[8192, 3072], type:FLOAT32
  T#71(arith.constant13) shape:[3072, 3072], type:FLOAT32
  T#72(arith.constant14) shape:[5120, 3072], type:FLOAT32
  T#73(arith.constant15) shape:[3072, 8192], type:FLOAT32
  T#74(arith.constant16) shape:[8192, 3072], type:FLOAT32
  T#75(arith.constant17) shape:[8192, 3072], type:FLOAT32
  T#76(arith.constant18) shape:[3072, 3072], type:FLOAT32
  T#77(arith.constant19) shape:[5120, 3072], type:FLOAT32
  T#78(arith.constant20) shape:[3072, 8192], type:FLOAT32
  T#79(arith.constant21) shape:[8192, 3072], type:FLOAT32
  T#80(arith.constant22) shape:[8192, 3072], type:FLOAT32
  T#81(arith.constant23) shape:[3072, 3072], type:FLOAT32
  T#82(arith.constant24) shape:[5120, 3072], type:FLOAT32
  T#83(arith.constant25) shape:[3072, 8192], type:FLOAT32
  T#84(arith.constant26) shape:[8192, 3072], type:FLOAT32
  T#85(arith.constant27) shape:[8192, 3072], type:FLOAT32
  T#86(arith.constant28) shape:[3072, 3072], type:FLOAT32
  T#87(arith.constant29) shape:[5120, 3072], type:FLOAT32
  T#88(arith.constant30) shape:[3072, 8192], type:FLOAT32
  T#89(arith.constant31) shape:[8192, 3072], type:FLOAT32
  T#90(arith.constant32) shape:[8192, 3072], type:FLOAT32
  T#91(arith.constant33) shape:[3072, 3072], type:FLOAT32
  T#92(arith.constant34) shape:[5120, 3072], type:FLOAT32
  T#93(arith.constant35) shape:[3072, 8192], type:FLOAT32
  T#94(arith.constant36) shape:[8192, 3072], type:FLOAT32
  T#95(arith.constant37) shape:[8192, 3072], type:FLOAT32
  T#96(arith.constant38) shape:[3072, 3072], type:FLOAT32
  T#97(arith.constant39) shape:[5120, 3072], type:FLOAT32
  T#98(arith.constant40) shape:[3072, 8192], type:FLOAT32
  T#99(arith.constant41) shape:[8192, 3072], type:FLOAT32
  T#100(arith.constant42) shape:[8192, 3072], type:FLOAT32
  T#101(arith.constant43) shape:[3072, 3072], type:FLOAT32
  T#102(arith.constant44) shape:[5120, 3072], type:FLOAT32
  T#103(arith.constant45) shape:[3072, 8192], type:FLOAT32
  T#104(arith.constant46) shape:[8192, 3072], type:FLOAT32
  T#105(arith.constant47) shape:[8192, 3072], type:FLOAT32
  T#106(arith.constant48) shape:[3072, 3072], type:FLOAT32
  T#107(arith.constant49) shape:[5120, 3072], type:FLOAT32
  T#108(arith.constant50) shape:[3072, 8192], type:FLOAT32
  T#109(arith.constant51) shape:[8192, 3072], type:FLOAT32
  T#110(arith.constant52) shape:[8192, 3072], type:FLOAT32
  T#111(arith.constant53) shape:[3072, 3072], type:FLOAT32
  T#112(arith.constant54) shape:[5120, 3072], type:FLOAT32
  T#113(arith.constant55) shape:[3072, 8192], type:FLOAT32
  T#114(arith.constant56) shape:[8192, 3072], type:FLOAT32
  T#115(arith.constant57) shape:[8192, 3072], type:FLOAT32
  T#116(arith.constant58) shape:[3072, 3072], type:FLOAT32
  T#117(arith.constant59) shape:[5120, 3072], type:FLOAT32
  T#118(arith.constant60) shape:[3072, 8192], type:FLOAT32
  T#119(arith.constant61) shape:[8192, 3072], type:FLOAT32
  T#120(arith.constant62) shape:[8192, 3072], type:FLOAT32
  T#121(arith.constant63) shape:[3072, 3072], type:FLOAT32
  T#122(arith.constant64) shape:[5120, 3072], type:FLOAT32
  T#123(arith.constant65) shape:[3072, 8192], type:FLOAT32
  T#124(arith.constant66) shape:[8192, 3072], type:FLOAT32
  T#125(arith.constant67) shape:[8192, 3072], type:FLOAT32
  T#126(arith.constant68) shape:[3072, 3072], type:FLOAT32
  T#127(arith.constant69) shape:[5120, 3072], type:FLOAT32
  T#128(arith.constant70) shape:[3072, 8192], type:FLOAT32
  T#129(arith.constant71) shape:[8192, 3072], type:FLOAT32
  T#130(arith.constant72) shape:[8192, 3072], type:FLOAT32
  T#131(arith.constant73) shape:[3072, 3072], type:FLOAT32
  T#132(arith.constant74) shape:[5120, 3072], type:FLOAT32
  T#133(arith.constant75) shape:[3072, 8192], type:FLOAT32
  T#134(arith.constant76) shape:[8192, 3072], type:FLOAT32
  T#135(arith.constant77) shape:[8192, 3072], type:FLOAT32
  T#136(arith.constant78) shape:[3072, 3072], type:FLOAT32
  T#137(arith.constant79) shape:[5120, 3072], type:FLOAT32
  T#138(arith.constant80) shape:[3072, 8192], type:FLOAT32
  T#139(arith.constant81) shape:[8192, 3072], type:FLOAT32
  T#140(arith.constant82) shape:[8192, 3072], type:FLOAT32
  T#141(arith.constant83) shape:[3072, 3072], type:FLOAT32
  T#142(arith.constant84) shape:[5120, 3072], type:FLOAT32
  T#143(arith.constant85) shape:[3072, 8192], type:FLOAT32
  T#144(arith.constant86) shape:[8192, 3072], type:FLOAT32
  T#145(arith.constant87) shape:[8192, 3072], type:FLOAT32
  T#146(arith.constant88) shape:[3072, 3072], type:FLOAT32
  T#147(arith.constant89) shape:[5120, 3072], type:FLOAT32
  T#148(arith.constant90) shape:[3072, 8192], type:FLOAT32
  T#149(arith.constant91) shape:[8192, 3072], type:FLOAT32
  T#150(arith.constant92) shape:[8192, 3072], type:FLOAT32
  T#151(arith.constant93) shape:[3072, 3072], type:FLOAT32
  T#152(arith.constant94) shape:[5120, 3072], type:FLOAT32
  T#153(arith.constant95) shape:[3072, 8192], type:FLOAT32
  T#154(arith.constant96) shape:[8192, 3072], type:FLOAT32
  T#155(arith.constant97) shape:[8192, 3072], type:FLOAT32
  T#156(arith.constant98) shape:[3072, 3072], type:FLOAT32
  T#157(arith.constant99) shape:[5120, 3072], type:FLOAT32
  T#158(arith.constant100) shape:[3072, 8192], type:FLOAT32
  T#159(arith.constant101) shape:[8192, 3072], type:FLOAT32
  T#160(arith.constant102) shape:[8192, 3072], type:FLOAT32
  T#161(arith.constant103) shape:[3072, 3072], type:FLOAT32
  T#162(arith.constant104) shape:[5120, 3072], type:FLOAT32
  T#163(arith.constant105) shape:[3072, 8192], type:FLOAT32
  T#164(arith.constant106) shape:[8192, 3072], type:FLOAT32
  T#165(arith.constant107) shape:[8192, 3072], type:FLOAT32
  T#166(arith.constant108) shape:[3072, 3072], type:FLOAT32
  T#167(arith.constant109) shape:[5120, 3072], type:FLOAT32
  T#168(arith.constant110) shape:[3072, 8192], type:FLOAT32
  T#169(arith.constant111) shape:[8192, 3072], type:FLOAT32
  T#170(arith.constant112) shape:[8192, 3072], type:FLOAT32
  T#171(arith.constant113) shape:[3072, 3072], type:FLOAT32
  T#172(arith.constant114) shape:[5120, 3072], type:FLOAT32
  T#173(arith.constant115) shape:[3072, 8192], type:FLOAT32
  T#174(arith.constant116) shape:[8192, 3072], type:FLOAT32
  T#175(arith.constant117) shape:[8192, 3072], type:FLOAT32
  T#176(arith.constant118) shape:[3072, 3072], type:FLOAT32
  T#177(arith.constant119) shape:[5120, 3072], type:FLOAT32
  T#178(arith.constant120) shape:[3072, 8192], type:FLOAT32
  T#179(arith.constant121) shape:[8192, 3072], type:FLOAT32
  T#180(arith.constant122) shape:[8192, 3072], type:FLOAT32
  T#181(arith.constant123) shape:[3072, 3072], type:FLOAT32
  T#182(arith.constant124) shape:[5120, 3072], type:FLOAT32
  T#183(arith.constant125) shape:[3072, 8192], type:FLOAT32
  T#184(arith.constant126) shape:[8192, 3072], type:FLOAT32
  T#185(arith.constant127) shape:[8192, 3072], type:FLOAT32
  T#186(arith.constant128) shape:[3072, 3072], type:FLOAT32
  T#187(arith.constant129) shape:[5120, 3072], type:FLOAT32
  T#188(arith.constant130) shape:[3072, 8192], type:FLOAT32
  T#189(arith.constant131) shape:[8192, 3072], type:FLOAT32
  T#190(arith.constant132) shape:[8192, 3072], type:FLOAT32
  T#191(arith.constant133) shape:[3072, 3072], type:FLOAT32
  T#192(arith.constant134) shape:[5120, 3072], type:FLOAT32
  T#193(arith.constant135) shape:[3072, 8192], type:FLOAT32
  T#194(arith.constant136) shape:[8192, 3072], type:FLOAT32
  T#195(arith.constant137) shape:[8192, 3072], type:FLOAT32
  T#196(arith.constant138) shape:[3072, 3072], type:FLOAT32
  T#197(arith.constant139) shape:[5120, 3072], type:FLOAT32
  T#198(arith.constant140) shape:[4], type:INT32
  T#199(arith.constant141) shape:[4], type:INT32
  T#200(arith.constant142) shape:[4], type:INT32
  T#201(arith.constant143) shape:[4], type:INT32
  T#202(arith.constant144) shape:[4], type:INT32
  T#203(arith.constant145) shape:[4], type:INT32
  T#204(arith.constant146) shape:[4], type:INT32
  T#205(arith.constant147) shape:[1, 1, 1], type:FLOAT32
  T#206(arith.constant148) shape:[1], type:INT32
  T#207(arith.constant149) shape:[1, 1, 3072], type:FLOAT32
  T#208(arith.constant150) shape:[1, 1, 3072], type:FLOAT32
  T#209(arith.constant151) shape:[1, 1, 3072], type:FLOAT32
  T#210(arith.constant152) shape:[1, 1, 3072], type:FLOAT32
  T#211(arith.constant153) shape:[1, 1, 3072], type:FLOAT32
  T#212(arith.constant154) shape:[1, 1, 3072], type:FLOAT32
  T#213(arith.constant155) shape:[1, 1, 3072], type:FLOAT32
  T#214(arith.constant156) shape:[1, 1, 3072], type:FLOAT32
  T#215(arith.constant157) shape:[1, 1, 3072], type:FLOAT32
  T#216(arith.constant158) shape:[1, 1, 3072], type:FLOAT32
  T#217(arith.constant159) shape:[1, 1, 3072], type:FLOAT32
  T#218(arith.constant160) shape:[1, 1, 3072], type:FLOAT32
  T#219(arith.constant161) shape:[1, 1, 3072], type:FLOAT32
  T#220(arith.constant162) shape:[1, 1, 3072], type:FLOAT32
  T#221(arith.constant163) shape:[1, 1, 3072], type:FLOAT32
  T#222(arith.constant164) shape:[1, 1, 3072], type:FLOAT32
  T#223(arith.constant165) shape:[1, 1, 3072], type:FLOAT32
  T#224(arith.constant166) shape:[1, 1, 3072], type:FLOAT32
  T#225(arith.constant167) shape:[1, 1, 3072], type:FLOAT32
  T#226(arith.constant168) shape:[1, 1, 3072], type:FLOAT32
  T#227(arith.constant169) shape:[1, 1, 3072], type:FLOAT32
  T#228(arith.constant170) shape:[1, 1, 3072], type:FLOAT32
  T#229(arith.constant171) shape:[1, 1, 3072], type:FLOAT32
  T#230(arith.constant172) shape:[1, 1, 3072], type:FLOAT32
  T#231(arith.constant173) shape:[1, 1, 3072], type:FLOAT32
  T#232(arith.constant174) shape:[1, 1, 3072], type:FLOAT32
  T#233(arith.constant175) shape:[1, 1, 3072], type:FLOAT32
  T#234(arith.constant176) shape:[1, 1, 3072], type:FLOAT32
  T#235(arith.constant177) shape:[1, 1, 3072], type:FLOAT32
  T#236(arith.constant178) shape:[1, 1, 3072], type:FLOAT32
  T#237(arith.constant179) shape:[1, 1, 3072], type:FLOAT32
  T#238(arith.constant180) shape:[1, 1, 3072], type:FLOAT32
  T#239(arith.constant181) shape:[1, 1, 3072], type:FLOAT32
  T#240(arith.constant182) shape:[1, 1, 3072], type:FLOAT32
  T#241(arith.constant183) shape:[1, 1, 3072], type:FLOAT32
  T#242(arith.constant184) shape:[1, 1, 3072], type:FLOAT32
  T#243(arith.constant185) shape:[1, 1, 3072], type:FLOAT32
  T#244(arith.constant186) shape:[1, 1, 3072], type:FLOAT32
  T#245(arith.constant187) shape:[1, 1, 3072], type:FLOAT32
  T#246(arith.constant188) shape:[1, 1, 3072], type:FLOAT32
  T#247(arith.constant189) shape:[1, 1, 3072], type:FLOAT32
  T#248(arith.constant190) shape:[1, 1, 3072], type:FLOAT32
  T#249(arith.constant191) shape:[1, 1, 3072], type:FLOAT32
  T#250(arith.constant192) shape:[1, 1, 3072], type:FLOAT32
  T#251(arith.constant193) shape:[1, 1, 3072], type:FLOAT32
  T#252(arith.constant194) shape:[1, 1, 3072], type:FLOAT32
  T#253(arith.constant195) shape:[1, 1, 3072], type:FLOAT32
  T#254(arith.constant196) shape:[1, 1, 3072], type:FLOAT32
  T#255(arith.constant197) shape:[1, 1, 3072], type:FLOAT32
  T#256(arith.constant198) shape:[1, 1, 3072], type:FLOAT32
  T#257(arith.constant199) shape:[1, 1, 3072], type:FLOAT32
  T#258(arith.constant200) shape:[1, 1, 3072], type:FLOAT32
  T#259(arith.constant201) shape:[1, 1, 3072], type:FLOAT32
  T#260(arith.constant202) shape:[1, 1, 3072], type:FLOAT32
  T#261(arith.constant203) shape:[1, 1, 3072], type:FLOAT32
  T#262(arith.constant204) shape:[1, 1, 3072], type:FLOAT32
  T#263(arith.constant205) shape:[1, 1, 3072], type:FLOAT32
  T#264(arith.constant206) shape:[1280, 1, 1, 1280], type:FLOAT32
  T#265(arith.constant207) shape:[0], type:INT32
  T#266(arith.constant208) shape:[4], type:INT32
  T#267(arith.constant209) shape:[4], type:INT32
  T#268(arith.constant210) shape:[4], type:INT32
  T#269(arith.constant211) shape:[4], type:INT32
  T#270(arith.constant212) shape:[4], type:INT32
  T#271(arith.constant213) shape:[], type:FLOAT32
  T#272(arith.constant214) shape:[1, 64], type:FLOAT32
  T#273(arith.constant215) shape:[2], type:INT32
  T#274(arith.constant216) shape:[3], type:INT32
  T#275(arith.constant217) shape:[4], type:INT32
  T#276(arith.constant218) shape:[1], type:INT32
  T#277(arith.constant219) shape:[1], type:INT32
  T#278(arith.constant220) shape:[1], type:INT32
  T#279(arith.constant221) shape:[1, 1], type:INT64
  T#280(arith.constant222) shape:[1, 1], type:INT64
  T#281(arith.constant223) shape:[1, 1, 1], type:FLOAT32
  T#282(arith.constant224) shape:[], type:INT32
  T#283(arith.constant225) shape:[128256, 3072], type:FLOAT32
  T#284(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;) shape:[1], type:INT32
  T#285(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;1) shape:[1, 3072], type:FLOAT32
  T#286(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;2) shape:[1, 1, 3072], type:FLOAT32
  T#287(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;) shape:[1], type:FLOAT32
  T#288(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;1) shape:[1, 1], type:FLOAT32
  T#289(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;2) shape:[1, 64], type:FLOAT32
  T#290(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;3) shape:[1, 64], type:FLOAT32
  T#291(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;4) shape:[1, 64], type:FLOAT32
  T#292(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;5) shape:[1], type:BOOL
  T#293(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;6) shape:[1], type:INT32
  T#294(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;7) shape:[1], type:INT32
  T#295(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;8) shape:[1, 1], type:INT32
  T#296(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;9) shape:[1, 1], type:INT64
  T#297(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;10) shape:[1, 1], type:BOOL
  T#298(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;11) shape:[1, 1], type:BOOL
  T#299(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;12) shape:[1, 1], type:BOOL
  T#300(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;13) shape:[1], type:BOOL
  T#301(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;14) shape:[1, 1, 1, 1280], type:FLOAT32
  T#302(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;15) shape:[1, 1, 1, 1], type:BOOL
  T#303(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;16) shape:[1, 1, 1, 1280], type:FLOAT32
  T#304(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;) shape:[1, 1, 3072], type:FLOAT32
  T#305(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 1], type:FLOAT32
  T#306(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#307(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#308(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#309(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;1) shape:[1, 1, 3072], type:FLOAT32
  T#310(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 3072], type:FLOAT32
  T#311(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#312(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#313(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#314(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#315(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#316(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#317(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#318(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#319(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#320(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#321(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 64], type:FLOAT32
  T#322(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1, 64], type:FLOAT32
  T#323(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1, 64], type:FLOAT32
  T#324(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1, 64], type:FLOAT32
  T#325(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 24, 1, 128], type:FLOAT32
  T#326(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 1, 24, 128], type:FLOAT32
  T#327(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 128], type:FLOAT32
  T#328(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 64], type:FLOAT32
  T#329(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 8, 1, 64], type:FLOAT32
  T#330(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 8, 1, 64], type:FLOAT32
  T#331(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 8, 1, 64], type:FLOAT32
  T#332(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 8, 1, 64], type:FLOAT32
  T#333(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 8, 1, 64], type:FLOAT32
  T#334(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 8, 1, 64], type:FLOAT32
  T#335(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 8, 1, 64], type:FLOAT32
  T#336(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 8, 1, 128], type:FLOAT32
  T#337(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 1, 8, 128], type:FLOAT32
  T#338(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[], type:INT32
  T#339(tfl.pack) shape:[4], type:INT32
  T#340(StatefulPartitionedCall:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#341(StatefulPartitionedCall:28) shape:[1, 1280, 8, 128], type:FLOAT32
  T#342(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 1, 24, 128], type:FLOAT32
  T#343(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#344(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#345(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;) shape:[1, 1, 3072], type:FLOAT32
  T#346(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#347(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#348(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#349(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#350(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#351(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#352(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#353(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#354(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#355(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#356(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#357(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#358(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#359(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0;) shape:[1, 1, 3072], type:FLOAT32
  T#360(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#361(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#362(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#363(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#364(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#365(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#366(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#367(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#368(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#369(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#370(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#371(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#372(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#373(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#374(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#375(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#376(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#377(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#378(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#379(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#380(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#381(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#382(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#383(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#384(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#385(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#386(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#387(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#388(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#389(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#390(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#391(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#392(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#393(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#394(StatefulPartitionedCall:1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#395(StatefulPartitionedCall:29) shape:[1, 1280, 8, 128], type:FLOAT32
  T#396(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#397(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#398(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#399(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1;) shape:[1, 1, 3072], type:FLOAT32
  T#400(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#401(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#402(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#403(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#404(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#405(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#406(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#407(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#408(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#409(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#410(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#411(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#412(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#413(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1;1) shape:[1, 1, 3072], type:FLOAT32
  T#414(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#415(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#416(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#417(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#418(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#419(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#420(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#421(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#422(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#423(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#424(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#425(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#426(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#427(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#428(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#429(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#430(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#431(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#432(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#433(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#434(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#435(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#436(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#437(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#438(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#439(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#440(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#441(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#442(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#443(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#444(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#445(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#446(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#447(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#448(StatefulPartitionedCall:12) shape:[1, 1280, 8, 128], type:FLOAT32
  T#449(StatefulPartitionedCall:40) shape:[1, 1280, 8, 128], type:FLOAT32
  T#450(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#451(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#452(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#453(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2;) shape:[1, 1, 3072], type:FLOAT32
  T#454(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#455(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#456(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#457(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#458(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#459(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#460(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#461(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#462(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#463(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#464(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#465(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#466(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#467(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2;1) shape:[1, 1, 3072], type:FLOAT32
  T#468(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#469(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#470(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#471(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#472(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#473(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#474(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#475(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#476(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#477(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#478(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#479(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#480(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#481(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#482(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#483(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#484(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#485(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#486(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#487(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#488(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#489(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#490(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#491(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#492(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#493(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#494(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#495(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#496(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#497(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#498(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#499(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#500(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#501(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#502(StatefulPartitionedCall:21) shape:[1, 1280, 8, 128], type:FLOAT32
  T#503(StatefulPartitionedCall:49) shape:[1, 1280, 8, 128], type:FLOAT32
  T#504(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#505(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#506(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#507(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3;) shape:[1, 1, 3072], type:FLOAT32
  T#508(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#509(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#510(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#511(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#512(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#513(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#514(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#515(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#516(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#517(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#518(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#519(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#520(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#521(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3;1) shape:[1, 1, 3072], type:FLOAT32
  T#522(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#523(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#524(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#525(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#526(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#527(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#528(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#529(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#530(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#531(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#532(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#533(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#534(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#535(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#536(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#537(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#538(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#539(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#540(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#541(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#542(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#543(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#544(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#545(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#546(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#547(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#548(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#549(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#550(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#551(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#552(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#553(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#554(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#555(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#556(StatefulPartitionedCall:22) shape:[1, 1280, 8, 128], type:FLOAT32
  T#557(StatefulPartitionedCall:50) shape:[1, 1280, 8, 128], type:FLOAT32
  T#558(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#559(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#560(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#561(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4;) shape:[1, 1, 3072], type:FLOAT32
  T#562(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#563(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#564(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#565(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#566(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#567(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#568(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#569(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#570(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#571(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#572(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#573(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#574(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#575(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4;1) shape:[1, 1, 3072], type:FLOAT32
  T#576(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#577(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#578(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#579(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#580(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#581(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#582(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#583(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#584(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#585(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#586(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#587(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#588(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#589(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#590(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#591(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#592(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#593(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#594(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#595(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#596(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#597(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#598(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#599(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#600(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#601(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#602(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#603(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#604(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#605(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#606(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#607(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#608(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#609(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#610(StatefulPartitionedCall:23) shape:[1, 1280, 8, 128], type:FLOAT32
  T#611(StatefulPartitionedCall:51) shape:[1, 1280, 8, 128], type:FLOAT32
  T#612(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#613(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#614(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#615(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5;) shape:[1, 1, 3072], type:FLOAT32
  T#616(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#617(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#618(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#619(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#620(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#621(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#622(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#623(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#624(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#625(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#626(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#627(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#628(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#629(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5;1) shape:[1, 1, 3072], type:FLOAT32
  T#630(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#631(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#632(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#633(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#634(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#635(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#636(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#637(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#638(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#639(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#640(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#641(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#642(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#643(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#644(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#645(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#646(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#647(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#648(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#649(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#650(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#651(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#652(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#653(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#654(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#655(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#656(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#657(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#658(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#659(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#660(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#661(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#662(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#663(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#664(StatefulPartitionedCall:24) shape:[1, 1280, 8, 128], type:FLOAT32
  T#665(StatefulPartitionedCall:52) shape:[1, 1280, 8, 128], type:FLOAT32
  T#666(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#667(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#668(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#669(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6;) shape:[1, 1, 3072], type:FLOAT32
  T#670(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#671(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#672(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#673(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#674(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#675(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#676(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#677(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#678(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#679(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#680(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#681(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#682(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#683(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6;1) shape:[1, 1, 3072], type:FLOAT32
  T#684(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#685(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#686(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#687(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#688(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#689(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#690(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#691(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#692(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#693(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#694(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#695(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#696(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#697(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#698(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#699(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#700(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#701(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#702(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#703(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#704(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#705(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#706(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#707(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#708(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#709(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#710(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#711(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#712(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#713(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#714(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#715(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#716(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#717(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#718(StatefulPartitionedCall:25) shape:[1, 1280, 8, 128], type:FLOAT32
  T#719(StatefulPartitionedCall:53) shape:[1, 1280, 8, 128], type:FLOAT32
  T#720(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#721(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#722(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#723(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7;) shape:[1, 1, 3072], type:FLOAT32
  T#724(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#725(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#726(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#727(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#728(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#729(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#730(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#731(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#732(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#733(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#734(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#735(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#736(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#737(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7;1) shape:[1, 1, 3072], type:FLOAT32
  T#738(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#739(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#740(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#741(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#742(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#743(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#744(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#745(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#746(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#747(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#748(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#749(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#750(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#751(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#752(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#753(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#754(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#755(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#756(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#757(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#758(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#759(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#760(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#761(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#762(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#763(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#764(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#765(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#766(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#767(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#768(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#769(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#770(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#771(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#772(StatefulPartitionedCall:26) shape:[1, 1280, 8, 128], type:FLOAT32
  T#773(StatefulPartitionedCall:54) shape:[1, 1280, 8, 128], type:FLOAT32
  T#774(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#775(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#776(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#777(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8;) shape:[1, 1, 3072], type:FLOAT32
  T#778(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#779(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#780(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#781(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#782(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#783(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#784(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#785(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#786(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#787(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#788(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#789(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#790(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#791(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8;1) shape:[1, 1, 3072], type:FLOAT32
  T#792(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#793(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#794(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#795(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#796(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#797(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#798(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#799(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#800(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#801(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#802(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#803(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#804(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#805(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#806(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#807(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#808(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#809(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#810(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#811(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#812(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#813(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#814(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#815(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#816(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#817(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#818(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#819(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#820(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#821(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#822(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#823(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#824(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#825(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#826(StatefulPartitionedCall:27) shape:[1, 1280, 8, 128], type:FLOAT32
  T#827(StatefulPartitionedCall:55) shape:[1, 1280, 8, 128], type:FLOAT32
  T#828(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#829(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#830(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#831(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9;) shape:[1, 1, 3072], type:FLOAT32
  T#832(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#833(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#834(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#835(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#836(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#837(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#838(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#839(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#840(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#841(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#842(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#843(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#844(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#845(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9;1) shape:[1, 1, 3072], type:FLOAT32
  T#846(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#847(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#848(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#849(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#850(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#851(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#852(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#853(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#854(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#855(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#856(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#857(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#858(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#859(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#860(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#861(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#862(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#863(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#864(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#865(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#866(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#867(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#868(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#869(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#870(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#871(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#872(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#873(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#874(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#875(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#876(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#877(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#878(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#879(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#880(StatefulPartitionedCall:2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#881(StatefulPartitionedCall:30) shape:[1, 1280, 8, 128], type:FLOAT32
  T#882(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#883(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#884(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#885(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10;) shape:[1, 1, 3072], type:FLOAT32
  T#886(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#887(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#888(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#889(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#890(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#891(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#892(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#893(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#894(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#895(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#896(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#897(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#898(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#899(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10;1) shape:[1, 1, 3072], type:FLOAT32
  T#900(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#901(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#902(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#903(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#904(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#905(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#906(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#907(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#908(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#909(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#910(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#911(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#912(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#913(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#914(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#915(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#916(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#917(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#918(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#919(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#920(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#921(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#922(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#923(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#924(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#925(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#926(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#927(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#928(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#929(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#930(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#931(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#932(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#933(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#934(StatefulPartitionedCall:3) shape:[1, 1280, 8, 128], type:FLOAT32
  T#935(StatefulPartitionedCall:31) shape:[1, 1280, 8, 128], type:FLOAT32
  T#936(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#937(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#938(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#939(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11;) shape:[1, 1, 3072], type:FLOAT32
  T#940(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#941(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#942(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#943(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#944(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#945(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#946(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#947(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#948(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#949(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#950(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#951(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#952(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#953(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11;1) shape:[1, 1, 3072], type:FLOAT32
  T#954(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#955(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#956(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#957(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#958(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#959(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#960(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#961(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#962(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#963(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#964(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#965(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#966(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#967(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#968(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#969(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#970(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#971(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#972(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#973(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#974(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#975(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#976(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#977(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#978(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#979(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#980(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#981(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#982(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#983(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#984(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#985(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#986(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#987(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#988(StatefulPartitionedCall:4) shape:[1, 1280, 8, 128], type:FLOAT32
  T#989(StatefulPartitionedCall:32) shape:[1, 1280, 8, 128], type:FLOAT32
  T#990(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#991(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#992(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#993(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12;) shape:[1, 1, 3072], type:FLOAT32
  T#994(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#995(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#996(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#997(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#998(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#999(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1000(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1001(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1002(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1003(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1004(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1005(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1006(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1007(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12;1) shape:[1, 1, 3072], type:FLOAT32
  T#1008(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1009(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1010(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1011(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1012(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1013(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1014(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1015(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1016(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1017(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1018(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1019(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1020(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1021(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1022(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1023(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1024(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1025(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1026(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1027(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1028(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1029(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1030(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1031(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1032(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1033(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1034(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1035(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1036(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1037(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1038(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1039(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1040(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1041(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1042(StatefulPartitionedCall:5) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1043(StatefulPartitionedCall:33) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1044(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1045(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1046(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1047(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13;) shape:[1, 1, 3072], type:FLOAT32
  T#1048(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1049(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1050(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1051(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1052(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1053(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1054(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1055(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1056(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1057(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1058(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1059(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1060(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1061(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13;1) shape:[1, 1, 3072], type:FLOAT32
  T#1062(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1063(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1064(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1065(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1066(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1067(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1068(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1069(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1070(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1071(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1072(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1073(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1074(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1075(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1076(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1077(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1078(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1079(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1080(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1081(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1082(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1083(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1084(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1085(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1086(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1087(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1088(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1089(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1090(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1091(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1092(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1093(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1094(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1095(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1096(StatefulPartitionedCall:6) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1097(StatefulPartitionedCall:34) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1098(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1099(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1100(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1101(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14;) shape:[1, 1, 3072], type:FLOAT32
  T#1102(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1103(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1104(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1105(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1106(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1107(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1108(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1109(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1110(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1111(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1112(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1113(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1114(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1115(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14;1) shape:[1, 1, 3072], type:FLOAT32
  T#1116(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1117(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1118(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1119(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1120(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1121(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1122(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1123(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1124(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1125(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1126(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1127(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1128(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1129(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1130(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1131(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1132(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1133(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1134(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1135(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1136(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1137(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1138(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1139(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1140(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1141(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1142(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1143(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1144(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1145(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1146(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1147(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1148(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1149(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1150(StatefulPartitionedCall:7) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1151(StatefulPartitionedCall:35) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1152(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1153(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1154(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1155(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15;) shape:[1, 1, 3072], type:FLOAT32
  T#1156(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1157(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1158(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1159(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1160(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1161(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1162(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1163(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1164(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1165(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1166(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1167(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1168(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1169(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15;1) shape:[1, 1, 3072], type:FLOAT32
  T#1170(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1171(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1172(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1173(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1174(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1175(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1176(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1177(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1178(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1179(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1180(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1181(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1182(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1183(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1184(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1185(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1186(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1187(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1188(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1189(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1190(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1191(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1192(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1193(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1194(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1195(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1196(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1197(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1198(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1199(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1200(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1201(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1202(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1203(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1204(StatefulPartitionedCall:8) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1205(StatefulPartitionedCall:36) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1206(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1207(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1208(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1209(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16;) shape:[1, 1, 3072], type:FLOAT32
  T#1210(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1211(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1212(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1213(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1214(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1215(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1216(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1217(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1218(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1219(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1220(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1221(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1222(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1223(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16;1) shape:[1, 1, 3072], type:FLOAT32
  T#1224(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1225(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1226(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1227(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1228(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1229(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1230(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1231(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1232(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1233(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1234(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1235(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1236(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1237(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1238(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1239(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1240(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1241(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1242(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1243(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1244(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1245(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1246(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1247(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1248(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1249(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1250(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1251(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1252(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1253(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1254(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1255(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1256(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1257(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1258(StatefulPartitionedCall:9) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1259(StatefulPartitionedCall:37) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1260(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1261(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1262(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1263(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17;) shape:[1, 1, 3072], type:FLOAT32
  T#1264(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1265(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1266(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1267(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1268(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1269(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1270(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1271(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1272(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1273(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1274(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1275(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1276(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1277(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17;1) shape:[1, 1, 3072], type:FLOAT32
  T#1278(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1279(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1280(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1281(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1282(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1283(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1284(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1285(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1286(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1287(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1288(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1289(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1290(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1291(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1292(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1293(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1294(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1295(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1296(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1297(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1298(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1299(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1300(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1301(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1302(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1303(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1304(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1305(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1306(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1307(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1308(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1309(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1310(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1311(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1312(StatefulPartitionedCall:10) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1313(StatefulPartitionedCall:38) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1314(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1315(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1316(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1317(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18;) shape:[1, 1, 3072], type:FLOAT32
  T#1318(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1319(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1320(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1321(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1322(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1323(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1324(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1325(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1326(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1327(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1328(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1329(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1330(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1331(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18;1) shape:[1, 1, 3072], type:FLOAT32
  T#1332(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1333(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1334(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1335(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1336(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1337(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1338(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1339(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1340(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1341(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1342(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1343(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1344(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1345(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1346(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1347(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1348(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1349(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1350(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1351(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1352(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1353(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1354(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1355(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1356(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1357(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1358(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1359(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1360(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1361(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1362(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1363(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1364(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1365(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1366(StatefulPartitionedCall:11) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1367(StatefulPartitionedCall:39) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1368(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1369(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1370(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1371(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19;) shape:[1, 1, 3072], type:FLOAT32
  T#1372(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1373(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1374(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1375(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1376(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1377(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1378(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1379(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1380(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1381(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1382(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1383(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1384(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1385(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19;1) shape:[1, 1, 3072], type:FLOAT32
  T#1386(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1387(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1388(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1389(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1390(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1391(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1392(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1393(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1394(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1395(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1396(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1397(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1398(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1399(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1400(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1401(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1402(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1403(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1404(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1405(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1406(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1407(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1408(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1409(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1410(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1411(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1412(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1413(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1414(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1415(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1416(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1417(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1418(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1419(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1420(StatefulPartitionedCall:13) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1421(StatefulPartitionedCall:41) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1422(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1423(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1424(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1425(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20;) shape:[1, 1, 3072], type:FLOAT32
  T#1426(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1427(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1428(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1429(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1430(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1431(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1432(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1433(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1434(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1435(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1436(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1437(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1438(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1439(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20;1) shape:[1, 1, 3072], type:FLOAT32
  T#1440(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1441(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1442(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1443(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1444(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1445(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1446(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1447(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1448(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1449(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1450(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1451(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1452(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1453(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1454(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1455(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1456(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1457(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1458(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1459(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1460(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1461(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1462(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1463(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1464(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1465(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1466(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1467(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1468(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1469(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1470(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1471(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1472(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1473(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1474(StatefulPartitionedCall:14) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1475(StatefulPartitionedCall:42) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1476(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1477(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1478(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1479(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21;) shape:[1, 1, 3072], type:FLOAT32
  T#1480(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1481(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1482(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1483(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1484(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1485(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1486(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1487(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1488(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1489(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1490(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1491(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1492(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1493(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21;1) shape:[1, 1, 3072], type:FLOAT32
  T#1494(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1495(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1496(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1497(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1498(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1499(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1500(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1501(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1502(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1503(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1504(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1505(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1506(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1507(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1508(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1509(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1510(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1511(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1512(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1513(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1514(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1515(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1516(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1517(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1518(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1519(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1520(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1521(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1522(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1523(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1524(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1525(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1526(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1527(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1528(StatefulPartitionedCall:15) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1529(StatefulPartitionedCall:43) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1530(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1531(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1532(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1533(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22;) shape:[1, 1, 3072], type:FLOAT32
  T#1534(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1535(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1536(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1537(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1538(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1539(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1540(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1541(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1542(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1543(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1544(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1545(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1546(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1547(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22;1) shape:[1, 1, 3072], type:FLOAT32
  T#1548(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1549(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1550(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1551(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1552(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1553(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1554(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1555(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1556(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1557(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1558(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1559(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1560(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1561(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1562(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1563(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1564(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1565(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1566(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1567(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1568(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1569(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1570(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1571(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1572(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1573(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1574(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1575(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1576(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1577(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1578(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1579(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1580(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1581(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1582(StatefulPartitionedCall:16) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1583(StatefulPartitionedCall:44) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1584(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1585(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1586(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1587(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23;) shape:[1, 1, 3072], type:FLOAT32
  T#1588(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1589(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1590(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1591(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1592(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1593(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1594(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1595(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1596(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1597(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1598(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1599(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1600(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1601(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23;1) shape:[1, 1, 3072], type:FLOAT32
  T#1602(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1603(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1604(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1605(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1606(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1607(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1608(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1609(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1610(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1611(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1612(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1613(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1614(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1615(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1616(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1617(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1618(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1619(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1620(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1621(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1622(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1623(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1624(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1625(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1626(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1627(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1628(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1629(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1630(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1631(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1632(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1633(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1634(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1635(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1636(StatefulPartitionedCall:17) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1637(StatefulPartitionedCall:45) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1638(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1639(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1640(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1641(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24;) shape:[1, 1, 3072], type:FLOAT32
  T#1642(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1643(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1644(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1645(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1646(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1647(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1648(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1649(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1650(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1651(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1652(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1653(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1654(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1655(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24;1) shape:[1, 1, 3072], type:FLOAT32
  T#1656(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1657(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1658(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1659(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1660(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1661(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1662(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1663(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1664(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1665(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1666(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1667(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1668(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1669(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1670(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1671(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1672(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1673(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1674(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1675(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1676(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1677(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1678(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1679(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1680(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1681(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1682(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1683(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1684(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1685(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1686(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1687(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1688(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1689(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1690(StatefulPartitionedCall:18) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1691(StatefulPartitionedCall:46) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1692(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1693(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1694(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1695(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25;) shape:[1, 1, 3072], type:FLOAT32
  T#1696(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1697(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1698(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1699(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1700(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1701(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1702(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1703(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1704(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1705(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1706(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1707(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1708(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1709(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25;1) shape:[1, 1, 3072], type:FLOAT32
  T#1710(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1711(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1712(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1713(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1714(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1715(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1716(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1717(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1718(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1719(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1720(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1721(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1722(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1723(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1724(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1725(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1726(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1727(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1728(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1729(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1730(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1731(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1732(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1733(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1734(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1735(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1736(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1737(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1738(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1739(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1740(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1741(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1742(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1743(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1744(StatefulPartitionedCall:19) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1745(StatefulPartitionedCall:47) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1746(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1747(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1748(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1749(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26;) shape:[1, 1, 3072], type:FLOAT32
  T#1750(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1751(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1752(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1753(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1754(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1755(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1756(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1757(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1758(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1759(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1760(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1761(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1762(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1763(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26;1) shape:[1, 1, 3072], type:FLOAT32
  T#1764(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1765(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1766(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1767(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1768(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1769(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1770(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1771(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;) shape:[1, 1, 5120], type:FLOAT32
  T#1772(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;1) shape:[1, 1, 8, 640], type:FLOAT32
  T#1773(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 1, 8, 384], type:FLOAT32
  T#1774(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 1, 8, 128], type:FLOAT32
  T#1775(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 1, 8, 128], type:FLOAT32
  T#1776(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 128], type:FLOAT32
  T#1777(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 24, 1, 64], type:FLOAT32
  T#1778(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 24, 1, 64], type:FLOAT32
  T#1779(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;) shape:[1, 24, 1, 64], type:FLOAT32
  T#1780(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;1) shape:[1, 24, 1, 64], type:FLOAT32
  T#1781(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 24, 1, 64], type:FLOAT32
  T#1782(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;2) shape:[1, 24, 1, 64], type:FLOAT32
  T#1783(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;3) shape:[1, 24, 1, 64], type:FLOAT32
  T#1784(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 24, 1, 64], type:FLOAT32
  T#1785(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1, 128], type:FLOAT32
  T#1786(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 1, 24, 128], type:FLOAT32
  T#1787(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1, 128], type:FLOAT32
  T#1788(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1, 64], type:FLOAT32
  T#1789(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1, 64], type:FLOAT32
  T#1790(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;4) shape:[1, 8, 1, 64], type:FLOAT32
  T#1791(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;5) shape:[1, 8, 1, 64], type:FLOAT32
  T#1792(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1, 64], type:FLOAT32
  T#1793(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;6) shape:[1, 8, 1, 64], type:FLOAT32
  T#1794(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;7) shape:[1, 8, 1, 64], type:FLOAT32
  T#1795(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1, 64], type:FLOAT32
  T#1796(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1, 128], type:FLOAT32
  T#1797(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;16) shape:[1, 1, 8, 128], type:FLOAT32
  T#1798(StatefulPartitionedCall:20) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1799(StatefulPartitionedCall:48) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1800(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;17) shape:[1, 1, 24, 128], type:FLOAT32
  T#1801(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;) shape:[1, 1, 3072], type:FLOAT32
  T#1802(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;1) shape:[1, 1, 3072], type:FLOAT32
  T#1803(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27;) shape:[1, 1, 3072], type:FLOAT32
  T#1804(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1805(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1806(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1807(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1808(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1809(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1810(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1811(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1812(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;) shape:[1, 1, 8192], type:FLOAT32
  T#1813(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;1) shape:[1, 1, 8192], type:FLOAT32
  T#1814(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;) shape:[1, 1, 8192], type:FLOAT32
  T#1815(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;2) shape:[1, 1, 8192], type:FLOAT32
  T#1816(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;) shape:[1, 1, 3072], type:FLOAT32
  T#1817(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27;1) shape:[1, 1, 3072], type:FLOAT32
  T#1818(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.normalization.RMSNorm_final_norm;) shape:[1, 1, 3072], type:FLOAT32
  T#1819(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.normalization.RMSNorm_final_norm;1) shape:[1, 1, 1], type:FLOAT32
  T#1820(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.normalization.RMSNorm_final_norm;2) shape:[1, 1, 1], type:FLOAT32
  T#1821(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.normalization.RMSNorm_final_norm;3) shape:[1, 1, 1], type:FLOAT32
  T#1822(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.normalization.RMSNorm_final_norm;4) shape:[1, 1, 1], type:FLOAT32
  T#1823(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.normalization.RMSNorm_final_norm;5) shape:[1, 1, 3072], type:FLOAT32
  T#1824(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.normalization.RMSNorm_final_norm;6) shape:[1, 1, 3072], type:FLOAT32
  T#1825(StatefulPartitionedCall:56) shape:[1, 1, 128256], type:FLOAT32

Subgraph#1 prefill_1024(T#1_0, T#1_1, T#1_2, T#1_3, T#1_4, T#1_5, T#1_6, T#1_7, T#1_8, T#1_9, T#1_10, T#1_11, T#1_12, T#1_13, T#1_14, T#1_15, T#1_16, T#1_17, T#1_18, T#1_19, T#1_20, T#1_21, T#1_22, T#1_23, T#1_24, T#1_25, T#1_26, T#1_27, T#1_28, T#1_29, T#1_30, T#1_31, T#1_32, T#1_33, T#1_34, T#1_35, T#1_36, T#1_37, T#1_38, T#1_39, T#1_40, T#1_41, T#1_42, T#1_43, T#1_44, T#1_45, T#1_46, T#1_47, T#1_48, T#1_49, T#1_50, T#1_51, T#1_52, T#1_53, T#1_54, T#1_55, T#1_56, T#1_57) -> [T#1_1823, T#1_512, T#1_454, T#1_1252, T#1_796, T#1_1766, T#1_1595, T#1_910, T#1_1708, T#1_1309, T#1_683, T#1_1867, T#1_1594, T#1_568, T#1_1480, T#1_853, T#1_1652, T#1_511, T#1_341, T#1_1024, T#1_1367, T#1_968, T#1_626, T#1_682, T#1_1366, T#1_1822, T#1_1424, T#1_739, T#1_911, T#1_1253, T#1_1025, T#1_1651, T#1_1138, T#1_1196, T#1_1081, T#1_740, T#1_1538, T#1_398, T#1_1310, T#1_1082, T#1_340, T#1_1537, T#1_1709, T#1_1195, T#1_854, T#1_455, T#1_397, T#1_797, T#1_1765, T#1_569, T#1_967, T#1_1423, T#1_625, T#1_1481, T#1_1866, T#1_1139]
  Op#0 RESHAPE(T#1_39, T#1_272) -> [T#1_280]
  Op#1 EMBEDDING_LOOKUP(T#1_280, T#1_279) -> [T#1_281]
  Op#2 RESHAPE(T#1_281, T#1_271) -> [T#1_282]
  Op#3 CAST(T#1_5) -> [T#1_283]
  Op#4 RESHAPE(T#1_283, T#1_270) -> [T#1_284]
  Op#5 MUL(T#1_284, T#1_269) -> [T#1_285]
  Op#6 COS(T#1_285) -> [T#1_286]
  Op#7 SIN(T#1_285) -> [T#1_287]
  Op#8 LESS(T#1_5, T#1_276) -> [T#1_288]
  Op#9 ADD(T#1_5, T#1_278) -> [T#1_289]
  Op#10 SELECT(T#1_288, T#1_289, T#1_5) -> [T#1_290]
  Op#11 RESHAPE(T#1_290, T#1_270) -> [T#1_291]
  Op#12 CAST(T#1_291) -> [T#1_292]
  Op#13 GREATER_EQUAL(T#1_292, T#1_268) -> [T#1_293]
  Op#14 LESS_EQUAL(T#1_292, T#1_267) -> [T#1_294]
  Op#15 LOGICAL_AND(T#1_293, T#1_294) -> [T#1_295]
  Op#16 REDUCE_ALL(T#1_295, T#1_275) -> [T#1_296]
  Op#17 GATHER_ND(T#1_260, T#1_292) -> [T#1_297]
  Op#18 RESHAPE(T#1_297, T#1_204) -> [T#1_298]
  Op#19 RESHAPE(T#1_296, T#1_274) -> [T#1_299]
  Op#20 SELECT_V2(T#1_299, T#1_298, T#1_266) -> [T#1_300]
  Op#21 MUL(T#1_281, T#1_282) -> [T#1_301]
  Op#22 SUM(T#1_301, T#1_273) -> [T#1_302]
  Op#23 MUL(T#1_302, T#1_203) -> [T#1_303]
  Op#24 ADD(T#1_303, T#1_277) -> [T#1_304]
  Op#25 RESHAPE(T#1_304, T#1_265) -> [T#1_305]
  Op#26 RSQRT(T#1_305) -> [T#1_306]
  Op#27 MUL(T#1_281, T#1_306) -> [T#1_307]
  Op#28 MUL(T#1_307, T#1_259) -> [T#1_308]
  Op#29 FULLY_CONNECTED(T#1_308, T#1_193, T#-1) -> [T#1_309]
  Op#30 RESHAPE(T#1_309, T#1_264) -> [T#1_310]
  Op#31 SLICE(T#1_310, T#1_202, T#1_201) -> [T#1_311]
  Op#32 SLICE(T#1_310, T#1_200, T#1_199) -> [T#1_312]
  Op#33 SLICE(T#1_310, T#1_198, T#1_199) -> [T#1_313]
  Op#34 RESHAPE(T#1_311, T#1_263) -> [T#1_314]
  Op#35 TRANSPOSE(T#1_314, T#1_262) -> [T#1_315]
  Op#36 SLICE(T#1_315, T#1_202, T#1_197) -> [T#1_316]
  Op#37 SLICE(T#1_315, T#1_196, T#1_197) -> [T#1_317]
  Op#38 MUL(T#1_316, T#1_286) -> [T#1_318]
  Op#39 MUL(T#1_317, T#1_287) -> [T#1_319]
  Op#40 SUB(T#1_318, T#1_319) -> [T#1_320]
  Op#41 MUL(T#1_317, T#1_286) -> [T#1_321]
  Op#42 MUL(T#1_316, T#1_287) -> [T#1_322]
  Op#43 ADD(T#1_321, T#1_322) -> [T#1_323]
  Op#44 CONCATENATION(T#1_320, T#1_323) -> [T#1_324]
  Op#45 TRANSPOSE(T#1_324, T#1_262) -> [T#1_325]
  Op#46 TRANSPOSE(T#1_312, T#1_262) -> [T#1_326]
  Op#47 SLICE(T#1_326, T#1_202, T#1_195) -> [T#1_327]
  Op#48 SLICE(T#1_326, T#1_196, T#1_195) -> [T#1_328]
  Op#49 MUL(T#1_327, T#1_286) -> [T#1_329]
  Op#50 MUL(T#1_328, T#1_287) -> [T#1_330]
  Op#51 SUB(T#1_329, T#1_330) -> [T#1_331]
  Op#52 MUL(T#1_328, T#1_286) -> [T#1_332]
  Op#53 MUL(T#1_327, T#1_287) -> [T#1_333]
  Op#54 ADD(T#1_332, T#1_333) -> [T#1_334]
  Op#55 CONCATENATION(T#1_331, T#1_334) -> [T#1_335]
  Op#56 TRANSPOSE(T#1_335, T#1_262) -> [T#1_336]
  Op#57 SLICE(T#1_5, T#1_194, T#1_275) -> [T#1_337]
  Op#58 RESHAPE(T#1_337, T#1_261) -> [T#1_338]
  Op#59 PACK(T#1_276, T#1_338, T#1_276, T#1_276) -> [T#1_339]
  Op#60 DYNAMIC_UPDATE_SLICE(T#1_42, T#1_336, T#1_339) -> [T#1_340]
  Op#61 DYNAMIC_UPDATE_SLICE(T#1_19, T#1_313, T#1_339) -> [T#1_341]
  Op#62 STABLEHLO_COMPOSITE(T#1_325, T#1_340, T#1_341, T#1_300) -> [T#1_342]
  Op#63 RESHAPE(T#1_342, T#1_271) -> [T#1_343]
  Op#64 FULLY_CONNECTED(T#1_343, T#1_192, T#-1) -> [T#1_344]
  Op#65 ADD(T#1_281, T#1_344) -> [T#1_345]
  Op#66 MUL(T#1_345, T#1_345) -> [T#1_346]
  Op#67 SUM(T#1_346, T#1_273) -> [T#1_347]
  Op#68 MUL(T#1_347, T#1_203) -> [T#1_348]
  Op#69 ADD(T#1_348, T#1_277) -> [T#1_349]
  Op#70 RESHAPE(T#1_349, T#1_265) -> [T#1_350]
  Op#71 RSQRT(T#1_350) -> [T#1_351]
  Op#72 MUL(T#1_345, T#1_351) -> [T#1_352]
  Op#73 MUL(T#1_352, T#1_258) -> [T#1_353]
  Op#74 FULLY_CONNECTED(T#1_353, T#1_191, T#-1) -> [T#1_354]
  Op#75 LOGISTIC(T#1_354) -> [T#1_355]
  Op#76 MUL(T#1_354, T#1_355) -> [T#1_356]
  Op#77 FULLY_CONNECTED(T#1_353, T#1_190, T#-1) -> [T#1_357]
  Op#78 MUL(T#1_356, T#1_357) -> [T#1_358]
  Op#79 FULLY_CONNECTED(T#1_358, T#1_189, T#-1) -> [T#1_359]
  Op#80 ADD(T#1_345, T#1_359) -> [T#1_360]
  Op#81 MUL(T#1_360, T#1_360) -> [T#1_361]
  Op#82 SUM(T#1_361, T#1_273) -> [T#1_362]
  Op#83 MUL(T#1_362, T#1_203) -> [T#1_363]
  Op#84 ADD(T#1_363, T#1_277) -> [T#1_364]
  Op#85 RESHAPE(T#1_364, T#1_265) -> [T#1_365]
  Op#86 RSQRT(T#1_365) -> [T#1_366]
  Op#87 MUL(T#1_360, T#1_366) -> [T#1_367]
  Op#88 MUL(T#1_367, T#1_257) -> [T#1_368]
  Op#89 FULLY_CONNECTED(T#1_368, T#1_188, T#-1) -> [T#1_369]
  Op#90 RESHAPE(T#1_369, T#1_264) -> [T#1_370]
  Op#91 SLICE(T#1_370, T#1_202, T#1_201) -> [T#1_371]
  Op#92 SLICE(T#1_370, T#1_200, T#1_199) -> [T#1_372]
  Op#93 SLICE(T#1_370, T#1_198, T#1_199) -> [T#1_373]
  Op#94 RESHAPE(T#1_371, T#1_263) -> [T#1_374]
  Op#95 TRANSPOSE(T#1_374, T#1_262) -> [T#1_375]
  Op#96 SLICE(T#1_375, T#1_202, T#1_197) -> [T#1_376]
  Op#97 SLICE(T#1_375, T#1_196, T#1_197) -> [T#1_377]
  Op#98 MUL(T#1_376, T#1_286) -> [T#1_378]
  Op#99 MUL(T#1_377, T#1_287) -> [T#1_379]
  Op#100 SUB(T#1_378, T#1_379) -> [T#1_380]
  Op#101 MUL(T#1_377, T#1_286) -> [T#1_381]
  Op#102 MUL(T#1_376, T#1_287) -> [T#1_382]
  Op#103 ADD(T#1_381, T#1_382) -> [T#1_383]
  Op#104 CONCATENATION(T#1_380, T#1_383) -> [T#1_384]
  Op#105 TRANSPOSE(T#1_384, T#1_262) -> [T#1_385]
  Op#106 TRANSPOSE(T#1_372, T#1_262) -> [T#1_386]
  Op#107 SLICE(T#1_386, T#1_202, T#1_195) -> [T#1_387]
  Op#108 SLICE(T#1_386, T#1_196, T#1_195) -> [T#1_388]
  Op#109 MUL(T#1_387, T#1_286) -> [T#1_389]
  Op#110 MUL(T#1_388, T#1_287) -> [T#1_390]
  Op#111 SUB(T#1_389, T#1_390) -> [T#1_391]
  Op#112 MUL(T#1_388, T#1_286) -> [T#1_392]
  Op#113 MUL(T#1_387, T#1_287) -> [T#1_393]
  Op#114 ADD(T#1_392, T#1_393) -> [T#1_394]
  Op#115 CONCATENATION(T#1_391, T#1_394) -> [T#1_395]
  Op#116 TRANSPOSE(T#1_395, T#1_262) -> [T#1_396]
  Op#117 DYNAMIC_UPDATE_SLICE(T#1_48, T#1_396, T#1_339) -> [T#1_397]
  Op#118 DYNAMIC_UPDATE_SLICE(T#1_38, T#1_373, T#1_339) -> [T#1_398]
  Op#119 STABLEHLO_COMPOSITE(T#1_385, T#1_397, T#1_398, T#1_300) -> [T#1_399]
  Op#120 RESHAPE(T#1_399, T#1_271) -> [T#1_400]
  Op#121 FULLY_CONNECTED(T#1_400, T#1_187, T#-1) -> [T#1_401]
  Op#122 ADD(T#1_360, T#1_401) -> [T#1_402]
  Op#123 MUL(T#1_402, T#1_402) -> [T#1_403]
  Op#124 SUM(T#1_403, T#1_273) -> [T#1_404]
  Op#125 MUL(T#1_404, T#1_203) -> [T#1_405]
  Op#126 ADD(T#1_405, T#1_277) -> [T#1_406]
  Op#127 RESHAPE(T#1_406, T#1_265) -> [T#1_407]
  Op#128 RSQRT(T#1_407) -> [T#1_408]
  Op#129 MUL(T#1_402, T#1_408) -> [T#1_409]
  Op#130 MUL(T#1_409, T#1_256) -> [T#1_410]
  Op#131 FULLY_CONNECTED(T#1_410, T#1_186, T#-1) -> [T#1_411]
  Op#132 LOGISTIC(T#1_411) -> [T#1_412]
  Op#133 MUL(T#1_411, T#1_412) -> [T#1_413]
  Op#134 FULLY_CONNECTED(T#1_410, T#1_185, T#-1) -> [T#1_414]
  Op#135 MUL(T#1_413, T#1_414) -> [T#1_415]
  Op#136 FULLY_CONNECTED(T#1_415, T#1_184, T#-1) -> [T#1_416]
  Op#137 ADD(T#1_402, T#1_416) -> [T#1_417]
  Op#138 MUL(T#1_417, T#1_417) -> [T#1_418]
  Op#139 SUM(T#1_418, T#1_273) -> [T#1_419]
  Op#140 MUL(T#1_419, T#1_203) -> [T#1_420]
  Op#141 ADD(T#1_420, T#1_277) -> [T#1_421]
  Op#142 RESHAPE(T#1_421, T#1_265) -> [T#1_422]
  Op#143 RSQRT(T#1_422) -> [T#1_423]
  Op#144 MUL(T#1_417, T#1_423) -> [T#1_424]
  Op#145 MUL(T#1_424, T#1_255) -> [T#1_425]
  Op#146 FULLY_CONNECTED(T#1_425, T#1_183, T#-1) -> [T#1_426]
  Op#147 RESHAPE(T#1_426, T#1_264) -> [T#1_427]
  Op#148 SLICE(T#1_427, T#1_202, T#1_201) -> [T#1_428]
  Op#149 SLICE(T#1_427, T#1_200, T#1_199) -> [T#1_429]
  Op#150 SLICE(T#1_427, T#1_198, T#1_199) -> [T#1_430]
  Op#151 RESHAPE(T#1_428, T#1_263) -> [T#1_431]
  Op#152 TRANSPOSE(T#1_431, T#1_262) -> [T#1_432]
  Op#153 SLICE(T#1_432, T#1_202, T#1_197) -> [T#1_433]
  Op#154 SLICE(T#1_432, T#1_196, T#1_197) -> [T#1_434]
  Op#155 MUL(T#1_433, T#1_286) -> [T#1_435]
  Op#156 MUL(T#1_434, T#1_287) -> [T#1_436]
  Op#157 SUB(T#1_435, T#1_436) -> [T#1_437]
  Op#158 MUL(T#1_434, T#1_286) -> [T#1_438]
  Op#159 MUL(T#1_433, T#1_287) -> [T#1_439]
  Op#160 ADD(T#1_438, T#1_439) -> [T#1_440]
  Op#161 CONCATENATION(T#1_437, T#1_440) -> [T#1_441]
  Op#162 TRANSPOSE(T#1_441, T#1_262) -> [T#1_442]
  Op#163 TRANSPOSE(T#1_429, T#1_262) -> [T#1_443]
  Op#164 SLICE(T#1_443, T#1_202, T#1_195) -> [T#1_444]
  Op#165 SLICE(T#1_443, T#1_196, T#1_195) -> [T#1_445]
  Op#166 MUL(T#1_444, T#1_286) -> [T#1_446]
  Op#167 MUL(T#1_445, T#1_287) -> [T#1_447]
  Op#168 SUB(T#1_446, T#1_447) -> [T#1_448]
  Op#169 MUL(T#1_445, T#1_286) -> [T#1_449]
  Op#170 MUL(T#1_444, T#1_287) -> [T#1_450]
  Op#171 ADD(T#1_449, T#1_450) -> [T#1_451]
  Op#172 CONCATENATION(T#1_448, T#1_451) -> [T#1_452]
  Op#173 TRANSPOSE(T#1_452, T#1_262) -> [T#1_453]
  Op#174 DYNAMIC_UPDATE_SLICE(T#1_2, T#1_453, T#1_339) -> [T#1_454]
  Op#175 DYNAMIC_UPDATE_SLICE(T#1_47, T#1_430, T#1_339) -> [T#1_455]
  Op#176 STABLEHLO_COMPOSITE(T#1_442, T#1_454, T#1_455, T#1_300) -> [T#1_456]
  Op#177 RESHAPE(T#1_456, T#1_271) -> [T#1_457]
  Op#178 FULLY_CONNECTED(T#1_457, T#1_182, T#-1) -> [T#1_458]
  Op#179 ADD(T#1_417, T#1_458) -> [T#1_459]
  Op#180 MUL(T#1_459, T#1_459) -> [T#1_460]
  Op#181 SUM(T#1_460, T#1_273) -> [T#1_461]
  Op#182 MUL(T#1_461, T#1_203) -> [T#1_462]
  Op#183 ADD(T#1_462, T#1_277) -> [T#1_463]
  Op#184 RESHAPE(T#1_463, T#1_265) -> [T#1_464]
  Op#185 RSQRT(T#1_464) -> [T#1_465]
  Op#186 MUL(T#1_459, T#1_465) -> [T#1_466]
  Op#187 MUL(T#1_466, T#1_254) -> [T#1_467]
  Op#188 FULLY_CONNECTED(T#1_467, T#1_181, T#-1) -> [T#1_468]
  Op#189 LOGISTIC(T#1_468) -> [T#1_469]
  Op#190 MUL(T#1_468, T#1_469) -> [T#1_470]
  Op#191 FULLY_CONNECTED(T#1_467, T#1_180, T#-1) -> [T#1_471]
  Op#192 MUL(T#1_470, T#1_471) -> [T#1_472]
  Op#193 FULLY_CONNECTED(T#1_472, T#1_179, T#-1) -> [T#1_473]
  Op#194 ADD(T#1_459, T#1_473) -> [T#1_474]
  Op#195 MUL(T#1_474, T#1_474) -> [T#1_475]
  Op#196 SUM(T#1_475, T#1_273) -> [T#1_476]
  Op#197 MUL(T#1_476, T#1_203) -> [T#1_477]
  Op#198 ADD(T#1_477, T#1_277) -> [T#1_478]
  Op#199 RESHAPE(T#1_478, T#1_265) -> [T#1_479]
  Op#200 RSQRT(T#1_479) -> [T#1_480]
  Op#201 MUL(T#1_474, T#1_480) -> [T#1_481]
  Op#202 MUL(T#1_481, T#1_253) -> [T#1_482]
  Op#203 FULLY_CONNECTED(T#1_482, T#1_178, T#-1) -> [T#1_483]
  Op#204 RESHAPE(T#1_483, T#1_264) -> [T#1_484]
  Op#205 SLICE(T#1_484, T#1_202, T#1_201) -> [T#1_485]
  Op#206 SLICE(T#1_484, T#1_200, T#1_199) -> [T#1_486]
  Op#207 SLICE(T#1_484, T#1_198, T#1_199) -> [T#1_487]
  Op#208 RESHAPE(T#1_485, T#1_263) -> [T#1_488]
  Op#209 TRANSPOSE(T#1_488, T#1_262) -> [T#1_489]
  Op#210 SLICE(T#1_489, T#1_202, T#1_197) -> [T#1_490]
  Op#211 SLICE(T#1_489, T#1_196, T#1_197) -> [T#1_491]
  Op#212 MUL(T#1_490, T#1_286) -> [T#1_492]
  Op#213 MUL(T#1_491, T#1_287) -> [T#1_493]
  Op#214 SUB(T#1_492, T#1_493) -> [T#1_494]
  Op#215 MUL(T#1_491, T#1_286) -> [T#1_495]
  Op#216 MUL(T#1_490, T#1_287) -> [T#1_496]
  Op#217 ADD(T#1_495, T#1_496) -> [T#1_497]
  Op#218 CONCATENATION(T#1_494, T#1_497) -> [T#1_498]
  Op#219 TRANSPOSE(T#1_498, T#1_262) -> [T#1_499]
  Op#220 TRANSPOSE(T#1_486, T#1_262) -> [T#1_500]
  Op#221 SLICE(T#1_500, T#1_202, T#1_195) -> [T#1_501]
  Op#222 SLICE(T#1_500, T#1_196, T#1_195) -> [T#1_502]
  Op#223 MUL(T#1_501, T#1_286) -> [T#1_503]
  Op#224 MUL(T#1_502, T#1_287) -> [T#1_504]
  Op#225 SUB(T#1_503, T#1_504) -> [T#1_505]
  Op#226 MUL(T#1_502, T#1_286) -> [T#1_506]
  Op#227 MUL(T#1_501, T#1_287) -> [T#1_507]
  Op#228 ADD(T#1_506, T#1_507) -> [T#1_508]
  Op#229 CONCATENATION(T#1_505, T#1_508) -> [T#1_509]
  Op#230 TRANSPOSE(T#1_509, T#1_262) -> [T#1_510]
  Op#231 DYNAMIC_UPDATE_SLICE(T#1_18, T#1_510, T#1_339) -> [T#1_511]
  Op#232 DYNAMIC_UPDATE_SLICE(T#1_1, T#1_487, T#1_339) -> [T#1_512]
  Op#233 STABLEHLO_COMPOSITE(T#1_499, T#1_511, T#1_512, T#1_300) -> [T#1_513]
  Op#234 RESHAPE(T#1_513, T#1_271) -> [T#1_514]
  Op#235 FULLY_CONNECTED(T#1_514, T#1_177, T#-1) -> [T#1_515]
  Op#236 ADD(T#1_474, T#1_515) -> [T#1_516]
  Op#237 MUL(T#1_516, T#1_516) -> [T#1_517]
  Op#238 SUM(T#1_517, T#1_273) -> [T#1_518]
  Op#239 MUL(T#1_518, T#1_203) -> [T#1_519]
  Op#240 ADD(T#1_519, T#1_277) -> [T#1_520]
  Op#241 RESHAPE(T#1_520, T#1_265) -> [T#1_521]
  Op#242 RSQRT(T#1_521) -> [T#1_522]
  Op#243 MUL(T#1_516, T#1_522) -> [T#1_523]
  Op#244 MUL(T#1_523, T#1_252) -> [T#1_524]
  Op#245 FULLY_CONNECTED(T#1_524, T#1_176, T#-1) -> [T#1_525]
  Op#246 LOGISTIC(T#1_525) -> [T#1_526]
  Op#247 MUL(T#1_525, T#1_526) -> [T#1_527]
  Op#248 FULLY_CONNECTED(T#1_524, T#1_175, T#-1) -> [T#1_528]
  Op#249 MUL(T#1_527, T#1_528) -> [T#1_529]
  Op#250 FULLY_CONNECTED(T#1_529, T#1_174, T#-1) -> [T#1_530]
  Op#251 ADD(T#1_516, T#1_530) -> [T#1_531]
  Op#252 MUL(T#1_531, T#1_531) -> [T#1_532]
  Op#253 SUM(T#1_532, T#1_273) -> [T#1_533]
  Op#254 MUL(T#1_533, T#1_203) -> [T#1_534]
  Op#255 ADD(T#1_534, T#1_277) -> [T#1_535]
  Op#256 RESHAPE(T#1_535, T#1_265) -> [T#1_536]
  Op#257 RSQRT(T#1_536) -> [T#1_537]
  Op#258 MUL(T#1_531, T#1_537) -> [T#1_538]
  Op#259 MUL(T#1_538, T#1_251) -> [T#1_539]
  Op#260 FULLY_CONNECTED(T#1_539, T#1_173, T#-1) -> [T#1_540]
  Op#261 RESHAPE(T#1_540, T#1_264) -> [T#1_541]
  Op#262 SLICE(T#1_541, T#1_202, T#1_201) -> [T#1_542]
  Op#263 SLICE(T#1_541, T#1_200, T#1_199) -> [T#1_543]
  Op#264 SLICE(T#1_541, T#1_198, T#1_199) -> [T#1_544]
  Op#265 RESHAPE(T#1_542, T#1_263) -> [T#1_545]
  Op#266 TRANSPOSE(T#1_545, T#1_262) -> [T#1_546]
  Op#267 SLICE(T#1_546, T#1_202, T#1_197) -> [T#1_547]
  Op#268 SLICE(T#1_546, T#1_196, T#1_197) -> [T#1_548]
  Op#269 MUL(T#1_547, T#1_286) -> [T#1_549]
  Op#270 MUL(T#1_548, T#1_287) -> [T#1_550]
  Op#271 SUB(T#1_549, T#1_550) -> [T#1_551]
  Op#272 MUL(T#1_548, T#1_286) -> [T#1_552]
  Op#273 MUL(T#1_547, T#1_287) -> [T#1_553]
  Op#274 ADD(T#1_552, T#1_553) -> [T#1_554]
  Op#275 CONCATENATION(T#1_551, T#1_554) -> [T#1_555]
  Op#276 TRANSPOSE(T#1_555, T#1_262) -> [T#1_556]
  Op#277 TRANSPOSE(T#1_543, T#1_262) -> [T#1_557]
  Op#278 SLICE(T#1_557, T#1_202, T#1_195) -> [T#1_558]
  Op#279 SLICE(T#1_557, T#1_196, T#1_195) -> [T#1_559]
  Op#280 MUL(T#1_558, T#1_286) -> [T#1_560]
  Op#281 MUL(T#1_559, T#1_287) -> [T#1_561]
  Op#282 SUB(T#1_560, T#1_561) -> [T#1_562]
  Op#283 MUL(T#1_559, T#1_286) -> [T#1_563]
  Op#284 MUL(T#1_558, T#1_287) -> [T#1_564]
  Op#285 ADD(T#1_563, T#1_564) -> [T#1_565]
  Op#286 CONCATENATION(T#1_562, T#1_565) -> [T#1_566]
  Op#287 TRANSPOSE(T#1_566, T#1_262) -> [T#1_567]
  Op#288 DYNAMIC_UPDATE_SLICE(T#1_14, T#1_567, T#1_339) -> [T#1_568]
  Op#289 DYNAMIC_UPDATE_SLICE(T#1_51, T#1_544, T#1_339) -> [T#1_569]
  Op#290 STABLEHLO_COMPOSITE(T#1_556, T#1_568, T#1_569, T#1_300) -> [T#1_570]
  Op#291 RESHAPE(T#1_570, T#1_271) -> [T#1_571]
  Op#292 FULLY_CONNECTED(T#1_571, T#1_172, T#-1) -> [T#1_572]
  Op#293 ADD(T#1_531, T#1_572) -> [T#1_573]
  Op#294 MUL(T#1_573, T#1_573) -> [T#1_574]
  Op#295 SUM(T#1_574, T#1_273) -> [T#1_575]
  Op#296 MUL(T#1_575, T#1_203) -> [T#1_576]
  Op#297 ADD(T#1_576, T#1_277) -> [T#1_577]
  Op#298 RESHAPE(T#1_577, T#1_265) -> [T#1_578]
  Op#299 RSQRT(T#1_578) -> [T#1_579]
  Op#300 MUL(T#1_573, T#1_579) -> [T#1_580]
  Op#301 MUL(T#1_580, T#1_250) -> [T#1_581]
  Op#302 FULLY_CONNECTED(T#1_581, T#1_171, T#-1) -> [T#1_582]
  Op#303 LOGISTIC(T#1_582) -> [T#1_583]
  Op#304 MUL(T#1_582, T#1_583) -> [T#1_584]
  Op#305 FULLY_CONNECTED(T#1_581, T#1_170, T#-1) -> [T#1_585]
  Op#306 MUL(T#1_584, T#1_585) -> [T#1_586]
  Op#307 FULLY_CONNECTED(T#1_586, T#1_169, T#-1) -> [T#1_587]
  Op#308 ADD(T#1_573, T#1_587) -> [T#1_588]
  Op#309 MUL(T#1_588, T#1_588) -> [T#1_589]
  Op#310 SUM(T#1_589, T#1_273) -> [T#1_590]
  Op#311 MUL(T#1_590, T#1_203) -> [T#1_591]
  Op#312 ADD(T#1_591, T#1_277) -> [T#1_592]
  Op#313 RESHAPE(T#1_592, T#1_265) -> [T#1_593]
  Op#314 RSQRT(T#1_593) -> [T#1_594]
  Op#315 MUL(T#1_588, T#1_594) -> [T#1_595]
  Op#316 MUL(T#1_595, T#1_249) -> [T#1_596]
  Op#317 FULLY_CONNECTED(T#1_596, T#1_168, T#-1) -> [T#1_597]
  Op#318 RESHAPE(T#1_597, T#1_264) -> [T#1_598]
  Op#319 SLICE(T#1_598, T#1_202, T#1_201) -> [T#1_599]
  Op#320 SLICE(T#1_598, T#1_200, T#1_199) -> [T#1_600]
  Op#321 SLICE(T#1_598, T#1_198, T#1_199) -> [T#1_601]
  Op#322 RESHAPE(T#1_599, T#1_263) -> [T#1_602]
  Op#323 TRANSPOSE(T#1_602, T#1_262) -> [T#1_603]
  Op#324 SLICE(T#1_603, T#1_202, T#1_197) -> [T#1_604]
  Op#325 SLICE(T#1_603, T#1_196, T#1_197) -> [T#1_605]
  Op#326 MUL(T#1_604, T#1_286) -> [T#1_606]
  Op#327 MUL(T#1_605, T#1_287) -> [T#1_607]
  Op#328 SUB(T#1_606, T#1_607) -> [T#1_608]
  Op#329 MUL(T#1_605, T#1_286) -> [T#1_609]
  Op#330 MUL(T#1_604, T#1_287) -> [T#1_610]
  Op#331 ADD(T#1_609, T#1_610) -> [T#1_611]
  Op#332 CONCATENATION(T#1_608, T#1_611) -> [T#1_612]
  Op#333 TRANSPOSE(T#1_612, T#1_262) -> [T#1_613]
  Op#334 TRANSPOSE(T#1_600, T#1_262) -> [T#1_614]
  Op#335 SLICE(T#1_614, T#1_202, T#1_195) -> [T#1_615]
  Op#336 SLICE(T#1_614, T#1_196, T#1_195) -> [T#1_616]
  Op#337 MUL(T#1_615, T#1_286) -> [T#1_617]
  Op#338 MUL(T#1_616, T#1_287) -> [T#1_618]
  Op#339 SUB(T#1_617, T#1_618) -> [T#1_619]
  Op#340 MUL(T#1_616, T#1_286) -> [T#1_620]
  Op#341 MUL(T#1_615, T#1_287) -> [T#1_621]
  Op#342 ADD(T#1_620, T#1_621) -> [T#1_622]
  Op#343 CONCATENATION(T#1_619, T#1_622) -> [T#1_623]
  Op#344 TRANSPOSE(T#1_623, T#1_262) -> [T#1_624]
  Op#345 DYNAMIC_UPDATE_SLICE(T#1_54, T#1_624, T#1_339) -> [T#1_625]
  Op#346 DYNAMIC_UPDATE_SLICE(T#1_23, T#1_601, T#1_339) -> [T#1_626]
  Op#347 STABLEHLO_COMPOSITE(T#1_613, T#1_625, T#1_626, T#1_300) -> [T#1_627]
  Op#348 RESHAPE(T#1_627, T#1_271) -> [T#1_628]
  Op#349 FULLY_CONNECTED(T#1_628, T#1_167, T#-1) -> [T#1_629]
  Op#350 ADD(T#1_588, T#1_629) -> [T#1_630]
  Op#351 MUL(T#1_630, T#1_630) -> [T#1_631]
  Op#352 SUM(T#1_631, T#1_273) -> [T#1_632]
  Op#353 MUL(T#1_632, T#1_203) -> [T#1_633]
  Op#354 ADD(T#1_633, T#1_277) -> [T#1_634]
  Op#355 RESHAPE(T#1_634, T#1_265) -> [T#1_635]
  Op#356 RSQRT(T#1_635) -> [T#1_636]
  Op#357 MUL(T#1_630, T#1_636) -> [T#1_637]
  Op#358 MUL(T#1_637, T#1_248) -> [T#1_638]
  Op#359 FULLY_CONNECTED(T#1_638, T#1_166, T#-1) -> [T#1_639]
  Op#360 LOGISTIC(T#1_639) -> [T#1_640]
  Op#361 MUL(T#1_639, T#1_640) -> [T#1_641]
  Op#362 FULLY_CONNECTED(T#1_638, T#1_165, T#-1) -> [T#1_642]
  Op#363 MUL(T#1_641, T#1_642) -> [T#1_643]
  Op#364 FULLY_CONNECTED(T#1_643, T#1_164, T#-1) -> [T#1_644]
  Op#365 ADD(T#1_630, T#1_644) -> [T#1_645]
  Op#366 MUL(T#1_645, T#1_645) -> [T#1_646]
  Op#367 SUM(T#1_646, T#1_273) -> [T#1_647]
  Op#368 MUL(T#1_647, T#1_203) -> [T#1_648]
  Op#369 ADD(T#1_648, T#1_277) -> [T#1_649]
  Op#370 RESHAPE(T#1_649, T#1_265) -> [T#1_650]
  Op#371 RSQRT(T#1_650) -> [T#1_651]
  Op#372 MUL(T#1_645, T#1_651) -> [T#1_652]
  Op#373 MUL(T#1_652, T#1_247) -> [T#1_653]
  Op#374 FULLY_CONNECTED(T#1_653, T#1_163, T#-1) -> [T#1_654]
  Op#375 RESHAPE(T#1_654, T#1_264) -> [T#1_655]
  Op#376 SLICE(T#1_655, T#1_202, T#1_201) -> [T#1_656]
  Op#377 SLICE(T#1_655, T#1_200, T#1_199) -> [T#1_657]
  Op#378 SLICE(T#1_655, T#1_198, T#1_199) -> [T#1_658]
  Op#379 RESHAPE(T#1_656, T#1_263) -> [T#1_659]
  Op#380 TRANSPOSE(T#1_659, T#1_262) -> [T#1_660]
  Op#381 SLICE(T#1_660, T#1_202, T#1_197) -> [T#1_661]
  Op#382 SLICE(T#1_660, T#1_196, T#1_197) -> [T#1_662]
  Op#383 MUL(T#1_661, T#1_286) -> [T#1_663]
  Op#384 MUL(T#1_662, T#1_287) -> [T#1_664]
  Op#385 SUB(T#1_663, T#1_664) -> [T#1_665]
  Op#386 MUL(T#1_662, T#1_286) -> [T#1_666]
  Op#387 MUL(T#1_661, T#1_287) -> [T#1_667]
  Op#388 ADD(T#1_666, T#1_667) -> [T#1_668]
  Op#389 CONCATENATION(T#1_665, T#1_668) -> [T#1_669]
  Op#390 TRANSPOSE(T#1_669, T#1_262) -> [T#1_670]
  Op#391 TRANSPOSE(T#1_657, T#1_262) -> [T#1_671]
  Op#392 SLICE(T#1_671, T#1_202, T#1_195) -> [T#1_672]
  Op#393 SLICE(T#1_671, T#1_196, T#1_195) -> [T#1_673]
  Op#394 MUL(T#1_672, T#1_286) -> [T#1_674]
  Op#395 MUL(T#1_673, T#1_287) -> [T#1_675]
  Op#396 SUB(T#1_674, T#1_675) -> [T#1_676]
  Op#397 MUL(T#1_673, T#1_286) -> [T#1_677]
  Op#398 MUL(T#1_672, T#1_287) -> [T#1_678]
  Op#399 ADD(T#1_677, T#1_678) -> [T#1_679]
  Op#400 CONCATENATION(T#1_676, T#1_679) -> [T#1_680]
  Op#401 TRANSPOSE(T#1_680, T#1_262) -> [T#1_681]
  Op#402 DYNAMIC_UPDATE_SLICE(T#1_24, T#1_681, T#1_339) -> [T#1_682]
  Op#403 DYNAMIC_UPDATE_SLICE(T#1_11, T#1_658, T#1_339) -> [T#1_683]
  Op#404 STABLEHLO_COMPOSITE(T#1_670, T#1_682, T#1_683, T#1_300) -> [T#1_684]
  Op#405 RESHAPE(T#1_684, T#1_271) -> [T#1_685]
  Op#406 FULLY_CONNECTED(T#1_685, T#1_162, T#-1) -> [T#1_686]
  Op#407 ADD(T#1_645, T#1_686) -> [T#1_687]
  Op#408 MUL(T#1_687, T#1_687) -> [T#1_688]
  Op#409 SUM(T#1_688, T#1_273) -> [T#1_689]
  Op#410 MUL(T#1_689, T#1_203) -> [T#1_690]
  Op#411 ADD(T#1_690, T#1_277) -> [T#1_691]
  Op#412 RESHAPE(T#1_691, T#1_265) -> [T#1_692]
  Op#413 RSQRT(T#1_692) -> [T#1_693]
  Op#414 MUL(T#1_687, T#1_693) -> [T#1_694]
  Op#415 MUL(T#1_694, T#1_246) -> [T#1_695]
  Op#416 FULLY_CONNECTED(T#1_695, T#1_161, T#-1) -> [T#1_696]
  Op#417 LOGISTIC(T#1_696) -> [T#1_697]
  Op#418 MUL(T#1_696, T#1_697) -> [T#1_698]
  Op#419 FULLY_CONNECTED(T#1_695, T#1_160, T#-1) -> [T#1_699]
  Op#420 MUL(T#1_698, T#1_699) -> [T#1_700]
  Op#421 FULLY_CONNECTED(T#1_700, T#1_159, T#-1) -> [T#1_701]
  Op#422 ADD(T#1_687, T#1_701) -> [T#1_702]
  Op#423 MUL(T#1_702, T#1_702) -> [T#1_703]
  Op#424 SUM(T#1_703, T#1_273) -> [T#1_704]
  Op#425 MUL(T#1_704, T#1_203) -> [T#1_705]
  Op#426 ADD(T#1_705, T#1_277) -> [T#1_706]
  Op#427 RESHAPE(T#1_706, T#1_265) -> [T#1_707]
  Op#428 RSQRT(T#1_707) -> [T#1_708]
  Op#429 MUL(T#1_702, T#1_708) -> [T#1_709]
  Op#430 MUL(T#1_709, T#1_245) -> [T#1_710]
  Op#431 FULLY_CONNECTED(T#1_710, T#1_158, T#-1) -> [T#1_711]
  Op#432 RESHAPE(T#1_711, T#1_264) -> [T#1_712]
  Op#433 SLICE(T#1_712, T#1_202, T#1_201) -> [T#1_713]
  Op#434 SLICE(T#1_712, T#1_200, T#1_199) -> [T#1_714]
  Op#435 SLICE(T#1_712, T#1_198, T#1_199) -> [T#1_715]
  Op#436 RESHAPE(T#1_713, T#1_263) -> [T#1_716]
  Op#437 TRANSPOSE(T#1_716, T#1_262) -> [T#1_717]
  Op#438 SLICE(T#1_717, T#1_202, T#1_197) -> [T#1_718]
  Op#439 SLICE(T#1_717, T#1_196, T#1_197) -> [T#1_719]
  Op#440 MUL(T#1_718, T#1_286) -> [T#1_720]
  Op#441 MUL(T#1_719, T#1_287) -> [T#1_721]
  Op#442 SUB(T#1_720, T#1_721) -> [T#1_722]
  Op#443 MUL(T#1_719, T#1_286) -> [T#1_723]
  Op#444 MUL(T#1_718, T#1_287) -> [T#1_724]
  Op#445 ADD(T#1_723, T#1_724) -> [T#1_725]
  Op#446 CONCATENATION(T#1_722, T#1_725) -> [T#1_726]
  Op#447 TRANSPOSE(T#1_726, T#1_262) -> [T#1_727]
  Op#448 TRANSPOSE(T#1_714, T#1_262) -> [T#1_728]
  Op#449 SLICE(T#1_728, T#1_202, T#1_195) -> [T#1_729]
  Op#450 SLICE(T#1_728, T#1_196, T#1_195) -> [T#1_730]
  Op#451 MUL(T#1_729, T#1_286) -> [T#1_731]
  Op#452 MUL(T#1_730, T#1_287) -> [T#1_732]
  Op#453 SUB(T#1_731, T#1_732) -> [T#1_733]
  Op#454 MUL(T#1_730, T#1_286) -> [T#1_734]
  Op#455 MUL(T#1_729, T#1_287) -> [T#1_735]
  Op#456 ADD(T#1_734, T#1_735) -> [T#1_736]
  Op#457 CONCATENATION(T#1_733, T#1_736) -> [T#1_737]
  Op#458 TRANSPOSE(T#1_737, T#1_262) -> [T#1_738]
  Op#459 DYNAMIC_UPDATE_SLICE(T#1_28, T#1_738, T#1_339) -> [T#1_739]
  Op#460 DYNAMIC_UPDATE_SLICE(T#1_36, T#1_715, T#1_339) -> [T#1_740]
  Op#461 STABLEHLO_COMPOSITE(T#1_727, T#1_739, T#1_740, T#1_300) -> [T#1_741]
  Op#462 RESHAPE(T#1_741, T#1_271) -> [T#1_742]
  Op#463 FULLY_CONNECTED(T#1_742, T#1_157, T#-1) -> [T#1_743]
  Op#464 ADD(T#1_702, T#1_743) -> [T#1_744]
  Op#465 MUL(T#1_744, T#1_744) -> [T#1_745]
  Op#466 SUM(T#1_745, T#1_273) -> [T#1_746]
  Op#467 MUL(T#1_746, T#1_203) -> [T#1_747]
  Op#468 ADD(T#1_747, T#1_277) -> [T#1_748]
  Op#469 RESHAPE(T#1_748, T#1_265) -> [T#1_749]
  Op#470 RSQRT(T#1_749) -> [T#1_750]
  Op#471 MUL(T#1_744, T#1_750) -> [T#1_751]
  Op#472 MUL(T#1_751, T#1_244) -> [T#1_752]
  Op#473 FULLY_CONNECTED(T#1_752, T#1_156, T#-1) -> [T#1_753]
  Op#474 LOGISTIC(T#1_753) -> [T#1_754]
  Op#475 MUL(T#1_753, T#1_754) -> [T#1_755]
  Op#476 FULLY_CONNECTED(T#1_752, T#1_155, T#-1) -> [T#1_756]
  Op#477 MUL(T#1_755, T#1_756) -> [T#1_757]
  Op#478 FULLY_CONNECTED(T#1_757, T#1_154, T#-1) -> [T#1_758]
  Op#479 ADD(T#1_744, T#1_758) -> [T#1_759]
  Op#480 MUL(T#1_759, T#1_759) -> [T#1_760]
  Op#481 SUM(T#1_760, T#1_273) -> [T#1_761]
  Op#482 MUL(T#1_761, T#1_203) -> [T#1_762]
  Op#483 ADD(T#1_762, T#1_277) -> [T#1_763]
  Op#484 RESHAPE(T#1_763, T#1_265) -> [T#1_764]
  Op#485 RSQRT(T#1_764) -> [T#1_765]
  Op#486 MUL(T#1_759, T#1_765) -> [T#1_766]
  Op#487 MUL(T#1_766, T#1_243) -> [T#1_767]
  Op#488 FULLY_CONNECTED(T#1_767, T#1_153, T#-1) -> [T#1_768]
  Op#489 RESHAPE(T#1_768, T#1_264) -> [T#1_769]
  Op#490 SLICE(T#1_769, T#1_202, T#1_201) -> [T#1_770]
  Op#491 SLICE(T#1_769, T#1_200, T#1_199) -> [T#1_771]
  Op#492 SLICE(T#1_769, T#1_198, T#1_199) -> [T#1_772]
  Op#493 RESHAPE(T#1_770, T#1_263) -> [T#1_773]
  Op#494 TRANSPOSE(T#1_773, T#1_262) -> [T#1_774]
  Op#495 SLICE(T#1_774, T#1_202, T#1_197) -> [T#1_775]
  Op#496 SLICE(T#1_774, T#1_196, T#1_197) -> [T#1_776]
  Op#497 MUL(T#1_775, T#1_286) -> [T#1_777]
  Op#498 MUL(T#1_776, T#1_287) -> [T#1_778]
  Op#499 SUB(T#1_777, T#1_778) -> [T#1_779]
  Op#500 MUL(T#1_776, T#1_286) -> [T#1_780]
  Op#501 MUL(T#1_775, T#1_287) -> [T#1_781]
  Op#502 ADD(T#1_780, T#1_781) -> [T#1_782]
  Op#503 CONCATENATION(T#1_779, T#1_782) -> [T#1_783]
  Op#504 TRANSPOSE(T#1_783, T#1_262) -> [T#1_784]
  Op#505 TRANSPOSE(T#1_771, T#1_262) -> [T#1_785]
  Op#506 SLICE(T#1_785, T#1_202, T#1_195) -> [T#1_786]
  Op#507 SLICE(T#1_785, T#1_196, T#1_195) -> [T#1_787]
  Op#508 MUL(T#1_786, T#1_286) -> [T#1_788]
  Op#509 MUL(T#1_787, T#1_287) -> [T#1_789]
  Op#510 SUB(T#1_788, T#1_789) -> [T#1_790]
  Op#511 MUL(T#1_787, T#1_286) -> [T#1_791]
  Op#512 MUL(T#1_786, T#1_287) -> [T#1_792]
  Op#513 ADD(T#1_791, T#1_792) -> [T#1_793]
  Op#514 CONCATENATION(T#1_790, T#1_793) -> [T#1_794]
  Op#515 TRANSPOSE(T#1_794, T#1_262) -> [T#1_795]
  Op#516 DYNAMIC_UPDATE_SLICE(T#1_4, T#1_795, T#1_339) -> [T#1_796]
  Op#517 DYNAMIC_UPDATE_SLICE(T#1_49, T#1_772, T#1_339) -> [T#1_797]
  Op#518 STABLEHLO_COMPOSITE(T#1_784, T#1_796, T#1_797, T#1_300) -> [T#1_798]
  Op#519 RESHAPE(T#1_798, T#1_271) -> [T#1_799]
  Op#520 FULLY_CONNECTED(T#1_799, T#1_152, T#-1) -> [T#1_800]
  Op#521 ADD(T#1_759, T#1_800) -> [T#1_801]
  Op#522 MUL(T#1_801, T#1_801) -> [T#1_802]
  Op#523 SUM(T#1_802, T#1_273) -> [T#1_803]
  Op#524 MUL(T#1_803, T#1_203) -> [T#1_804]
  Op#525 ADD(T#1_804, T#1_277) -> [T#1_805]
  Op#526 RESHAPE(T#1_805, T#1_265) -> [T#1_806]
  Op#527 RSQRT(T#1_806) -> [T#1_807]
  Op#528 MUL(T#1_801, T#1_807) -> [T#1_808]
  Op#529 MUL(T#1_808, T#1_242) -> [T#1_809]
  Op#530 FULLY_CONNECTED(T#1_809, T#1_151, T#-1) -> [T#1_810]
  Op#531 LOGISTIC(T#1_810) -> [T#1_811]
  Op#532 MUL(T#1_810, T#1_811) -> [T#1_812]
  Op#533 FULLY_CONNECTED(T#1_809, T#1_150, T#-1) -> [T#1_813]
  Op#534 MUL(T#1_812, T#1_813) -> [T#1_814]
  Op#535 FULLY_CONNECTED(T#1_814, T#1_149, T#-1) -> [T#1_815]
  Op#536 ADD(T#1_801, T#1_815) -> [T#1_816]
  Op#537 MUL(T#1_816, T#1_816) -> [T#1_817]
  Op#538 SUM(T#1_817, T#1_273) -> [T#1_818]
  Op#539 MUL(T#1_818, T#1_203) -> [T#1_819]
  Op#540 ADD(T#1_819, T#1_277) -> [T#1_820]
  Op#541 RESHAPE(T#1_820, T#1_265) -> [T#1_821]
  Op#542 RSQRT(T#1_821) -> [T#1_822]
  Op#543 MUL(T#1_816, T#1_822) -> [T#1_823]
  Op#544 MUL(T#1_823, T#1_241) -> [T#1_824]
  Op#545 FULLY_CONNECTED(T#1_824, T#1_148, T#-1) -> [T#1_825]
  Op#546 RESHAPE(T#1_825, T#1_264) -> [T#1_826]
  Op#547 SLICE(T#1_826, T#1_202, T#1_201) -> [T#1_827]
  Op#548 SLICE(T#1_826, T#1_200, T#1_199) -> [T#1_828]
  Op#549 SLICE(T#1_826, T#1_198, T#1_199) -> [T#1_829]
  Op#550 RESHAPE(T#1_827, T#1_263) -> [T#1_830]
  Op#551 TRANSPOSE(T#1_830, T#1_262) -> [T#1_831]
  Op#552 SLICE(T#1_831, T#1_202, T#1_197) -> [T#1_832]
  Op#553 SLICE(T#1_831, T#1_196, T#1_197) -> [T#1_833]
  Op#554 MUL(T#1_832, T#1_286) -> [T#1_834]
  Op#555 MUL(T#1_833, T#1_287) -> [T#1_835]
  Op#556 SUB(T#1_834, T#1_835) -> [T#1_836]
  Op#557 MUL(T#1_833, T#1_286) -> [T#1_837]
  Op#558 MUL(T#1_832, T#1_287) -> [T#1_838]
  Op#559 ADD(T#1_837, T#1_838) -> [T#1_839]
  Op#560 CONCATENATION(T#1_836, T#1_839) -> [T#1_840]
  Op#561 TRANSPOSE(T#1_840, T#1_262) -> [T#1_841]
  Op#562 TRANSPOSE(T#1_828, T#1_262) -> [T#1_842]
  Op#563 SLICE(T#1_842, T#1_202, T#1_195) -> [T#1_843]
  Op#564 SLICE(T#1_842, T#1_196, T#1_195) -> [T#1_844]
  Op#565 MUL(T#1_843, T#1_286) -> [T#1_845]
  Op#566 MUL(T#1_844, T#1_287) -> [T#1_846]
  Op#567 SUB(T#1_845, T#1_846) -> [T#1_847]
  Op#568 MUL(T#1_844, T#1_286) -> [T#1_848]
  Op#569 MUL(T#1_843, T#1_287) -> [T#1_849]
  Op#570 ADD(T#1_848, T#1_849) -> [T#1_850]
  Op#571 CONCATENATION(T#1_847, T#1_850) -> [T#1_851]
  Op#572 TRANSPOSE(T#1_851, T#1_262) -> [T#1_852]
  Op#573 DYNAMIC_UPDATE_SLICE(T#1_16, T#1_852, T#1_339) -> [T#1_853]
  Op#574 DYNAMIC_UPDATE_SLICE(T#1_46, T#1_829, T#1_339) -> [T#1_854]
  Op#575 STABLEHLO_COMPOSITE(T#1_841, T#1_853, T#1_854, T#1_300) -> [T#1_855]
  Op#576 RESHAPE(T#1_855, T#1_271) -> [T#1_856]
  Op#577 FULLY_CONNECTED(T#1_856, T#1_147, T#-1) -> [T#1_857]
  Op#578 ADD(T#1_816, T#1_857) -> [T#1_858]
  Op#579 MUL(T#1_858, T#1_858) -> [T#1_859]
  Op#580 SUM(T#1_859, T#1_273) -> [T#1_860]
  Op#581 MUL(T#1_860, T#1_203) -> [T#1_861]
  Op#582 ADD(T#1_861, T#1_277) -> [T#1_862]
  Op#583 RESHAPE(T#1_862, T#1_265) -> [T#1_863]
  Op#584 RSQRT(T#1_863) -> [T#1_864]
  Op#585 MUL(T#1_858, T#1_864) -> [T#1_865]
  Op#586 MUL(T#1_865, T#1_240) -> [T#1_866]
  Op#587 FULLY_CONNECTED(T#1_866, T#1_146, T#-1) -> [T#1_867]
  Op#588 LOGISTIC(T#1_867) -> [T#1_868]
  Op#589 MUL(T#1_867, T#1_868) -> [T#1_869]
  Op#590 FULLY_CONNECTED(T#1_866, T#1_145, T#-1) -> [T#1_870]
  Op#591 MUL(T#1_869, T#1_870) -> [T#1_871]
  Op#592 FULLY_CONNECTED(T#1_871, T#1_144, T#-1) -> [T#1_872]
  Op#593 ADD(T#1_858, T#1_872) -> [T#1_873]
  Op#594 MUL(T#1_873, T#1_873) -> [T#1_874]
  Op#595 SUM(T#1_874, T#1_273) -> [T#1_875]
  Op#596 MUL(T#1_875, T#1_203) -> [T#1_876]
  Op#597 ADD(T#1_876, T#1_277) -> [T#1_877]
  Op#598 RESHAPE(T#1_877, T#1_265) -> [T#1_878]
  Op#599 RSQRT(T#1_878) -> [T#1_879]
  Op#600 MUL(T#1_873, T#1_879) -> [T#1_880]
  Op#601 MUL(T#1_880, T#1_239) -> [T#1_881]
  Op#602 FULLY_CONNECTED(T#1_881, T#1_143, T#-1) -> [T#1_882]
  Op#603 RESHAPE(T#1_882, T#1_264) -> [T#1_883]
  Op#604 SLICE(T#1_883, T#1_202, T#1_201) -> [T#1_884]
  Op#605 SLICE(T#1_883, T#1_200, T#1_199) -> [T#1_885]
  Op#606 SLICE(T#1_883, T#1_198, T#1_199) -> [T#1_886]
  Op#607 RESHAPE(T#1_884, T#1_263) -> [T#1_887]
  Op#608 TRANSPOSE(T#1_887, T#1_262) -> [T#1_888]
  Op#609 SLICE(T#1_888, T#1_202, T#1_197) -> [T#1_889]
  Op#610 SLICE(T#1_888, T#1_196, T#1_197) -> [T#1_890]
  Op#611 MUL(T#1_889, T#1_286) -> [T#1_891]
  Op#612 MUL(T#1_890, T#1_287) -> [T#1_892]
  Op#613 SUB(T#1_891, T#1_892) -> [T#1_893]
  Op#614 MUL(T#1_890, T#1_286) -> [T#1_894]
  Op#615 MUL(T#1_889, T#1_287) -> [T#1_895]
  Op#616 ADD(T#1_894, T#1_895) -> [T#1_896]
  Op#617 CONCATENATION(T#1_893, T#1_896) -> [T#1_897]
  Op#618 TRANSPOSE(T#1_897, T#1_262) -> [T#1_898]
  Op#619 TRANSPOSE(T#1_885, T#1_262) -> [T#1_899]
  Op#620 SLICE(T#1_899, T#1_202, T#1_195) -> [T#1_900]
  Op#621 SLICE(T#1_899, T#1_196, T#1_195) -> [T#1_901]
  Op#622 MUL(T#1_900, T#1_286) -> [T#1_902]
  Op#623 MUL(T#1_901, T#1_287) -> [T#1_903]
  Op#624 SUB(T#1_902, T#1_903) -> [T#1_904]
  Op#625 MUL(T#1_901, T#1_286) -> [T#1_905]
  Op#626 MUL(T#1_900, T#1_287) -> [T#1_906]
  Op#627 ADD(T#1_905, T#1_906) -> [T#1_907]
  Op#628 CONCATENATION(T#1_904, T#1_907) -> [T#1_908]
  Op#629 TRANSPOSE(T#1_908, T#1_262) -> [T#1_909]
  Op#630 DYNAMIC_UPDATE_SLICE(T#1_8, T#1_909, T#1_339) -> [T#1_910]
  Op#631 DYNAMIC_UPDATE_SLICE(T#1_29, T#1_886, T#1_339) -> [T#1_911]
  Op#632 STABLEHLO_COMPOSITE(T#1_898, T#1_910, T#1_911, T#1_300) -> [T#1_912]
  Op#633 RESHAPE(T#1_912, T#1_271) -> [T#1_913]
  Op#634 FULLY_CONNECTED(T#1_913, T#1_142, T#-1) -> [T#1_914]
  Op#635 ADD(T#1_873, T#1_914) -> [T#1_915]
  Op#636 MUL(T#1_915, T#1_915) -> [T#1_916]
  Op#637 SUM(T#1_916, T#1_273) -> [T#1_917]
  Op#638 MUL(T#1_917, T#1_203) -> [T#1_918]
  Op#639 ADD(T#1_918, T#1_277) -> [T#1_919]
  Op#640 RESHAPE(T#1_919, T#1_265) -> [T#1_920]
  Op#641 RSQRT(T#1_920) -> [T#1_921]
  Op#642 MUL(T#1_915, T#1_921) -> [T#1_922]
  Op#643 MUL(T#1_922, T#1_238) -> [T#1_923]
  Op#644 FULLY_CONNECTED(T#1_923, T#1_141, T#-1) -> [T#1_924]
  Op#645 LOGISTIC(T#1_924) -> [T#1_925]
  Op#646 MUL(T#1_924, T#1_925) -> [T#1_926]
  Op#647 FULLY_CONNECTED(T#1_923, T#1_140, T#-1) -> [T#1_927]
  Op#648 MUL(T#1_926, T#1_927) -> [T#1_928]
  Op#649 FULLY_CONNECTED(T#1_928, T#1_139, T#-1) -> [T#1_929]
  Op#650 ADD(T#1_915, T#1_929) -> [T#1_930]
  Op#651 MUL(T#1_930, T#1_930) -> [T#1_931]
  Op#652 SUM(T#1_931, T#1_273) -> [T#1_932]
  Op#653 MUL(T#1_932, T#1_203) -> [T#1_933]
  Op#654 ADD(T#1_933, T#1_277) -> [T#1_934]
  Op#655 RESHAPE(T#1_934, T#1_265) -> [T#1_935]
  Op#656 RSQRT(T#1_935) -> [T#1_936]
  Op#657 MUL(T#1_930, T#1_936) -> [T#1_937]
  Op#658 MUL(T#1_937, T#1_237) -> [T#1_938]
  Op#659 FULLY_CONNECTED(T#1_938, T#1_138, T#-1) -> [T#1_939]
  Op#660 RESHAPE(T#1_939, T#1_264) -> [T#1_940]
  Op#661 SLICE(T#1_940, T#1_202, T#1_201) -> [T#1_941]
  Op#662 SLICE(T#1_940, T#1_200, T#1_199) -> [T#1_942]
  Op#663 SLICE(T#1_940, T#1_198, T#1_199) -> [T#1_943]
  Op#664 RESHAPE(T#1_941, T#1_263) -> [T#1_944]
  Op#665 TRANSPOSE(T#1_944, T#1_262) -> [T#1_945]
  Op#666 SLICE(T#1_945, T#1_202, T#1_197) -> [T#1_946]
  Op#667 SLICE(T#1_945, T#1_196, T#1_197) -> [T#1_947]
  Op#668 MUL(T#1_946, T#1_286) -> [T#1_948]
  Op#669 MUL(T#1_947, T#1_287) -> [T#1_949]
  Op#670 SUB(T#1_948, T#1_949) -> [T#1_950]
  Op#671 MUL(T#1_947, T#1_286) -> [T#1_951]
  Op#672 MUL(T#1_946, T#1_287) -> [T#1_952]
  Op#673 ADD(T#1_951, T#1_952) -> [T#1_953]
  Op#674 CONCATENATION(T#1_950, T#1_953) -> [T#1_954]
  Op#675 TRANSPOSE(T#1_954, T#1_262) -> [T#1_955]
  Op#676 TRANSPOSE(T#1_942, T#1_262) -> [T#1_956]
  Op#677 SLICE(T#1_956, T#1_202, T#1_195) -> [T#1_957]
  Op#678 SLICE(T#1_956, T#1_196, T#1_195) -> [T#1_958]
  Op#679 MUL(T#1_957, T#1_286) -> [T#1_959]
  Op#680 MUL(T#1_958, T#1_287) -> [T#1_960]
  Op#681 SUB(T#1_959, T#1_960) -> [T#1_961]
  Op#682 MUL(T#1_958, T#1_286) -> [T#1_962]
  Op#683 MUL(T#1_957, T#1_287) -> [T#1_963]
  Op#684 ADD(T#1_962, T#1_963) -> [T#1_964]
  Op#685 CONCATENATION(T#1_961, T#1_964) -> [T#1_965]
  Op#686 TRANSPOSE(T#1_965, T#1_262) -> [T#1_966]
  Op#687 DYNAMIC_UPDATE_SLICE(T#1_52, T#1_966, T#1_339) -> [T#1_967]
  Op#688 DYNAMIC_UPDATE_SLICE(T#1_22, T#1_943, T#1_339) -> [T#1_968]
  Op#689 STABLEHLO_COMPOSITE(T#1_955, T#1_967, T#1_968, T#1_300) -> [T#1_969]
  Op#690 RESHAPE(T#1_969, T#1_271) -> [T#1_970]
  Op#691 FULLY_CONNECTED(T#1_970, T#1_137, T#-1) -> [T#1_971]
  Op#692 ADD(T#1_930, T#1_971) -> [T#1_972]
  Op#693 MUL(T#1_972, T#1_972) -> [T#1_973]
  Op#694 SUM(T#1_973, T#1_273) -> [T#1_974]
  Op#695 MUL(T#1_974, T#1_203) -> [T#1_975]
  Op#696 ADD(T#1_975, T#1_277) -> [T#1_976]
  Op#697 RESHAPE(T#1_976, T#1_265) -> [T#1_977]
  Op#698 RSQRT(T#1_977) -> [T#1_978]
  Op#699 MUL(T#1_972, T#1_978) -> [T#1_979]
  Op#700 MUL(T#1_979, T#1_236) -> [T#1_980]
  Op#701 FULLY_CONNECTED(T#1_980, T#1_136, T#-1) -> [T#1_981]
  Op#702 LOGISTIC(T#1_981) -> [T#1_982]
  Op#703 MUL(T#1_981, T#1_982) -> [T#1_983]
  Op#704 FULLY_CONNECTED(T#1_980, T#1_135, T#-1) -> [T#1_984]
  Op#705 MUL(T#1_983, T#1_984) -> [T#1_985]
  Op#706 FULLY_CONNECTED(T#1_985, T#1_134, T#-1) -> [T#1_986]
  Op#707 ADD(T#1_972, T#1_986) -> [T#1_987]
  Op#708 MUL(T#1_987, T#1_987) -> [T#1_988]
  Op#709 SUM(T#1_988, T#1_273) -> [T#1_989]
  Op#710 MUL(T#1_989, T#1_203) -> [T#1_990]
  Op#711 ADD(T#1_990, T#1_277) -> [T#1_991]
  Op#712 RESHAPE(T#1_991, T#1_265) -> [T#1_992]
  Op#713 RSQRT(T#1_992) -> [T#1_993]
  Op#714 MUL(T#1_987, T#1_993) -> [T#1_994]
  Op#715 MUL(T#1_994, T#1_235) -> [T#1_995]
  Op#716 FULLY_CONNECTED(T#1_995, T#1_133, T#-1) -> [T#1_996]
  Op#717 RESHAPE(T#1_996, T#1_264) -> [T#1_997]
  Op#718 SLICE(T#1_997, T#1_202, T#1_201) -> [T#1_998]
  Op#719 SLICE(T#1_997, T#1_200, T#1_199) -> [T#1_999]
  Op#720 SLICE(T#1_997, T#1_198, T#1_199) -> [T#1_1000]
  Op#721 RESHAPE(T#1_998, T#1_263) -> [T#1_1001]
  Op#722 TRANSPOSE(T#1_1001, T#1_262) -> [T#1_1002]
  Op#723 SLICE(T#1_1002, T#1_202, T#1_197) -> [T#1_1003]
  Op#724 SLICE(T#1_1002, T#1_196, T#1_197) -> [T#1_1004]
  Op#725 MUL(T#1_1003, T#1_286) -> [T#1_1005]
  Op#726 MUL(T#1_1004, T#1_287) -> [T#1_1006]
  Op#727 SUB(T#1_1005, T#1_1006) -> [T#1_1007]
  Op#728 MUL(T#1_1004, T#1_286) -> [T#1_1008]
  Op#729 MUL(T#1_1003, T#1_287) -> [T#1_1009]
  Op#730 ADD(T#1_1008, T#1_1009) -> [T#1_1010]
  Op#731 CONCATENATION(T#1_1007, T#1_1010) -> [T#1_1011]
  Op#732 TRANSPOSE(T#1_1011, T#1_262) -> [T#1_1012]
  Op#733 TRANSPOSE(T#1_999, T#1_262) -> [T#1_1013]
  Op#734 SLICE(T#1_1013, T#1_202, T#1_195) -> [T#1_1014]
  Op#735 SLICE(T#1_1013, T#1_196, T#1_195) -> [T#1_1015]
  Op#736 MUL(T#1_1014, T#1_286) -> [T#1_1016]
  Op#737 MUL(T#1_1015, T#1_287) -> [T#1_1017]
  Op#738 SUB(T#1_1016, T#1_1017) -> [T#1_1018]
  Op#739 MUL(T#1_1015, T#1_286) -> [T#1_1019]
  Op#740 MUL(T#1_1014, T#1_287) -> [T#1_1020]
  Op#741 ADD(T#1_1019, T#1_1020) -> [T#1_1021]
  Op#742 CONCATENATION(T#1_1018, T#1_1021) -> [T#1_1022]
  Op#743 TRANSPOSE(T#1_1022, T#1_262) -> [T#1_1023]
  Op#744 DYNAMIC_UPDATE_SLICE(T#1_20, T#1_1023, T#1_339) -> [T#1_1024]
  Op#745 DYNAMIC_UPDATE_SLICE(T#1_31, T#1_1000, T#1_339) -> [T#1_1025]
  Op#746 STABLEHLO_COMPOSITE(T#1_1012, T#1_1024, T#1_1025, T#1_300) -> [T#1_1026]
  Op#747 RESHAPE(T#1_1026, T#1_271) -> [T#1_1027]
  Op#748 FULLY_CONNECTED(T#1_1027, T#1_132, T#-1) -> [T#1_1028]
  Op#749 ADD(T#1_987, T#1_1028) -> [T#1_1029]
  Op#750 MUL(T#1_1029, T#1_1029) -> [T#1_1030]
  Op#751 SUM(T#1_1030, T#1_273) -> [T#1_1031]
  Op#752 MUL(T#1_1031, T#1_203) -> [T#1_1032]
  Op#753 ADD(T#1_1032, T#1_277) -> [T#1_1033]
  Op#754 RESHAPE(T#1_1033, T#1_265) -> [T#1_1034]
  Op#755 RSQRT(T#1_1034) -> [T#1_1035]
  Op#756 MUL(T#1_1029, T#1_1035) -> [T#1_1036]
  Op#757 MUL(T#1_1036, T#1_234) -> [T#1_1037]
  Op#758 FULLY_CONNECTED(T#1_1037, T#1_131, T#-1) -> [T#1_1038]
  Op#759 LOGISTIC(T#1_1038) -> [T#1_1039]
  Op#760 MUL(T#1_1038, T#1_1039) -> [T#1_1040]
  Op#761 FULLY_CONNECTED(T#1_1037, T#1_130, T#-1) -> [T#1_1041]
  Op#762 MUL(T#1_1040, T#1_1041) -> [T#1_1042]
  Op#763 FULLY_CONNECTED(T#1_1042, T#1_129, T#-1) -> [T#1_1043]
  Op#764 ADD(T#1_1029, T#1_1043) -> [T#1_1044]
  Op#765 MUL(T#1_1044, T#1_1044) -> [T#1_1045]
  Op#766 SUM(T#1_1045, T#1_273) -> [T#1_1046]
  Op#767 MUL(T#1_1046, T#1_203) -> [T#1_1047]
  Op#768 ADD(T#1_1047, T#1_277) -> [T#1_1048]
  Op#769 RESHAPE(T#1_1048, T#1_265) -> [T#1_1049]
  Op#770 RSQRT(T#1_1049) -> [T#1_1050]
  Op#771 MUL(T#1_1044, T#1_1050) -> [T#1_1051]
  Op#772 MUL(T#1_1051, T#1_233) -> [T#1_1052]
  Op#773 FULLY_CONNECTED(T#1_1052, T#1_128, T#-1) -> [T#1_1053]
  Op#774 RESHAPE(T#1_1053, T#1_264) -> [T#1_1054]
  Op#775 SLICE(T#1_1054, T#1_202, T#1_201) -> [T#1_1055]
  Op#776 SLICE(T#1_1054, T#1_200, T#1_199) -> [T#1_1056]
  Op#777 SLICE(T#1_1054, T#1_198, T#1_199) -> [T#1_1057]
  Op#778 RESHAPE(T#1_1055, T#1_263) -> [T#1_1058]
  Op#779 TRANSPOSE(T#1_1058, T#1_262) -> [T#1_1059]
  Op#780 SLICE(T#1_1059, T#1_202, T#1_197) -> [T#1_1060]
  Op#781 SLICE(T#1_1059, T#1_196, T#1_197) -> [T#1_1061]
  Op#782 MUL(T#1_1060, T#1_286) -> [T#1_1062]
  Op#783 MUL(T#1_1061, T#1_287) -> [T#1_1063]
  Op#784 SUB(T#1_1062, T#1_1063) -> [T#1_1064]
  Op#785 MUL(T#1_1061, T#1_286) -> [T#1_1065]
  Op#786 MUL(T#1_1060, T#1_287) -> [T#1_1066]
  Op#787 ADD(T#1_1065, T#1_1066) -> [T#1_1067]
  Op#788 CONCATENATION(T#1_1064, T#1_1067) -> [T#1_1068]
  Op#789 TRANSPOSE(T#1_1068, T#1_262) -> [T#1_1069]
  Op#790 TRANSPOSE(T#1_1056, T#1_262) -> [T#1_1070]
  Op#791 SLICE(T#1_1070, T#1_202, T#1_195) -> [T#1_1071]
  Op#792 SLICE(T#1_1070, T#1_196, T#1_195) -> [T#1_1072]
  Op#793 MUL(T#1_1071, T#1_286) -> [T#1_1073]
  Op#794 MUL(T#1_1072, T#1_287) -> [T#1_1074]
  Op#795 SUB(T#1_1073, T#1_1074) -> [T#1_1075]
  Op#796 MUL(T#1_1072, T#1_286) -> [T#1_1076]
  Op#797 MUL(T#1_1071, T#1_287) -> [T#1_1077]
  Op#798 ADD(T#1_1076, T#1_1077) -> [T#1_1078]
  Op#799 CONCATENATION(T#1_1075, T#1_1078) -> [T#1_1079]
  Op#800 TRANSPOSE(T#1_1079, T#1_262) -> [T#1_1080]
  Op#801 DYNAMIC_UPDATE_SLICE(T#1_35, T#1_1080, T#1_339) -> [T#1_1081]
  Op#802 DYNAMIC_UPDATE_SLICE(T#1_41, T#1_1057, T#1_339) -> [T#1_1082]
  Op#803 STABLEHLO_COMPOSITE(T#1_1069, T#1_1081, T#1_1082, T#1_300) -> [T#1_1083]
  Op#804 RESHAPE(T#1_1083, T#1_271) -> [T#1_1084]
  Op#805 FULLY_CONNECTED(T#1_1084, T#1_127, T#-1) -> [T#1_1085]
  Op#806 ADD(T#1_1044, T#1_1085) -> [T#1_1086]
  Op#807 MUL(T#1_1086, T#1_1086) -> [T#1_1087]
  Op#808 SUM(T#1_1087, T#1_273) -> [T#1_1088]
  Op#809 MUL(T#1_1088, T#1_203) -> [T#1_1089]
  Op#810 ADD(T#1_1089, T#1_277) -> [T#1_1090]
  Op#811 RESHAPE(T#1_1090, T#1_265) -> [T#1_1091]
  Op#812 RSQRT(T#1_1091) -> [T#1_1092]
  Op#813 MUL(T#1_1086, T#1_1092) -> [T#1_1093]
  Op#814 MUL(T#1_1093, T#1_232) -> [T#1_1094]
  Op#815 FULLY_CONNECTED(T#1_1094, T#1_126, T#-1) -> [T#1_1095]
  Op#816 LOGISTIC(T#1_1095) -> [T#1_1096]
  Op#817 MUL(T#1_1095, T#1_1096) -> [T#1_1097]
  Op#818 FULLY_CONNECTED(T#1_1094, T#1_125, T#-1) -> [T#1_1098]
  Op#819 MUL(T#1_1097, T#1_1098) -> [T#1_1099]
  Op#820 FULLY_CONNECTED(T#1_1099, T#1_124, T#-1) -> [T#1_1100]
  Op#821 ADD(T#1_1086, T#1_1100) -> [T#1_1101]
  Op#822 MUL(T#1_1101, T#1_1101) -> [T#1_1102]
  Op#823 SUM(T#1_1102, T#1_273) -> [T#1_1103]
  Op#824 MUL(T#1_1103, T#1_203) -> [T#1_1104]
  Op#825 ADD(T#1_1104, T#1_277) -> [T#1_1105]
  Op#826 RESHAPE(T#1_1105, T#1_265) -> [T#1_1106]
  Op#827 RSQRT(T#1_1106) -> [T#1_1107]
  Op#828 MUL(T#1_1101, T#1_1107) -> [T#1_1108]
  Op#829 MUL(T#1_1108, T#1_231) -> [T#1_1109]
  Op#830 FULLY_CONNECTED(T#1_1109, T#1_123, T#-1) -> [T#1_1110]
  Op#831 RESHAPE(T#1_1110, T#1_264) -> [T#1_1111]
  Op#832 SLICE(T#1_1111, T#1_202, T#1_201) -> [T#1_1112]
  Op#833 SLICE(T#1_1111, T#1_200, T#1_199) -> [T#1_1113]
  Op#834 SLICE(T#1_1111, T#1_198, T#1_199) -> [T#1_1114]
  Op#835 RESHAPE(T#1_1112, T#1_263) -> [T#1_1115]
  Op#836 TRANSPOSE(T#1_1115, T#1_262) -> [T#1_1116]
  Op#837 SLICE(T#1_1116, T#1_202, T#1_197) -> [T#1_1117]
  Op#838 SLICE(T#1_1116, T#1_196, T#1_197) -> [T#1_1118]
  Op#839 MUL(T#1_1117, T#1_286) -> [T#1_1119]
  Op#840 MUL(T#1_1118, T#1_287) -> [T#1_1120]
  Op#841 SUB(T#1_1119, T#1_1120) -> [T#1_1121]
  Op#842 MUL(T#1_1118, T#1_286) -> [T#1_1122]
  Op#843 MUL(T#1_1117, T#1_287) -> [T#1_1123]
  Op#844 ADD(T#1_1122, T#1_1123) -> [T#1_1124]
  Op#845 CONCATENATION(T#1_1121, T#1_1124) -> [T#1_1125]
  Op#846 TRANSPOSE(T#1_1125, T#1_262) -> [T#1_1126]
  Op#847 TRANSPOSE(T#1_1113, T#1_262) -> [T#1_1127]
  Op#848 SLICE(T#1_1127, T#1_202, T#1_195) -> [T#1_1128]
  Op#849 SLICE(T#1_1127, T#1_196, T#1_195) -> [T#1_1129]
  Op#850 MUL(T#1_1128, T#1_286) -> [T#1_1130]
  Op#851 MUL(T#1_1129, T#1_287) -> [T#1_1131]
  Op#852 SUB(T#1_1130, T#1_1131) -> [T#1_1132]
  Op#853 MUL(T#1_1129, T#1_286) -> [T#1_1133]
  Op#854 MUL(T#1_1128, T#1_287) -> [T#1_1134]
  Op#855 ADD(T#1_1133, T#1_1134) -> [T#1_1135]
  Op#856 CONCATENATION(T#1_1132, T#1_1135) -> [T#1_1136]
  Op#857 TRANSPOSE(T#1_1136, T#1_262) -> [T#1_1137]
  Op#858 DYNAMIC_UPDATE_SLICE(T#1_33, T#1_1137, T#1_339) -> [T#1_1138]
  Op#859 DYNAMIC_UPDATE_SLICE(T#1_57, T#1_1114, T#1_339) -> [T#1_1139]
  Op#860 STABLEHLO_COMPOSITE(T#1_1126, T#1_1138, T#1_1139, T#1_300) -> [T#1_1140]
  Op#861 RESHAPE(T#1_1140, T#1_271) -> [T#1_1141]
  Op#862 FULLY_CONNECTED(T#1_1141, T#1_122, T#-1) -> [T#1_1142]
  Op#863 ADD(T#1_1101, T#1_1142) -> [T#1_1143]
  Op#864 MUL(T#1_1143, T#1_1143) -> [T#1_1144]
  Op#865 SUM(T#1_1144, T#1_273) -> [T#1_1145]
  Op#866 MUL(T#1_1145, T#1_203) -> [T#1_1146]
  Op#867 ADD(T#1_1146, T#1_277) -> [T#1_1147]
  Op#868 RESHAPE(T#1_1147, T#1_265) -> [T#1_1148]
  Op#869 RSQRT(T#1_1148) -> [T#1_1149]
  Op#870 MUL(T#1_1143, T#1_1149) -> [T#1_1150]
  Op#871 MUL(T#1_1150, T#1_230) -> [T#1_1151]
  Op#872 FULLY_CONNECTED(T#1_1151, T#1_121, T#-1) -> [T#1_1152]
  Op#873 LOGISTIC(T#1_1152) -> [T#1_1153]
  Op#874 MUL(T#1_1152, T#1_1153) -> [T#1_1154]
  Op#875 FULLY_CONNECTED(T#1_1151, T#1_120, T#-1) -> [T#1_1155]
  Op#876 MUL(T#1_1154, T#1_1155) -> [T#1_1156]
  Op#877 FULLY_CONNECTED(T#1_1156, T#1_119, T#-1) -> [T#1_1157]
  Op#878 ADD(T#1_1143, T#1_1157) -> [T#1_1158]
  Op#879 MUL(T#1_1158, T#1_1158) -> [T#1_1159]
  Op#880 SUM(T#1_1159, T#1_273) -> [T#1_1160]
  Op#881 MUL(T#1_1160, T#1_203) -> [T#1_1161]
  Op#882 ADD(T#1_1161, T#1_277) -> [T#1_1162]
  Op#883 RESHAPE(T#1_1162, T#1_265) -> [T#1_1163]
  Op#884 RSQRT(T#1_1163) -> [T#1_1164]
  Op#885 MUL(T#1_1158, T#1_1164) -> [T#1_1165]
  Op#886 MUL(T#1_1165, T#1_229) -> [T#1_1166]
  Op#887 FULLY_CONNECTED(T#1_1166, T#1_118, T#-1) -> [T#1_1167]
  Op#888 RESHAPE(T#1_1167, T#1_264) -> [T#1_1168]
  Op#889 SLICE(T#1_1168, T#1_202, T#1_201) -> [T#1_1169]
  Op#890 SLICE(T#1_1168, T#1_200, T#1_199) -> [T#1_1170]
  Op#891 SLICE(T#1_1168, T#1_198, T#1_199) -> [T#1_1171]
  Op#892 RESHAPE(T#1_1169, T#1_263) -> [T#1_1172]
  Op#893 TRANSPOSE(T#1_1172, T#1_262) -> [T#1_1173]
  Op#894 SLICE(T#1_1173, T#1_202, T#1_197) -> [T#1_1174]
  Op#895 SLICE(T#1_1173, T#1_196, T#1_197) -> [T#1_1175]
  Op#896 MUL(T#1_1174, T#1_286) -> [T#1_1176]
  Op#897 MUL(T#1_1175, T#1_287) -> [T#1_1177]
  Op#898 SUB(T#1_1176, T#1_1177) -> [T#1_1178]
  Op#899 MUL(T#1_1175, T#1_286) -> [T#1_1179]
  Op#900 MUL(T#1_1174, T#1_287) -> [T#1_1180]
  Op#901 ADD(T#1_1179, T#1_1180) -> [T#1_1181]
  Op#902 CONCATENATION(T#1_1178, T#1_1181) -> [T#1_1182]
  Op#903 TRANSPOSE(T#1_1182, T#1_262) -> [T#1_1183]
  Op#904 TRANSPOSE(T#1_1170, T#1_262) -> [T#1_1184]
  Op#905 SLICE(T#1_1184, T#1_202, T#1_195) -> [T#1_1185]
  Op#906 SLICE(T#1_1184, T#1_196, T#1_195) -> [T#1_1186]
  Op#907 MUL(T#1_1185, T#1_286) -> [T#1_1187]
  Op#908 MUL(T#1_1186, T#1_287) -> [T#1_1188]
  Op#909 SUB(T#1_1187, T#1_1188) -> [T#1_1189]
  Op#910 MUL(T#1_1186, T#1_286) -> [T#1_1190]
  Op#911 MUL(T#1_1185, T#1_287) -> [T#1_1191]
  Op#912 ADD(T#1_1190, T#1_1191) -> [T#1_1192]
  Op#913 CONCATENATION(T#1_1189, T#1_1192) -> [T#1_1193]
  Op#914 TRANSPOSE(T#1_1193, T#1_262) -> [T#1_1194]
  Op#915 DYNAMIC_UPDATE_SLICE(T#1_45, T#1_1194, T#1_339) -> [T#1_1195]
  Op#916 DYNAMIC_UPDATE_SLICE(T#1_34, T#1_1171, T#1_339) -> [T#1_1196]
  Op#917 STABLEHLO_COMPOSITE(T#1_1183, T#1_1195, T#1_1196, T#1_300) -> [T#1_1197]
  Op#918 RESHAPE(T#1_1197, T#1_271) -> [T#1_1198]
  Op#919 FULLY_CONNECTED(T#1_1198, T#1_117, T#-1) -> [T#1_1199]
  Op#920 ADD(T#1_1158, T#1_1199) -> [T#1_1200]
  Op#921 MUL(T#1_1200, T#1_1200) -> [T#1_1201]
  Op#922 SUM(T#1_1201, T#1_273) -> [T#1_1202]
  Op#923 MUL(T#1_1202, T#1_203) -> [T#1_1203]
  Op#924 ADD(T#1_1203, T#1_277) -> [T#1_1204]
  Op#925 RESHAPE(T#1_1204, T#1_265) -> [T#1_1205]
  Op#926 RSQRT(T#1_1205) -> [T#1_1206]
  Op#927 MUL(T#1_1200, T#1_1206) -> [T#1_1207]
  Op#928 MUL(T#1_1207, T#1_228) -> [T#1_1208]
  Op#929 FULLY_CONNECTED(T#1_1208, T#1_116, T#-1) -> [T#1_1209]
  Op#930 LOGISTIC(T#1_1209) -> [T#1_1210]
  Op#931 MUL(T#1_1209, T#1_1210) -> [T#1_1211]
  Op#932 FULLY_CONNECTED(T#1_1208, T#1_115, T#-1) -> [T#1_1212]
  Op#933 MUL(T#1_1211, T#1_1212) -> [T#1_1213]
  Op#934 FULLY_CONNECTED(T#1_1213, T#1_114, T#-1) -> [T#1_1214]
  Op#935 ADD(T#1_1200, T#1_1214) -> [T#1_1215]
  Op#936 MUL(T#1_1215, T#1_1215) -> [T#1_1216]
  Op#937 SUM(T#1_1216, T#1_273) -> [T#1_1217]
  Op#938 MUL(T#1_1217, T#1_203) -> [T#1_1218]
  Op#939 ADD(T#1_1218, T#1_277) -> [T#1_1219]
  Op#940 RESHAPE(T#1_1219, T#1_265) -> [T#1_1220]
  Op#941 RSQRT(T#1_1220) -> [T#1_1221]
  Op#942 MUL(T#1_1215, T#1_1221) -> [T#1_1222]
  Op#943 MUL(T#1_1222, T#1_227) -> [T#1_1223]
  Op#944 FULLY_CONNECTED(T#1_1223, T#1_113, T#-1) -> [T#1_1224]
  Op#945 RESHAPE(T#1_1224, T#1_264) -> [T#1_1225]
  Op#946 SLICE(T#1_1225, T#1_202, T#1_201) -> [T#1_1226]
  Op#947 SLICE(T#1_1225, T#1_200, T#1_199) -> [T#1_1227]
  Op#948 SLICE(T#1_1225, T#1_198, T#1_199) -> [T#1_1228]
  Op#949 RESHAPE(T#1_1226, T#1_263) -> [T#1_1229]
  Op#950 TRANSPOSE(T#1_1229, T#1_262) -> [T#1_1230]
  Op#951 SLICE(T#1_1230, T#1_202, T#1_197) -> [T#1_1231]
  Op#952 SLICE(T#1_1230, T#1_196, T#1_197) -> [T#1_1232]
  Op#953 MUL(T#1_1231, T#1_286) -> [T#1_1233]
  Op#954 MUL(T#1_1232, T#1_287) -> [T#1_1234]
  Op#955 SUB(T#1_1233, T#1_1234) -> [T#1_1235]
  Op#956 MUL(T#1_1232, T#1_286) -> [T#1_1236]
  Op#957 MUL(T#1_1231, T#1_287) -> [T#1_1237]
  Op#958 ADD(T#1_1236, T#1_1237) -> [T#1_1238]
  Op#959 CONCATENATION(T#1_1235, T#1_1238) -> [T#1_1239]
  Op#960 TRANSPOSE(T#1_1239, T#1_262) -> [T#1_1240]
  Op#961 TRANSPOSE(T#1_1227, T#1_262) -> [T#1_1241]
  Op#962 SLICE(T#1_1241, T#1_202, T#1_195) -> [T#1_1242]
  Op#963 SLICE(T#1_1241, T#1_196, T#1_195) -> [T#1_1243]
  Op#964 MUL(T#1_1242, T#1_286) -> [T#1_1244]
  Op#965 MUL(T#1_1243, T#1_287) -> [T#1_1245]
  Op#966 SUB(T#1_1244, T#1_1245) -> [T#1_1246]
  Op#967 MUL(T#1_1243, T#1_286) -> [T#1_1247]
  Op#968 MUL(T#1_1242, T#1_287) -> [T#1_1248]
  Op#969 ADD(T#1_1247, T#1_1248) -> [T#1_1249]
  Op#970 CONCATENATION(T#1_1246, T#1_1249) -> [T#1_1250]
  Op#971 TRANSPOSE(T#1_1250, T#1_262) -> [T#1_1251]
  Op#972 DYNAMIC_UPDATE_SLICE(T#1_3, T#1_1251, T#1_339) -> [T#1_1252]
  Op#973 DYNAMIC_UPDATE_SLICE(T#1_30, T#1_1228, T#1_339) -> [T#1_1253]
  Op#974 STABLEHLO_COMPOSITE(T#1_1240, T#1_1252, T#1_1253, T#1_300) -> [T#1_1254]
  Op#975 RESHAPE(T#1_1254, T#1_271) -> [T#1_1255]
  Op#976 FULLY_CONNECTED(T#1_1255, T#1_112, T#-1) -> [T#1_1256]
  Op#977 ADD(T#1_1215, T#1_1256) -> [T#1_1257]
  Op#978 MUL(T#1_1257, T#1_1257) -> [T#1_1258]
  Op#979 SUM(T#1_1258, T#1_273) -> [T#1_1259]
  Op#980 MUL(T#1_1259, T#1_203) -> [T#1_1260]
  Op#981 ADD(T#1_1260, T#1_277) -> [T#1_1261]
  Op#982 RESHAPE(T#1_1261, T#1_265) -> [T#1_1262]
  Op#983 RSQRT(T#1_1262) -> [T#1_1263]
  Op#984 MUL(T#1_1257, T#1_1263) -> [T#1_1264]
  Op#985 MUL(T#1_1264, T#1_226) -> [T#1_1265]
  Op#986 FULLY_CONNECTED(T#1_1265, T#1_111, T#-1) -> [T#1_1266]
  Op#987 LOGISTIC(T#1_1266) -> [T#1_1267]
  Op#988 MUL(T#1_1266, T#1_1267) -> [T#1_1268]
  Op#989 FULLY_CONNECTED(T#1_1265, T#1_110, T#-1) -> [T#1_1269]
  Op#990 MUL(T#1_1268, T#1_1269) -> [T#1_1270]
  Op#991 FULLY_CONNECTED(T#1_1270, T#1_109, T#-1) -> [T#1_1271]
  Op#992 ADD(T#1_1257, T#1_1271) -> [T#1_1272]
  Op#993 MUL(T#1_1272, T#1_1272) -> [T#1_1273]
  Op#994 SUM(T#1_1273, T#1_273) -> [T#1_1274]
  Op#995 MUL(T#1_1274, T#1_203) -> [T#1_1275]
  Op#996 ADD(T#1_1275, T#1_277) -> [T#1_1276]
  Op#997 RESHAPE(T#1_1276, T#1_265) -> [T#1_1277]
  Op#998 RSQRT(T#1_1277) -> [T#1_1278]
  Op#999 MUL(T#1_1272, T#1_1278) -> [T#1_1279]
  Op#1000 MUL(T#1_1279, T#1_225) -> [T#1_1280]
  Op#1001 FULLY_CONNECTED(T#1_1280, T#1_108, T#-1) -> [T#1_1281]
  Op#1002 RESHAPE(T#1_1281, T#1_264) -> [T#1_1282]
  Op#1003 SLICE(T#1_1282, T#1_202, T#1_201) -> [T#1_1283]
  Op#1004 SLICE(T#1_1282, T#1_200, T#1_199) -> [T#1_1284]
  Op#1005 SLICE(T#1_1282, T#1_198, T#1_199) -> [T#1_1285]
  Op#1006 RESHAPE(T#1_1283, T#1_263) -> [T#1_1286]
  Op#1007 TRANSPOSE(T#1_1286, T#1_262) -> [T#1_1287]
  Op#1008 SLICE(T#1_1287, T#1_202, T#1_197) -> [T#1_1288]
  Op#1009 SLICE(T#1_1287, T#1_196, T#1_197) -> [T#1_1289]
  Op#1010 MUL(T#1_1288, T#1_286) -> [T#1_1290]
  Op#1011 MUL(T#1_1289, T#1_287) -> [T#1_1291]
  Op#1012 SUB(T#1_1290, T#1_1291) -> [T#1_1292]
  Op#1013 MUL(T#1_1289, T#1_286) -> [T#1_1293]
  Op#1014 MUL(T#1_1288, T#1_287) -> [T#1_1294]
  Op#1015 ADD(T#1_1293, T#1_1294) -> [T#1_1295]
  Op#1016 CONCATENATION(T#1_1292, T#1_1295) -> [T#1_1296]
  Op#1017 TRANSPOSE(T#1_1296, T#1_262) -> [T#1_1297]
  Op#1018 TRANSPOSE(T#1_1284, T#1_262) -> [T#1_1298]
  Op#1019 SLICE(T#1_1298, T#1_202, T#1_195) -> [T#1_1299]
  Op#1020 SLICE(T#1_1298, T#1_196, T#1_195) -> [T#1_1300]
  Op#1021 MUL(T#1_1299, T#1_286) -> [T#1_1301]
  Op#1022 MUL(T#1_1300, T#1_287) -> [T#1_1302]
  Op#1023 SUB(T#1_1301, T#1_1302) -> [T#1_1303]
  Op#1024 MUL(T#1_1300, T#1_286) -> [T#1_1304]
  Op#1025 MUL(T#1_1299, T#1_287) -> [T#1_1305]
  Op#1026 ADD(T#1_1304, T#1_1305) -> [T#1_1306]
  Op#1027 CONCATENATION(T#1_1303, T#1_1306) -> [T#1_1307]
  Op#1028 TRANSPOSE(T#1_1307, T#1_262) -> [T#1_1308]
  Op#1029 DYNAMIC_UPDATE_SLICE(T#1_10, T#1_1308, T#1_339) -> [T#1_1309]
  Op#1030 DYNAMIC_UPDATE_SLICE(T#1_40, T#1_1285, T#1_339) -> [T#1_1310]
  Op#1031 STABLEHLO_COMPOSITE(T#1_1297, T#1_1309, T#1_1310, T#1_300) -> [T#1_1311]
  Op#1032 RESHAPE(T#1_1311, T#1_271) -> [T#1_1312]
  Op#1033 FULLY_CONNECTED(T#1_1312, T#1_107, T#-1) -> [T#1_1313]
  Op#1034 ADD(T#1_1272, T#1_1313) -> [T#1_1314]
  Op#1035 MUL(T#1_1314, T#1_1314) -> [T#1_1315]
  Op#1036 SUM(T#1_1315, T#1_273) -> [T#1_1316]
  Op#1037 MUL(T#1_1316, T#1_203) -> [T#1_1317]
  Op#1038 ADD(T#1_1317, T#1_277) -> [T#1_1318]
  Op#1039 RESHAPE(T#1_1318, T#1_265) -> [T#1_1319]
  Op#1040 RSQRT(T#1_1319) -> [T#1_1320]
  Op#1041 MUL(T#1_1314, T#1_1320) -> [T#1_1321]
  Op#1042 MUL(T#1_1321, T#1_224) -> [T#1_1322]
  Op#1043 FULLY_CONNECTED(T#1_1322, T#1_106, T#-1) -> [T#1_1323]
  Op#1044 LOGISTIC(T#1_1323) -> [T#1_1324]
  Op#1045 MUL(T#1_1323, T#1_1324) -> [T#1_1325]
  Op#1046 FULLY_CONNECTED(T#1_1322, T#1_105, T#-1) -> [T#1_1326]
  Op#1047 MUL(T#1_1325, T#1_1326) -> [T#1_1327]
  Op#1048 FULLY_CONNECTED(T#1_1327, T#1_104, T#-1) -> [T#1_1328]
  Op#1049 ADD(T#1_1314, T#1_1328) -> [T#1_1329]
  Op#1050 MUL(T#1_1329, T#1_1329) -> [T#1_1330]
  Op#1051 SUM(T#1_1330, T#1_273) -> [T#1_1331]
  Op#1052 MUL(T#1_1331, T#1_203) -> [T#1_1332]
  Op#1053 ADD(T#1_1332, T#1_277) -> [T#1_1333]
  Op#1054 RESHAPE(T#1_1333, T#1_265) -> [T#1_1334]
  Op#1055 RSQRT(T#1_1334) -> [T#1_1335]
  Op#1056 MUL(T#1_1329, T#1_1335) -> [T#1_1336]
  Op#1057 MUL(T#1_1336, T#1_223) -> [T#1_1337]
  Op#1058 FULLY_CONNECTED(T#1_1337, T#1_103, T#-1) -> [T#1_1338]
  Op#1059 RESHAPE(T#1_1338, T#1_264) -> [T#1_1339]
  Op#1060 SLICE(T#1_1339, T#1_202, T#1_201) -> [T#1_1340]
  Op#1061 SLICE(T#1_1339, T#1_200, T#1_199) -> [T#1_1341]
  Op#1062 SLICE(T#1_1339, T#1_198, T#1_199) -> [T#1_1342]
  Op#1063 RESHAPE(T#1_1340, T#1_263) -> [T#1_1343]
  Op#1064 TRANSPOSE(T#1_1343, T#1_262) -> [T#1_1344]
  Op#1065 SLICE(T#1_1344, T#1_202, T#1_197) -> [T#1_1345]
  Op#1066 SLICE(T#1_1344, T#1_196, T#1_197) -> [T#1_1346]
  Op#1067 MUL(T#1_1345, T#1_286) -> [T#1_1347]
  Op#1068 MUL(T#1_1346, T#1_287) -> [T#1_1348]
  Op#1069 SUB(T#1_1347, T#1_1348) -> [T#1_1349]
  Op#1070 MUL(T#1_1346, T#1_286) -> [T#1_1350]
  Op#1071 MUL(T#1_1345, T#1_287) -> [T#1_1351]
  Op#1072 ADD(T#1_1350, T#1_1351) -> [T#1_1352]
  Op#1073 CONCATENATION(T#1_1349, T#1_1352) -> [T#1_1353]
  Op#1074 TRANSPOSE(T#1_1353, T#1_262) -> [T#1_1354]
  Op#1075 TRANSPOSE(T#1_1341, T#1_262) -> [T#1_1355]
  Op#1076 SLICE(T#1_1355, T#1_202, T#1_195) -> [T#1_1356]
  Op#1077 SLICE(T#1_1355, T#1_196, T#1_195) -> [T#1_1357]
  Op#1078 MUL(T#1_1356, T#1_286) -> [T#1_1358]
  Op#1079 MUL(T#1_1357, T#1_287) -> [T#1_1359]
  Op#1080 SUB(T#1_1358, T#1_1359) -> [T#1_1360]
  Op#1081 MUL(T#1_1357, T#1_286) -> [T#1_1361]
  Op#1082 MUL(T#1_1356, T#1_287) -> [T#1_1362]
  Op#1083 ADD(T#1_1361, T#1_1362) -> [T#1_1363]
  Op#1084 CONCATENATION(T#1_1360, T#1_1363) -> [T#1_1364]
  Op#1085 TRANSPOSE(T#1_1364, T#1_262) -> [T#1_1365]
  Op#1086 DYNAMIC_UPDATE_SLICE(T#1_25, T#1_1365, T#1_339) -> [T#1_1366]
  Op#1087 DYNAMIC_UPDATE_SLICE(T#1_21, T#1_1342, T#1_339) -> [T#1_1367]
  Op#1088 STABLEHLO_COMPOSITE(T#1_1354, T#1_1366, T#1_1367, T#1_300) -> [T#1_1368]
  Op#1089 RESHAPE(T#1_1368, T#1_271) -> [T#1_1369]
  Op#1090 FULLY_CONNECTED(T#1_1369, T#1_102, T#-1) -> [T#1_1370]
  Op#1091 ADD(T#1_1329, T#1_1370) -> [T#1_1371]
  Op#1092 MUL(T#1_1371, T#1_1371) -> [T#1_1372]
  Op#1093 SUM(T#1_1372, T#1_273) -> [T#1_1373]
  Op#1094 MUL(T#1_1373, T#1_203) -> [T#1_1374]
  Op#1095 ADD(T#1_1374, T#1_277) -> [T#1_1375]
  Op#1096 RESHAPE(T#1_1375, T#1_265) -> [T#1_1376]
  Op#1097 RSQRT(T#1_1376) -> [T#1_1377]
  Op#1098 MUL(T#1_1371, T#1_1377) -> [T#1_1378]
  Op#1099 MUL(T#1_1378, T#1_222) -> [T#1_1379]
  Op#1100 FULLY_CONNECTED(T#1_1379, T#1_101, T#-1) -> [T#1_1380]
  Op#1101 LOGISTIC(T#1_1380) -> [T#1_1381]
  Op#1102 MUL(T#1_1380, T#1_1381) -> [T#1_1382]
  Op#1103 FULLY_CONNECTED(T#1_1379, T#1_100, T#-1) -> [T#1_1383]
  Op#1104 MUL(T#1_1382, T#1_1383) -> [T#1_1384]
  Op#1105 FULLY_CONNECTED(T#1_1384, T#1_99, T#-1) -> [T#1_1385]
  Op#1106 ADD(T#1_1371, T#1_1385) -> [T#1_1386]
  Op#1107 MUL(T#1_1386, T#1_1386) -> [T#1_1387]
  Op#1108 SUM(T#1_1387, T#1_273) -> [T#1_1388]
  Op#1109 MUL(T#1_1388, T#1_203) -> [T#1_1389]
  Op#1110 ADD(T#1_1389, T#1_277) -> [T#1_1390]
  Op#1111 RESHAPE(T#1_1390, T#1_265) -> [T#1_1391]
  Op#1112 RSQRT(T#1_1391) -> [T#1_1392]
  Op#1113 MUL(T#1_1386, T#1_1392) -> [T#1_1393]
  Op#1114 MUL(T#1_1393, T#1_221) -> [T#1_1394]
  Op#1115 FULLY_CONNECTED(T#1_1394, T#1_98, T#-1) -> [T#1_1395]
  Op#1116 RESHAPE(T#1_1395, T#1_264) -> [T#1_1396]
  Op#1117 SLICE(T#1_1396, T#1_202, T#1_201) -> [T#1_1397]
  Op#1118 SLICE(T#1_1396, T#1_200, T#1_199) -> [T#1_1398]
  Op#1119 SLICE(T#1_1396, T#1_198, T#1_199) -> [T#1_1399]
  Op#1120 RESHAPE(T#1_1397, T#1_263) -> [T#1_1400]
  Op#1121 TRANSPOSE(T#1_1400, T#1_262) -> [T#1_1401]
  Op#1122 SLICE(T#1_1401, T#1_202, T#1_197) -> [T#1_1402]
  Op#1123 SLICE(T#1_1401, T#1_196, T#1_197) -> [T#1_1403]
  Op#1124 MUL(T#1_1402, T#1_286) -> [T#1_1404]
  Op#1125 MUL(T#1_1403, T#1_287) -> [T#1_1405]
  Op#1126 SUB(T#1_1404, T#1_1405) -> [T#1_1406]
  Op#1127 MUL(T#1_1403, T#1_286) -> [T#1_1407]
  Op#1128 MUL(T#1_1402, T#1_287) -> [T#1_1408]
  Op#1129 ADD(T#1_1407, T#1_1408) -> [T#1_1409]
  Op#1130 CONCATENATION(T#1_1406, T#1_1409) -> [T#1_1410]
  Op#1131 TRANSPOSE(T#1_1410, T#1_262) -> [T#1_1411]
  Op#1132 TRANSPOSE(T#1_1398, T#1_262) -> [T#1_1412]
  Op#1133 SLICE(T#1_1412, T#1_202, T#1_195) -> [T#1_1413]
  Op#1134 SLICE(T#1_1412, T#1_196, T#1_195) -> [T#1_1414]
  Op#1135 MUL(T#1_1413, T#1_286) -> [T#1_1415]
  Op#1136 MUL(T#1_1414, T#1_287) -> [T#1_1416]
  Op#1137 SUB(T#1_1415, T#1_1416) -> [T#1_1417]
  Op#1138 MUL(T#1_1414, T#1_286) -> [T#1_1418]
  Op#1139 MUL(T#1_1413, T#1_287) -> [T#1_1419]
  Op#1140 ADD(T#1_1418, T#1_1419) -> [T#1_1420]
  Op#1141 CONCATENATION(T#1_1417, T#1_1420) -> [T#1_1421]
  Op#1142 TRANSPOSE(T#1_1421, T#1_262) -> [T#1_1422]
  Op#1143 DYNAMIC_UPDATE_SLICE(T#1_53, T#1_1422, T#1_339) -> [T#1_1423]
  Op#1144 DYNAMIC_UPDATE_SLICE(T#1_27, T#1_1399, T#1_339) -> [T#1_1424]
  Op#1145 STABLEHLO_COMPOSITE(T#1_1411, T#1_1423, T#1_1424, T#1_300) -> [T#1_1425]
  Op#1146 RESHAPE(T#1_1425, T#1_271) -> [T#1_1426]
  Op#1147 FULLY_CONNECTED(T#1_1426, T#1_97, T#-1) -> [T#1_1427]
  Op#1148 ADD(T#1_1386, T#1_1427) -> [T#1_1428]
  Op#1149 MUL(T#1_1428, T#1_1428) -> [T#1_1429]
  Op#1150 SUM(T#1_1429, T#1_273) -> [T#1_1430]
  Op#1151 MUL(T#1_1430, T#1_203) -> [T#1_1431]
  Op#1152 ADD(T#1_1431, T#1_277) -> [T#1_1432]
  Op#1153 RESHAPE(T#1_1432, T#1_265) -> [T#1_1433]
  Op#1154 RSQRT(T#1_1433) -> [T#1_1434]
  Op#1155 MUL(T#1_1428, T#1_1434) -> [T#1_1435]
  Op#1156 MUL(T#1_1435, T#1_220) -> [T#1_1436]
  Op#1157 FULLY_CONNECTED(T#1_1436, T#1_96, T#-1) -> [T#1_1437]
  Op#1158 LOGISTIC(T#1_1437) -> [T#1_1438]
  Op#1159 MUL(T#1_1437, T#1_1438) -> [T#1_1439]
  Op#1160 FULLY_CONNECTED(T#1_1436, T#1_95, T#-1) -> [T#1_1440]
  Op#1161 MUL(T#1_1439, T#1_1440) -> [T#1_1441]
  Op#1162 FULLY_CONNECTED(T#1_1441, T#1_94, T#-1) -> [T#1_1442]
  Op#1163 ADD(T#1_1428, T#1_1442) -> [T#1_1443]
  Op#1164 MUL(T#1_1443, T#1_1443) -> [T#1_1444]
  Op#1165 SUM(T#1_1444, T#1_273) -> [T#1_1445]
  Op#1166 MUL(T#1_1445, T#1_203) -> [T#1_1446]
  Op#1167 ADD(T#1_1446, T#1_277) -> [T#1_1447]
  Op#1168 RESHAPE(T#1_1447, T#1_265) -> [T#1_1448]
  Op#1169 RSQRT(T#1_1448) -> [T#1_1449]
  Op#1170 MUL(T#1_1443, T#1_1449) -> [T#1_1450]
  Op#1171 MUL(T#1_1450, T#1_219) -> [T#1_1451]
  Op#1172 FULLY_CONNECTED(T#1_1451, T#1_93, T#-1) -> [T#1_1452]
  Op#1173 RESHAPE(T#1_1452, T#1_264) -> [T#1_1453]
  Op#1174 SLICE(T#1_1453, T#1_202, T#1_201) -> [T#1_1454]
  Op#1175 SLICE(T#1_1453, T#1_200, T#1_199) -> [T#1_1455]
  Op#1176 SLICE(T#1_1453, T#1_198, T#1_199) -> [T#1_1456]
  Op#1177 RESHAPE(T#1_1454, T#1_263) -> [T#1_1457]
  Op#1178 TRANSPOSE(T#1_1457, T#1_262) -> [T#1_1458]
  Op#1179 SLICE(T#1_1458, T#1_202, T#1_197) -> [T#1_1459]
  Op#1180 SLICE(T#1_1458, T#1_196, T#1_197) -> [T#1_1460]
  Op#1181 MUL(T#1_1459, T#1_286) -> [T#1_1461]
  Op#1182 MUL(T#1_1460, T#1_287) -> [T#1_1462]
  Op#1183 SUB(T#1_1461, T#1_1462) -> [T#1_1463]
  Op#1184 MUL(T#1_1460, T#1_286) -> [T#1_1464]
  Op#1185 MUL(T#1_1459, T#1_287) -> [T#1_1465]
  Op#1186 ADD(T#1_1464, T#1_1465) -> [T#1_1466]
  Op#1187 CONCATENATION(T#1_1463, T#1_1466) -> [T#1_1467]
  Op#1188 TRANSPOSE(T#1_1467, T#1_262) -> [T#1_1468]
  Op#1189 TRANSPOSE(T#1_1455, T#1_262) -> [T#1_1469]
  Op#1190 SLICE(T#1_1469, T#1_202, T#1_195) -> [T#1_1470]
  Op#1191 SLICE(T#1_1469, T#1_196, T#1_195) -> [T#1_1471]
  Op#1192 MUL(T#1_1470, T#1_286) -> [T#1_1472]
  Op#1193 MUL(T#1_1471, T#1_287) -> [T#1_1473]
  Op#1194 SUB(T#1_1472, T#1_1473) -> [T#1_1474]
  Op#1195 MUL(T#1_1471, T#1_286) -> [T#1_1475]
  Op#1196 MUL(T#1_1470, T#1_287) -> [T#1_1476]
  Op#1197 ADD(T#1_1475, T#1_1476) -> [T#1_1477]
  Op#1198 CONCATENATION(T#1_1474, T#1_1477) -> [T#1_1478]
  Op#1199 TRANSPOSE(T#1_1478, T#1_262) -> [T#1_1479]
  Op#1200 DYNAMIC_UPDATE_SLICE(T#1_15, T#1_1479, T#1_339) -> [T#1_1480]
  Op#1201 DYNAMIC_UPDATE_SLICE(T#1_55, T#1_1456, T#1_339) -> [T#1_1481]
  Op#1202 STABLEHLO_COMPOSITE(T#1_1468, T#1_1480, T#1_1481, T#1_300) -> [T#1_1482]
  Op#1203 RESHAPE(T#1_1482, T#1_271) -> [T#1_1483]
  Op#1204 FULLY_CONNECTED(T#1_1483, T#1_92, T#-1) -> [T#1_1484]
  Op#1205 ADD(T#1_1443, T#1_1484) -> [T#1_1485]
  Op#1206 MUL(T#1_1485, T#1_1485) -> [T#1_1486]
  Op#1207 SUM(T#1_1486, T#1_273) -> [T#1_1487]
  Op#1208 MUL(T#1_1487, T#1_203) -> [T#1_1488]
  Op#1209 ADD(T#1_1488, T#1_277) -> [T#1_1489]
  Op#1210 RESHAPE(T#1_1489, T#1_265) -> [T#1_1490]
  Op#1211 RSQRT(T#1_1490) -> [T#1_1491]
  Op#1212 MUL(T#1_1485, T#1_1491) -> [T#1_1492]
  Op#1213 MUL(T#1_1492, T#1_218) -> [T#1_1493]
  Op#1214 FULLY_CONNECTED(T#1_1493, T#1_91, T#-1) -> [T#1_1494]
  Op#1215 LOGISTIC(T#1_1494) -> [T#1_1495]
  Op#1216 MUL(T#1_1494, T#1_1495) -> [T#1_1496]
  Op#1217 FULLY_CONNECTED(T#1_1493, T#1_90, T#-1) -> [T#1_1497]
  Op#1218 MUL(T#1_1496, T#1_1497) -> [T#1_1498]
  Op#1219 FULLY_CONNECTED(T#1_1498, T#1_89, T#-1) -> [T#1_1499]
  Op#1220 ADD(T#1_1485, T#1_1499) -> [T#1_1500]
  Op#1221 MUL(T#1_1500, T#1_1500) -> [T#1_1501]
  Op#1222 SUM(T#1_1501, T#1_273) -> [T#1_1502]
  Op#1223 MUL(T#1_1502, T#1_203) -> [T#1_1503]
  Op#1224 ADD(T#1_1503, T#1_277) -> [T#1_1504]
  Op#1225 RESHAPE(T#1_1504, T#1_265) -> [T#1_1505]
  Op#1226 RSQRT(T#1_1505) -> [T#1_1506]
  Op#1227 MUL(T#1_1500, T#1_1506) -> [T#1_1507]
  Op#1228 MUL(T#1_1507, T#1_217) -> [T#1_1508]
  Op#1229 FULLY_CONNECTED(T#1_1508, T#1_88, T#-1) -> [T#1_1509]
  Op#1230 RESHAPE(T#1_1509, T#1_264) -> [T#1_1510]
  Op#1231 SLICE(T#1_1510, T#1_202, T#1_201) -> [T#1_1511]
  Op#1232 SLICE(T#1_1510, T#1_200, T#1_199) -> [T#1_1512]
  Op#1233 SLICE(T#1_1510, T#1_198, T#1_199) -> [T#1_1513]
  Op#1234 RESHAPE(T#1_1511, T#1_263) -> [T#1_1514]
  Op#1235 TRANSPOSE(T#1_1514, T#1_262) -> [T#1_1515]
  Op#1236 SLICE(T#1_1515, T#1_202, T#1_197) -> [T#1_1516]
  Op#1237 SLICE(T#1_1515, T#1_196, T#1_197) -> [T#1_1517]
  Op#1238 MUL(T#1_1516, T#1_286) -> [T#1_1518]
  Op#1239 MUL(T#1_1517, T#1_287) -> [T#1_1519]
  Op#1240 SUB(T#1_1518, T#1_1519) -> [T#1_1520]
  Op#1241 MUL(T#1_1517, T#1_286) -> [T#1_1521]
  Op#1242 MUL(T#1_1516, T#1_287) -> [T#1_1522]
  Op#1243 ADD(T#1_1521, T#1_1522) -> [T#1_1523]
  Op#1244 CONCATENATION(T#1_1520, T#1_1523) -> [T#1_1524]
  Op#1245 TRANSPOSE(T#1_1524, T#1_262) -> [T#1_1525]
  Op#1246 TRANSPOSE(T#1_1512, T#1_262) -> [T#1_1526]
  Op#1247 SLICE(T#1_1526, T#1_202, T#1_195) -> [T#1_1527]
  Op#1248 SLICE(T#1_1526, T#1_196, T#1_195) -> [T#1_1528]
  Op#1249 MUL(T#1_1527, T#1_286) -> [T#1_1529]
  Op#1250 MUL(T#1_1528, T#1_287) -> [T#1_1530]
  Op#1251 SUB(T#1_1529, T#1_1530) -> [T#1_1531]
  Op#1252 MUL(T#1_1528, T#1_286) -> [T#1_1532]
  Op#1253 MUL(T#1_1527, T#1_287) -> [T#1_1533]
  Op#1254 ADD(T#1_1532, T#1_1533) -> [T#1_1534]
  Op#1255 CONCATENATION(T#1_1531, T#1_1534) -> [T#1_1535]
  Op#1256 TRANSPOSE(T#1_1535, T#1_262) -> [T#1_1536]
  Op#1257 DYNAMIC_UPDATE_SLICE(T#1_43, T#1_1536, T#1_339) -> [T#1_1537]
  Op#1258 DYNAMIC_UPDATE_SLICE(T#1_37, T#1_1513, T#1_339) -> [T#1_1538]
  Op#1259 STABLEHLO_COMPOSITE(T#1_1525, T#1_1537, T#1_1538, T#1_300) -> [T#1_1539]
  Op#1260 RESHAPE(T#1_1539, T#1_271) -> [T#1_1540]
  Op#1261 FULLY_CONNECTED(T#1_1540, T#1_87, T#-1) -> [T#1_1541]
  Op#1262 ADD(T#1_1500, T#1_1541) -> [T#1_1542]
  Op#1263 MUL(T#1_1542, T#1_1542) -> [T#1_1543]
  Op#1264 SUM(T#1_1543, T#1_273) -> [T#1_1544]
  Op#1265 MUL(T#1_1544, T#1_203) -> [T#1_1545]
  Op#1266 ADD(T#1_1545, T#1_277) -> [T#1_1546]
  Op#1267 RESHAPE(T#1_1546, T#1_265) -> [T#1_1547]
  Op#1268 RSQRT(T#1_1547) -> [T#1_1548]
  Op#1269 MUL(T#1_1542, T#1_1548) -> [T#1_1549]
  Op#1270 MUL(T#1_1549, T#1_216) -> [T#1_1550]
  Op#1271 FULLY_CONNECTED(T#1_1550, T#1_86, T#-1) -> [T#1_1551]
  Op#1272 LOGISTIC(T#1_1551) -> [T#1_1552]
  Op#1273 MUL(T#1_1551, T#1_1552) -> [T#1_1553]
  Op#1274 FULLY_CONNECTED(T#1_1550, T#1_85, T#-1) -> [T#1_1554]
  Op#1275 MUL(T#1_1553, T#1_1554) -> [T#1_1555]
  Op#1276 FULLY_CONNECTED(T#1_1555, T#1_84, T#-1) -> [T#1_1556]
  Op#1277 ADD(T#1_1542, T#1_1556) -> [T#1_1557]
  Op#1278 MUL(T#1_1557, T#1_1557) -> [T#1_1558]
  Op#1279 SUM(T#1_1558, T#1_273) -> [T#1_1559]
  Op#1280 MUL(T#1_1559, T#1_203) -> [T#1_1560]
  Op#1281 ADD(T#1_1560, T#1_277) -> [T#1_1561]
  Op#1282 RESHAPE(T#1_1561, T#1_265) -> [T#1_1562]
  Op#1283 RSQRT(T#1_1562) -> [T#1_1563]
  Op#1284 MUL(T#1_1557, T#1_1563) -> [T#1_1564]
  Op#1285 MUL(T#1_1564, T#1_215) -> [T#1_1565]
  Op#1286 FULLY_CONNECTED(T#1_1565, T#1_83, T#-1) -> [T#1_1566]
  Op#1287 RESHAPE(T#1_1566, T#1_264) -> [T#1_1567]
  Op#1288 SLICE(T#1_1567, T#1_202, T#1_201) -> [T#1_1568]
  Op#1289 SLICE(T#1_1567, T#1_200, T#1_199) -> [T#1_1569]
  Op#1290 SLICE(T#1_1567, T#1_198, T#1_199) -> [T#1_1570]
  Op#1291 RESHAPE(T#1_1568, T#1_263) -> [T#1_1571]
  Op#1292 TRANSPOSE(T#1_1571, T#1_262) -> [T#1_1572]
  Op#1293 SLICE(T#1_1572, T#1_202, T#1_197) -> [T#1_1573]
  Op#1294 SLICE(T#1_1572, T#1_196, T#1_197) -> [T#1_1574]
  Op#1295 MUL(T#1_1573, T#1_286) -> [T#1_1575]
  Op#1296 MUL(T#1_1574, T#1_287) -> [T#1_1576]
  Op#1297 SUB(T#1_1575, T#1_1576) -> [T#1_1577]
  Op#1298 MUL(T#1_1574, T#1_286) -> [T#1_1578]
  Op#1299 MUL(T#1_1573, T#1_287) -> [T#1_1579]
  Op#1300 ADD(T#1_1578, T#1_1579) -> [T#1_1580]
  Op#1301 CONCATENATION(T#1_1577, T#1_1580) -> [T#1_1581]
  Op#1302 TRANSPOSE(T#1_1581, T#1_262) -> [T#1_1582]
  Op#1303 TRANSPOSE(T#1_1569, T#1_262) -> [T#1_1583]
  Op#1304 SLICE(T#1_1583, T#1_202, T#1_195) -> [T#1_1584]
  Op#1305 SLICE(T#1_1583, T#1_196, T#1_195) -> [T#1_1585]
  Op#1306 MUL(T#1_1584, T#1_286) -> [T#1_1586]
  Op#1307 MUL(T#1_1585, T#1_287) -> [T#1_1587]
  Op#1308 SUB(T#1_1586, T#1_1587) -> [T#1_1588]
  Op#1309 MUL(T#1_1585, T#1_286) -> [T#1_1589]
  Op#1310 MUL(T#1_1584, T#1_287) -> [T#1_1590]
  Op#1311 ADD(T#1_1589, T#1_1590) -> [T#1_1591]
  Op#1312 CONCATENATION(T#1_1588, T#1_1591) -> [T#1_1592]
  Op#1313 TRANSPOSE(T#1_1592, T#1_262) -> [T#1_1593]
  Op#1314 DYNAMIC_UPDATE_SLICE(T#1_13, T#1_1593, T#1_339) -> [T#1_1594]
  Op#1315 DYNAMIC_UPDATE_SLICE(T#1_7, T#1_1570, T#1_339) -> [T#1_1595]
  Op#1316 STABLEHLO_COMPOSITE(T#1_1582, T#1_1594, T#1_1595, T#1_300) -> [T#1_1596]
  Op#1317 RESHAPE(T#1_1596, T#1_271) -> [T#1_1597]
  Op#1318 FULLY_CONNECTED(T#1_1597, T#1_82, T#-1) -> [T#1_1598]
  Op#1319 ADD(T#1_1557, T#1_1598) -> [T#1_1599]
  Op#1320 MUL(T#1_1599, T#1_1599) -> [T#1_1600]
  Op#1321 SUM(T#1_1600, T#1_273) -> [T#1_1601]
  Op#1322 MUL(T#1_1601, T#1_203) -> [T#1_1602]
  Op#1323 ADD(T#1_1602, T#1_277) -> [T#1_1603]
  Op#1324 RESHAPE(T#1_1603, T#1_265) -> [T#1_1604]
  Op#1325 RSQRT(T#1_1604) -> [T#1_1605]
  Op#1326 MUL(T#1_1599, T#1_1605) -> [T#1_1606]
  Op#1327 MUL(T#1_1606, T#1_214) -> [T#1_1607]
  Op#1328 FULLY_CONNECTED(T#1_1607, T#1_81, T#-1) -> [T#1_1608]
  Op#1329 LOGISTIC(T#1_1608) -> [T#1_1609]
  Op#1330 MUL(T#1_1608, T#1_1609) -> [T#1_1610]
  Op#1331 FULLY_CONNECTED(T#1_1607, T#1_80, T#-1) -> [T#1_1611]
  Op#1332 MUL(T#1_1610, T#1_1611) -> [T#1_1612]
  Op#1333 FULLY_CONNECTED(T#1_1612, T#1_79, T#-1) -> [T#1_1613]
  Op#1334 ADD(T#1_1599, T#1_1613) -> [T#1_1614]
  Op#1335 MUL(T#1_1614, T#1_1614) -> [T#1_1615]
  Op#1336 SUM(T#1_1615, T#1_273) -> [T#1_1616]
  Op#1337 MUL(T#1_1616, T#1_203) -> [T#1_1617]
  Op#1338 ADD(T#1_1617, T#1_277) -> [T#1_1618]
  Op#1339 RESHAPE(T#1_1618, T#1_265) -> [T#1_1619]
  Op#1340 RSQRT(T#1_1619) -> [T#1_1620]
  Op#1341 MUL(T#1_1614, T#1_1620) -> [T#1_1621]
  Op#1342 MUL(T#1_1621, T#1_213) -> [T#1_1622]
  Op#1343 FULLY_CONNECTED(T#1_1622, T#1_78, T#-1) -> [T#1_1623]
  Op#1344 RESHAPE(T#1_1623, T#1_264) -> [T#1_1624]
  Op#1345 SLICE(T#1_1624, T#1_202, T#1_201) -> [T#1_1625]
  Op#1346 SLICE(T#1_1624, T#1_200, T#1_199) -> [T#1_1626]
  Op#1347 SLICE(T#1_1624, T#1_198, T#1_199) -> [T#1_1627]
  Op#1348 RESHAPE(T#1_1625, T#1_263) -> [T#1_1628]
  Op#1349 TRANSPOSE(T#1_1628, T#1_262) -> [T#1_1629]
  Op#1350 SLICE(T#1_1629, T#1_202, T#1_197) -> [T#1_1630]
  Op#1351 SLICE(T#1_1629, T#1_196, T#1_197) -> [T#1_1631]
  Op#1352 MUL(T#1_1630, T#1_286) -> [T#1_1632]
  Op#1353 MUL(T#1_1631, T#1_287) -> [T#1_1633]
  Op#1354 SUB(T#1_1632, T#1_1633) -> [T#1_1634]
  Op#1355 MUL(T#1_1631, T#1_286) -> [T#1_1635]
  Op#1356 MUL(T#1_1630, T#1_287) -> [T#1_1636]
  Op#1357 ADD(T#1_1635, T#1_1636) -> [T#1_1637]
  Op#1358 CONCATENATION(T#1_1634, T#1_1637) -> [T#1_1638]
  Op#1359 TRANSPOSE(T#1_1638, T#1_262) -> [T#1_1639]
  Op#1360 TRANSPOSE(T#1_1626, T#1_262) -> [T#1_1640]
  Op#1361 SLICE(T#1_1640, T#1_202, T#1_195) -> [T#1_1641]
  Op#1362 SLICE(T#1_1640, T#1_196, T#1_195) -> [T#1_1642]
  Op#1363 MUL(T#1_1641, T#1_286) -> [T#1_1643]
  Op#1364 MUL(T#1_1642, T#1_287) -> [T#1_1644]
  Op#1365 SUB(T#1_1643, T#1_1644) -> [T#1_1645]
  Op#1366 MUL(T#1_1642, T#1_286) -> [T#1_1646]
  Op#1367 MUL(T#1_1641, T#1_287) -> [T#1_1647]
  Op#1368 ADD(T#1_1646, T#1_1647) -> [T#1_1648]
  Op#1369 CONCATENATION(T#1_1645, T#1_1648) -> [T#1_1649]
  Op#1370 TRANSPOSE(T#1_1649, T#1_262) -> [T#1_1650]
  Op#1371 DYNAMIC_UPDATE_SLICE(T#1_32, T#1_1650, T#1_339) -> [T#1_1651]
  Op#1372 DYNAMIC_UPDATE_SLICE(T#1_17, T#1_1627, T#1_339) -> [T#1_1652]
  Op#1373 STABLEHLO_COMPOSITE(T#1_1639, T#1_1651, T#1_1652, T#1_300) -> [T#1_1653]
  Op#1374 RESHAPE(T#1_1653, T#1_271) -> [T#1_1654]
  Op#1375 FULLY_CONNECTED(T#1_1654, T#1_77, T#-1) -> [T#1_1655]
  Op#1376 ADD(T#1_1614, T#1_1655) -> [T#1_1656]
  Op#1377 MUL(T#1_1656, T#1_1656) -> [T#1_1657]
  Op#1378 SUM(T#1_1657, T#1_273) -> [T#1_1658]
  Op#1379 MUL(T#1_1658, T#1_203) -> [T#1_1659]
  Op#1380 ADD(T#1_1659, T#1_277) -> [T#1_1660]
  Op#1381 RESHAPE(T#1_1660, T#1_265) -> [T#1_1661]
  Op#1382 RSQRT(T#1_1661) -> [T#1_1662]
  Op#1383 MUL(T#1_1656, T#1_1662) -> [T#1_1663]
  Op#1384 MUL(T#1_1663, T#1_212) -> [T#1_1664]
  Op#1385 FULLY_CONNECTED(T#1_1664, T#1_76, T#-1) -> [T#1_1665]
  Op#1386 LOGISTIC(T#1_1665) -> [T#1_1666]
  Op#1387 MUL(T#1_1665, T#1_1666) -> [T#1_1667]
  Op#1388 FULLY_CONNECTED(T#1_1664, T#1_75, T#-1) -> [T#1_1668]
  Op#1389 MUL(T#1_1667, T#1_1668) -> [T#1_1669]
  Op#1390 FULLY_CONNECTED(T#1_1669, T#1_74, T#-1) -> [T#1_1670]
  Op#1391 ADD(T#1_1656, T#1_1670) -> [T#1_1671]
  Op#1392 MUL(T#1_1671, T#1_1671) -> [T#1_1672]
  Op#1393 SUM(T#1_1672, T#1_273) -> [T#1_1673]
  Op#1394 MUL(T#1_1673, T#1_203) -> [T#1_1674]
  Op#1395 ADD(T#1_1674, T#1_277) -> [T#1_1675]
  Op#1396 RESHAPE(T#1_1675, T#1_265) -> [T#1_1676]
  Op#1397 RSQRT(T#1_1676) -> [T#1_1677]
  Op#1398 MUL(T#1_1671, T#1_1677) -> [T#1_1678]
  Op#1399 MUL(T#1_1678, T#1_211) -> [T#1_1679]
  Op#1400 FULLY_CONNECTED(T#1_1679, T#1_73, T#-1) -> [T#1_1680]
  Op#1401 RESHAPE(T#1_1680, T#1_264) -> [T#1_1681]
  Op#1402 SLICE(T#1_1681, T#1_202, T#1_201) -> [T#1_1682]
  Op#1403 SLICE(T#1_1681, T#1_200, T#1_199) -> [T#1_1683]
  Op#1404 SLICE(T#1_1681, T#1_198, T#1_199) -> [T#1_1684]
  Op#1405 RESHAPE(T#1_1682, T#1_263) -> [T#1_1685]
  Op#1406 TRANSPOSE(T#1_1685, T#1_262) -> [T#1_1686]
  Op#1407 SLICE(T#1_1686, T#1_202, T#1_197) -> [T#1_1687]
  Op#1408 SLICE(T#1_1686, T#1_196, T#1_197) -> [T#1_1688]
  Op#1409 MUL(T#1_1687, T#1_286) -> [T#1_1689]
  Op#1410 MUL(T#1_1688, T#1_287) -> [T#1_1690]
  Op#1411 SUB(T#1_1689, T#1_1690) -> [T#1_1691]
  Op#1412 MUL(T#1_1688, T#1_286) -> [T#1_1692]
  Op#1413 MUL(T#1_1687, T#1_287) -> [T#1_1693]
  Op#1414 ADD(T#1_1692, T#1_1693) -> [T#1_1694]
  Op#1415 CONCATENATION(T#1_1691, T#1_1694) -> [T#1_1695]
  Op#1416 TRANSPOSE(T#1_1695, T#1_262) -> [T#1_1696]
  Op#1417 TRANSPOSE(T#1_1683, T#1_262) -> [T#1_1697]
  Op#1418 SLICE(T#1_1697, T#1_202, T#1_195) -> [T#1_1698]
  Op#1419 SLICE(T#1_1697, T#1_196, T#1_195) -> [T#1_1699]
  Op#1420 MUL(T#1_1698, T#1_286) -> [T#1_1700]
  Op#1421 MUL(T#1_1699, T#1_287) -> [T#1_1701]
  Op#1422 SUB(T#1_1700, T#1_1701) -> [T#1_1702]
  Op#1423 MUL(T#1_1699, T#1_286) -> [T#1_1703]
  Op#1424 MUL(T#1_1698, T#1_287) -> [T#1_1704]
  Op#1425 ADD(T#1_1703, T#1_1704) -> [T#1_1705]
  Op#1426 CONCATENATION(T#1_1702, T#1_1705) -> [T#1_1706]
  Op#1427 TRANSPOSE(T#1_1706, T#1_262) -> [T#1_1707]
  Op#1428 DYNAMIC_UPDATE_SLICE(T#1_9, T#1_1707, T#1_339) -> [T#1_1708]
  Op#1429 DYNAMIC_UPDATE_SLICE(T#1_44, T#1_1684, T#1_339) -> [T#1_1709]
  Op#1430 STABLEHLO_COMPOSITE(T#1_1696, T#1_1708, T#1_1709, T#1_300) -> [T#1_1710]
  Op#1431 RESHAPE(T#1_1710, T#1_271) -> [T#1_1711]
  Op#1432 FULLY_CONNECTED(T#1_1711, T#1_72, T#-1) -> [T#1_1712]
  Op#1433 ADD(T#1_1671, T#1_1712) -> [T#1_1713]
  Op#1434 MUL(T#1_1713, T#1_1713) -> [T#1_1714]
  Op#1435 SUM(T#1_1714, T#1_273) -> [T#1_1715]
  Op#1436 MUL(T#1_1715, T#1_203) -> [T#1_1716]
  Op#1437 ADD(T#1_1716, T#1_277) -> [T#1_1717]
  Op#1438 RESHAPE(T#1_1717, T#1_265) -> [T#1_1718]
  Op#1439 RSQRT(T#1_1718) -> [T#1_1719]
  Op#1440 MUL(T#1_1713, T#1_1719) -> [T#1_1720]
  Op#1441 MUL(T#1_1720, T#1_210) -> [T#1_1721]
  Op#1442 FULLY_CONNECTED(T#1_1721, T#1_71, T#-1) -> [T#1_1722]
  Op#1443 LOGISTIC(T#1_1722) -> [T#1_1723]
  Op#1444 MUL(T#1_1722, T#1_1723) -> [T#1_1724]
  Op#1445 FULLY_CONNECTED(T#1_1721, T#1_70, T#-1) -> [T#1_1725]
  Op#1446 MUL(T#1_1724, T#1_1725) -> [T#1_1726]
  Op#1447 FULLY_CONNECTED(T#1_1726, T#1_69, T#-1) -> [T#1_1727]
  Op#1448 ADD(T#1_1713, T#1_1727) -> [T#1_1728]
  Op#1449 MUL(T#1_1728, T#1_1728) -> [T#1_1729]
  Op#1450 SUM(T#1_1729, T#1_273) -> [T#1_1730]
  Op#1451 MUL(T#1_1730, T#1_203) -> [T#1_1731]
  Op#1452 ADD(T#1_1731, T#1_277) -> [T#1_1732]
  Op#1453 RESHAPE(T#1_1732, T#1_265) -> [T#1_1733]
  Op#1454 RSQRT(T#1_1733) -> [T#1_1734]
  Op#1455 MUL(T#1_1728, T#1_1734) -> [T#1_1735]
  Op#1456 MUL(T#1_1735, T#1_209) -> [T#1_1736]
  Op#1457 FULLY_CONNECTED(T#1_1736, T#1_68, T#-1) -> [T#1_1737]
  Op#1458 RESHAPE(T#1_1737, T#1_264) -> [T#1_1738]
  Op#1459 SLICE(T#1_1738, T#1_202, T#1_201) -> [T#1_1739]
  Op#1460 SLICE(T#1_1738, T#1_200, T#1_199) -> [T#1_1740]
  Op#1461 SLICE(T#1_1738, T#1_198, T#1_199) -> [T#1_1741]
  Op#1462 RESHAPE(T#1_1739, T#1_263) -> [T#1_1742]
  Op#1463 TRANSPOSE(T#1_1742, T#1_262) -> [T#1_1743]
  Op#1464 SLICE(T#1_1743, T#1_202, T#1_197) -> [T#1_1744]
  Op#1465 SLICE(T#1_1743, T#1_196, T#1_197) -> [T#1_1745]
  Op#1466 MUL(T#1_1744, T#1_286) -> [T#1_1746]
  Op#1467 MUL(T#1_1745, T#1_287) -> [T#1_1747]
  Op#1468 SUB(T#1_1746, T#1_1747) -> [T#1_1748]
  Op#1469 MUL(T#1_1745, T#1_286) -> [T#1_1749]
  Op#1470 MUL(T#1_1744, T#1_287) -> [T#1_1750]
  Op#1471 ADD(T#1_1749, T#1_1750) -> [T#1_1751]
  Op#1472 CONCATENATION(T#1_1748, T#1_1751) -> [T#1_1752]
  Op#1473 TRANSPOSE(T#1_1752, T#1_262) -> [T#1_1753]
  Op#1474 TRANSPOSE(T#1_1740, T#1_262) -> [T#1_1754]
  Op#1475 SLICE(T#1_1754, T#1_202, T#1_195) -> [T#1_1755]
  Op#1476 SLICE(T#1_1754, T#1_196, T#1_195) -> [T#1_1756]
  Op#1477 MUL(T#1_1755, T#1_286) -> [T#1_1757]
  Op#1478 MUL(T#1_1756, T#1_287) -> [T#1_1758]
  Op#1479 SUB(T#1_1757, T#1_1758) -> [T#1_1759]
  Op#1480 MUL(T#1_1756, T#1_286) -> [T#1_1760]
  Op#1481 MUL(T#1_1755, T#1_287) -> [T#1_1761]
  Op#1482 ADD(T#1_1760, T#1_1761) -> [T#1_1762]
  Op#1483 CONCATENATION(T#1_1759, T#1_1762) -> [T#1_1763]
  Op#1484 TRANSPOSE(T#1_1763, T#1_262) -> [T#1_1764]
  Op#1485 DYNAMIC_UPDATE_SLICE(T#1_50, T#1_1764, T#1_339) -> [T#1_1765]
  Op#1486 DYNAMIC_UPDATE_SLICE(T#1_6, T#1_1741, T#1_339) -> [T#1_1766]
  Op#1487 STABLEHLO_COMPOSITE(T#1_1753, T#1_1765, T#1_1766, T#1_300) -> [T#1_1767]
  Op#1488 RESHAPE(T#1_1767, T#1_271) -> [T#1_1768]
  Op#1489 FULLY_CONNECTED(T#1_1768, T#1_67, T#-1) -> [T#1_1769]
  Op#1490 ADD(T#1_1728, T#1_1769) -> [T#1_1770]
  Op#1491 MUL(T#1_1770, T#1_1770) -> [T#1_1771]
  Op#1492 SUM(T#1_1771, T#1_273) -> [T#1_1772]
  Op#1493 MUL(T#1_1772, T#1_203) -> [T#1_1773]
  Op#1494 ADD(T#1_1773, T#1_277) -> [T#1_1774]
  Op#1495 RESHAPE(T#1_1774, T#1_265) -> [T#1_1775]
  Op#1496 RSQRT(T#1_1775) -> [T#1_1776]
  Op#1497 MUL(T#1_1770, T#1_1776) -> [T#1_1777]
  Op#1498 MUL(T#1_1777, T#1_208) -> [T#1_1778]
  Op#1499 FULLY_CONNECTED(T#1_1778, T#1_66, T#-1) -> [T#1_1779]
  Op#1500 LOGISTIC(T#1_1779) -> [T#1_1780]
  Op#1501 MUL(T#1_1779, T#1_1780) -> [T#1_1781]
  Op#1502 FULLY_CONNECTED(T#1_1778, T#1_65, T#-1) -> [T#1_1782]
  Op#1503 MUL(T#1_1781, T#1_1782) -> [T#1_1783]
  Op#1504 FULLY_CONNECTED(T#1_1783, T#1_64, T#-1) -> [T#1_1784]
  Op#1505 ADD(T#1_1770, T#1_1784) -> [T#1_1785]
  Op#1506 MUL(T#1_1785, T#1_1785) -> [T#1_1786]
  Op#1507 SUM(T#1_1786, T#1_273) -> [T#1_1787]
  Op#1508 MUL(T#1_1787, T#1_203) -> [T#1_1788]
  Op#1509 ADD(T#1_1788, T#1_277) -> [T#1_1789]
  Op#1510 RESHAPE(T#1_1789, T#1_265) -> [T#1_1790]
  Op#1511 RSQRT(T#1_1790) -> [T#1_1791]
  Op#1512 MUL(T#1_1785, T#1_1791) -> [T#1_1792]
  Op#1513 MUL(T#1_1792, T#1_207) -> [T#1_1793]
  Op#1514 FULLY_CONNECTED(T#1_1793, T#1_63, T#-1) -> [T#1_1794]
  Op#1515 RESHAPE(T#1_1794, T#1_264) -> [T#1_1795]
  Op#1516 SLICE(T#1_1795, T#1_202, T#1_201) -> [T#1_1796]
  Op#1517 SLICE(T#1_1795, T#1_200, T#1_199) -> [T#1_1797]
  Op#1518 SLICE(T#1_1795, T#1_198, T#1_199) -> [T#1_1798]
  Op#1519 RESHAPE(T#1_1796, T#1_263) -> [T#1_1799]
  Op#1520 TRANSPOSE(T#1_1799, T#1_262) -> [T#1_1800]
  Op#1521 SLICE(T#1_1800, T#1_202, T#1_197) -> [T#1_1801]
  Op#1522 SLICE(T#1_1800, T#1_196, T#1_197) -> [T#1_1802]
  Op#1523 MUL(T#1_1801, T#1_286) -> [T#1_1803]
  Op#1524 MUL(T#1_1802, T#1_287) -> [T#1_1804]
  Op#1525 SUB(T#1_1803, T#1_1804) -> [T#1_1805]
  Op#1526 MUL(T#1_1802, T#1_286) -> [T#1_1806]
  Op#1527 MUL(T#1_1801, T#1_287) -> [T#1_1807]
  Op#1528 ADD(T#1_1806, T#1_1807) -> [T#1_1808]
  Op#1529 CONCATENATION(T#1_1805, T#1_1808) -> [T#1_1809]
  Op#1530 TRANSPOSE(T#1_1809, T#1_262) -> [T#1_1810]
  Op#1531 TRANSPOSE(T#1_1797, T#1_262) -> [T#1_1811]
  Op#1532 SLICE(T#1_1811, T#1_202, T#1_195) -> [T#1_1812]
  Op#1533 SLICE(T#1_1811, T#1_196, T#1_195) -> [T#1_1813]
  Op#1534 MUL(T#1_1812, T#1_286) -> [T#1_1814]
  Op#1535 MUL(T#1_1813, T#1_287) -> [T#1_1815]
  Op#1536 SUB(T#1_1814, T#1_1815) -> [T#1_1816]
  Op#1537 MUL(T#1_1813, T#1_286) -> [T#1_1817]
  Op#1538 MUL(T#1_1812, T#1_287) -> [T#1_1818]
  Op#1539 ADD(T#1_1817, T#1_1818) -> [T#1_1819]
  Op#1540 CONCATENATION(T#1_1816, T#1_1819) -> [T#1_1820]
  Op#1541 TRANSPOSE(T#1_1820, T#1_262) -> [T#1_1821]
  Op#1542 DYNAMIC_UPDATE_SLICE(T#1_26, T#1_1821, T#1_339) -> [T#1_1822]
  Op#1543 DYNAMIC_UPDATE_SLICE(T#1_0, T#1_1798, T#1_339) -> [T#1_1823]
  Op#1544 STABLEHLO_COMPOSITE(T#1_1810, T#1_1822, T#1_1823, T#1_300) -> [T#1_1824]
  Op#1545 RESHAPE(T#1_1824, T#1_271) -> [T#1_1825]
  Op#1546 FULLY_CONNECTED(T#1_1825, T#1_62, T#-1) -> [T#1_1826]
  Op#1547 ADD(T#1_1785, T#1_1826) -> [T#1_1827]
  Op#1548 MUL(T#1_1827, T#1_1827) -> [T#1_1828]
  Op#1549 SUM(T#1_1828, T#1_273) -> [T#1_1829]
  Op#1550 MUL(T#1_1829, T#1_203) -> [T#1_1830]
  Op#1551 ADD(T#1_1830, T#1_277) -> [T#1_1831]
  Op#1552 RESHAPE(T#1_1831, T#1_265) -> [T#1_1832]
  Op#1553 RSQRT(T#1_1832) -> [T#1_1833]
  Op#1554 MUL(T#1_1827, T#1_1833) -> [T#1_1834]
  Op#1555 MUL(T#1_1834, T#1_206) -> [T#1_1835]
  Op#1556 FULLY_CONNECTED(T#1_1835, T#1_61, T#-1) -> [T#1_1836]
  Op#1557 LOGISTIC(T#1_1836) -> [T#1_1837]
  Op#1558 MUL(T#1_1836, T#1_1837) -> [T#1_1838]
  Op#1559 FULLY_CONNECTED(T#1_1835, T#1_60, T#-1) -> [T#1_1839]
  Op#1560 MUL(T#1_1838, T#1_1839) -> [T#1_1840]
  Op#1561 FULLY_CONNECTED(T#1_1840, T#1_59, T#-1) -> [T#1_1841]
  Op#1562 ADD(T#1_1827, T#1_1841) -> [T#1_1842]
  Op#1563 MUL(T#1_1842, T#1_1842) -> [T#1_1843]
  Op#1564 SUM(T#1_1843, T#1_273) -> [T#1_1844]
  Op#1565 MUL(T#1_1844, T#1_203) -> [T#1_1845]
  Op#1566 ADD(T#1_1845, T#1_277) -> [T#1_1846]
  Op#1567 RESHAPE(T#1_1846, T#1_265) -> [T#1_1847]
  Op#1568 RSQRT(T#1_1847) -> [T#1_1848]
  Op#1569 MUL(T#1_1842, T#1_1848) -> [T#1_1849]
  Op#1570 MUL(T#1_1849, T#1_205) -> [T#1_1850]
  Op#1571 FULLY_CONNECTED(T#1_1850, T#1_58, T#-1) -> [T#1_1851]
  Op#1572 RESHAPE(T#1_1851, T#1_264) -> [T#1_1852]
  Op#1573 SLICE(T#1_1852, T#1_200, T#1_199) -> [T#1_1853]
  Op#1574 SLICE(T#1_1852, T#1_198, T#1_199) -> [T#1_1854]
  Op#1575 TRANSPOSE(T#1_1853, T#1_262) -> [T#1_1855]
  Op#1576 SLICE(T#1_1855, T#1_202, T#1_195) -> [T#1_1856]
  Op#1577 SLICE(T#1_1855, T#1_196, T#1_195) -> [T#1_1857]
  Op#1578 MUL(T#1_1856, T#1_286) -> [T#1_1858]
  Op#1579 MUL(T#1_1857, T#1_287) -> [T#1_1859]
  Op#1580 SUB(T#1_1858, T#1_1859) -> [T#1_1860]
  Op#1581 MUL(T#1_1857, T#1_286) -> [T#1_1861]
  Op#1582 MUL(T#1_1856, T#1_287) -> [T#1_1862]
  Op#1583 ADD(T#1_1861, T#1_1862) -> [T#1_1863]
  Op#1584 CONCATENATION(T#1_1860, T#1_1863) -> [T#1_1864]
  Op#1585 TRANSPOSE(T#1_1864, T#1_262) -> [T#1_1865]
  Op#1586 DYNAMIC_UPDATE_SLICE(T#1_56, T#1_1865, T#1_339) -> [T#1_1866]
  Op#1587 DYNAMIC_UPDATE_SLICE(T#1_12, T#1_1854, T#1_339) -> [T#1_1867]

Tensors of Subgraph#1
  T#1_0(prefill_1024_kv_cache_v_26:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1(prefill_1024_kv_cache_v_3:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_2(prefill_1024_kv_cache_k_2:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_3(prefill_1024_kv_cache_k_16:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_4(prefill_1024_kv_cache_k_8:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_5(prefill_1024_input_pos:0) shape:[1024], type:INT32
  T#1_6(prefill_1024_kv_cache_v_25:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_7(prefill_1024_kv_cache_v_22:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_8(prefill_1024_kv_cache_k_10:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_9(prefill_1024_kv_cache_k_24:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_10(prefill_1024_kv_cache_k_17:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_11(prefill_1024_kv_cache_v_6:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_12(prefill_1024_kv_cache_v_27:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_13(prefill_1024_kv_cache_k_22:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_14(prefill_1024_kv_cache_k_4:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_15(prefill_1024_kv_cache_k_20:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_16(prefill_1024_kv_cache_k_9:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_17(prefill_1024_kv_cache_v_23:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_18(prefill_1024_kv_cache_k_3:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_19(prefill_1024_kv_cache_v_0:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_20(prefill_1024_kv_cache_k_12:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_21(prefill_1024_kv_cache_v_18:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_22(prefill_1024_kv_cache_v_11:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_23(prefill_1024_kv_cache_v_5:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_24(prefill_1024_kv_cache_k_6:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_25(prefill_1024_kv_cache_k_18:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_26(prefill_1024_kv_cache_k_26:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_27(prefill_1024_kv_cache_v_19:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_28(prefill_1024_kv_cache_k_7:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_29(prefill_1024_kv_cache_v_10:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_30(prefill_1024_kv_cache_v_16:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_31(prefill_1024_kv_cache_v_12:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_32(prefill_1024_kv_cache_k_23:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_33(prefill_1024_kv_cache_k_14:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_34(prefill_1024_kv_cache_v_15:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_35(prefill_1024_kv_cache_k_13:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_36(prefill_1024_kv_cache_v_7:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_37(prefill_1024_kv_cache_v_21:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_38(prefill_1024_kv_cache_v_1:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_39(prefill_1024_tokens:0) shape:[1, 1024], type:INT32
  T#1_40(prefill_1024_kv_cache_v_17:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_41(prefill_1024_kv_cache_v_13:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_42(prefill_1024_kv_cache_k_0:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_43(prefill_1024_kv_cache_k_21:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_44(prefill_1024_kv_cache_v_24:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_45(prefill_1024_kv_cache_k_15:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_46(prefill_1024_kv_cache_v_9:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_47(prefill_1024_kv_cache_v_2:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_48(prefill_1024_kv_cache_k_1:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_49(prefill_1024_kv_cache_v_8:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_50(prefill_1024_kv_cache_k_25:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_51(prefill_1024_kv_cache_v_4:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_52(prefill_1024_kv_cache_k_11:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_53(prefill_1024_kv_cache_k_19:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_54(prefill_1024_kv_cache_k_5:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_55(prefill_1024_kv_cache_v_20:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_56(prefill_1024_kv_cache_k_27:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_57(prefill_1024_kv_cache_v_14:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_58(arith.constant226) shape:[5120, 3072], type:FLOAT32
  T#1_59(arith.constant227) shape:[3072, 8192], type:FLOAT32
  T#1_60(arith.constant228) shape:[8192, 3072], type:FLOAT32
  T#1_61(arith.constant229) shape:[8192, 3072], type:FLOAT32
  T#1_62(arith.constant230) shape:[3072, 3072], type:FLOAT32
  T#1_63(arith.constant231) shape:[5120, 3072], type:FLOAT32
  T#1_64(arith.constant232) shape:[3072, 8192], type:FLOAT32
  T#1_65(arith.constant233) shape:[8192, 3072], type:FLOAT32
  T#1_66(arith.constant234) shape:[8192, 3072], type:FLOAT32
  T#1_67(arith.constant235) shape:[3072, 3072], type:FLOAT32
  T#1_68(arith.constant236) shape:[5120, 3072], type:FLOAT32
  T#1_69(arith.constant237) shape:[3072, 8192], type:FLOAT32
  T#1_70(arith.constant238) shape:[8192, 3072], type:FLOAT32
  T#1_71(arith.constant239) shape:[8192, 3072], type:FLOAT32
  T#1_72(arith.constant240) shape:[3072, 3072], type:FLOAT32
  T#1_73(arith.constant241) shape:[5120, 3072], type:FLOAT32
  T#1_74(arith.constant242) shape:[3072, 8192], type:FLOAT32
  T#1_75(arith.constant243) shape:[8192, 3072], type:FLOAT32
  T#1_76(arith.constant244) shape:[8192, 3072], type:FLOAT32
  T#1_77(arith.constant245) shape:[3072, 3072], type:FLOAT32
  T#1_78(arith.constant246) shape:[5120, 3072], type:FLOAT32
  T#1_79(arith.constant247) shape:[3072, 8192], type:FLOAT32
  T#1_80(arith.constant248) shape:[8192, 3072], type:FLOAT32
  T#1_81(arith.constant249) shape:[8192, 3072], type:FLOAT32
  T#1_82(arith.constant250) shape:[3072, 3072], type:FLOAT32
  T#1_83(arith.constant251) shape:[5120, 3072], type:FLOAT32
  T#1_84(arith.constant252) shape:[3072, 8192], type:FLOAT32
  T#1_85(arith.constant253) shape:[8192, 3072], type:FLOAT32
  T#1_86(arith.constant254) shape:[8192, 3072], type:FLOAT32
  T#1_87(arith.constant255) shape:[3072, 3072], type:FLOAT32
  T#1_88(arith.constant256) shape:[5120, 3072], type:FLOAT32
  T#1_89(arith.constant257) shape:[3072, 8192], type:FLOAT32
  T#1_90(arith.constant258) shape:[8192, 3072], type:FLOAT32
  T#1_91(arith.constant259) shape:[8192, 3072], type:FLOAT32
  T#1_92(arith.constant260) shape:[3072, 3072], type:FLOAT32
  T#1_93(arith.constant261) shape:[5120, 3072], type:FLOAT32
  T#1_94(arith.constant262) shape:[3072, 8192], type:FLOAT32
  T#1_95(arith.constant263) shape:[8192, 3072], type:FLOAT32
  T#1_96(arith.constant264) shape:[8192, 3072], type:FLOAT32
  T#1_97(arith.constant265) shape:[3072, 3072], type:FLOAT32
  T#1_98(arith.constant266) shape:[5120, 3072], type:FLOAT32
  T#1_99(arith.constant267) shape:[3072, 8192], type:FLOAT32
  T#1_100(arith.constant268) shape:[8192, 3072], type:FLOAT32
  T#1_101(arith.constant269) shape:[8192, 3072], type:FLOAT32
  T#1_102(arith.constant270) shape:[3072, 3072], type:FLOAT32
  T#1_103(arith.constant271) shape:[5120, 3072], type:FLOAT32
  T#1_104(arith.constant272) shape:[3072, 8192], type:FLOAT32
  T#1_105(arith.constant273) shape:[8192, 3072], type:FLOAT32
  T#1_106(arith.constant274) shape:[8192, 3072], type:FLOAT32
  T#1_107(arith.constant275) shape:[3072, 3072], type:FLOAT32
  T#1_108(arith.constant276) shape:[5120, 3072], type:FLOAT32
  T#1_109(arith.constant277) shape:[3072, 8192], type:FLOAT32
  T#1_110(arith.constant278) shape:[8192, 3072], type:FLOAT32
  T#1_111(arith.constant279) shape:[8192, 3072], type:FLOAT32
  T#1_112(arith.constant280) shape:[3072, 3072], type:FLOAT32
  T#1_113(arith.constant281) shape:[5120, 3072], type:FLOAT32
  T#1_114(arith.constant282) shape:[3072, 8192], type:FLOAT32
  T#1_115(arith.constant283) shape:[8192, 3072], type:FLOAT32
  T#1_116(arith.constant284) shape:[8192, 3072], type:FLOAT32
  T#1_117(arith.constant285) shape:[3072, 3072], type:FLOAT32
  T#1_118(arith.constant286) shape:[5120, 3072], type:FLOAT32
  T#1_119(arith.constant287) shape:[3072, 8192], type:FLOAT32
  T#1_120(arith.constant288) shape:[8192, 3072], type:FLOAT32
  T#1_121(arith.constant289) shape:[8192, 3072], type:FLOAT32
  T#1_122(arith.constant290) shape:[3072, 3072], type:FLOAT32
  T#1_123(arith.constant291) shape:[5120, 3072], type:FLOAT32
  T#1_124(arith.constant292) shape:[3072, 8192], type:FLOAT32
  T#1_125(arith.constant293) shape:[8192, 3072], type:FLOAT32
  T#1_126(arith.constant294) shape:[8192, 3072], type:FLOAT32
  T#1_127(arith.constant295) shape:[3072, 3072], type:FLOAT32
  T#1_128(arith.constant296) shape:[5120, 3072], type:FLOAT32
  T#1_129(arith.constant297) shape:[3072, 8192], type:FLOAT32
  T#1_130(arith.constant298) shape:[8192, 3072], type:FLOAT32
  T#1_131(arith.constant299) shape:[8192, 3072], type:FLOAT32
  T#1_132(arith.constant300) shape:[3072, 3072], type:FLOAT32
  T#1_133(arith.constant301) shape:[5120, 3072], type:FLOAT32
  T#1_134(arith.constant302) shape:[3072, 8192], type:FLOAT32
  T#1_135(arith.constant303) shape:[8192, 3072], type:FLOAT32
  T#1_136(arith.constant304) shape:[8192, 3072], type:FLOAT32
  T#1_137(arith.constant305) shape:[3072, 3072], type:FLOAT32
  T#1_138(arith.constant306) shape:[5120, 3072], type:FLOAT32
  T#1_139(arith.constant307) shape:[3072, 8192], type:FLOAT32
  T#1_140(arith.constant308) shape:[8192, 3072], type:FLOAT32
  T#1_141(arith.constant309) shape:[8192, 3072], type:FLOAT32
  T#1_142(arith.constant310) shape:[3072, 3072], type:FLOAT32
  T#1_143(arith.constant311) shape:[5120, 3072], type:FLOAT32
  T#1_144(arith.constant312) shape:[3072, 8192], type:FLOAT32
  T#1_145(arith.constant313) shape:[8192, 3072], type:FLOAT32
  T#1_146(arith.constant314) shape:[8192, 3072], type:FLOAT32
  T#1_147(arith.constant315) shape:[3072, 3072], type:FLOAT32
  T#1_148(arith.constant316) shape:[5120, 3072], type:FLOAT32
  T#1_149(arith.constant317) shape:[3072, 8192], type:FLOAT32
  T#1_150(arith.constant318) shape:[8192, 3072], type:FLOAT32
  T#1_151(arith.constant319) shape:[8192, 3072], type:FLOAT32
  T#1_152(arith.constant320) shape:[3072, 3072], type:FLOAT32
  T#1_153(arith.constant321) shape:[5120, 3072], type:FLOAT32
  T#1_154(arith.constant322) shape:[3072, 8192], type:FLOAT32
  T#1_155(arith.constant323) shape:[8192, 3072], type:FLOAT32
  T#1_156(arith.constant324) shape:[8192, 3072], type:FLOAT32
  T#1_157(arith.constant325) shape:[3072, 3072], type:FLOAT32
  T#1_158(arith.constant326) shape:[5120, 3072], type:FLOAT32
  T#1_159(arith.constant327) shape:[3072, 8192], type:FLOAT32
  T#1_160(arith.constant328) shape:[8192, 3072], type:FLOAT32
  T#1_161(arith.constant329) shape:[8192, 3072], type:FLOAT32
  T#1_162(arith.constant330) shape:[3072, 3072], type:FLOAT32
  T#1_163(arith.constant331) shape:[5120, 3072], type:FLOAT32
  T#1_164(arith.constant332) shape:[3072, 8192], type:FLOAT32
  T#1_165(arith.constant333) shape:[8192, 3072], type:FLOAT32
  T#1_166(arith.constant334) shape:[8192, 3072], type:FLOAT32
  T#1_167(arith.constant335) shape:[3072, 3072], type:FLOAT32
  T#1_168(arith.constant336) shape:[5120, 3072], type:FLOAT32
  T#1_169(arith.constant337) shape:[3072, 8192], type:FLOAT32
  T#1_170(arith.constant338) shape:[8192, 3072], type:FLOAT32
  T#1_171(arith.constant339) shape:[8192, 3072], type:FLOAT32
  T#1_172(arith.constant340) shape:[3072, 3072], type:FLOAT32
  T#1_173(arith.constant341) shape:[5120, 3072], type:FLOAT32
  T#1_174(arith.constant342) shape:[3072, 8192], type:FLOAT32
  T#1_175(arith.constant343) shape:[8192, 3072], type:FLOAT32
  T#1_176(arith.constant344) shape:[8192, 3072], type:FLOAT32
  T#1_177(arith.constant345) shape:[3072, 3072], type:FLOAT32
  T#1_178(arith.constant346) shape:[5120, 3072], type:FLOAT32
  T#1_179(arith.constant347) shape:[3072, 8192], type:FLOAT32
  T#1_180(arith.constant348) shape:[8192, 3072], type:FLOAT32
  T#1_181(arith.constant349) shape:[8192, 3072], type:FLOAT32
  T#1_182(arith.constant350) shape:[3072, 3072], type:FLOAT32
  T#1_183(arith.constant351) shape:[5120, 3072], type:FLOAT32
  T#1_184(arith.constant352) shape:[3072, 8192], type:FLOAT32
  T#1_185(arith.constant353) shape:[8192, 3072], type:FLOAT32
  T#1_186(arith.constant354) shape:[8192, 3072], type:FLOAT32
  T#1_187(arith.constant355) shape:[3072, 3072], type:FLOAT32
  T#1_188(arith.constant356) shape:[5120, 3072], type:FLOAT32
  T#1_189(arith.constant357) shape:[3072, 8192], type:FLOAT32
  T#1_190(arith.constant358) shape:[8192, 3072], type:FLOAT32
  T#1_191(arith.constant359) shape:[8192, 3072], type:FLOAT32
  T#1_192(arith.constant360) shape:[3072, 3072], type:FLOAT32
  T#1_193(arith.constant361) shape:[5120, 3072], type:FLOAT32
  T#1_194(arith.constant362) shape:[1], type:INT32
  T#1_195(arith.constant363) shape:[4], type:INT32
  T#1_196(arith.constant364) shape:[4], type:INT32
  T#1_197(arith.constant365) shape:[4], type:INT32
  T#1_198(arith.constant366) shape:[4], type:INT32
  T#1_199(arith.constant367) shape:[4], type:INT32
  T#1_200(arith.constant368) shape:[4], type:INT32
  T#1_201(arith.constant369) shape:[4], type:INT32
  T#1_202(arith.constant370) shape:[4], type:INT32
  T#1_203(arith.constant371) shape:[], type:FLOAT32
  T#1_204(arith.constant372) shape:[4], type:INT32
  T#1_205(arith.constant373) shape:[1, 1, 3072], type:FLOAT32
  T#1_206(arith.constant374) shape:[1, 1, 3072], type:FLOAT32
  T#1_207(arith.constant375) shape:[1, 1, 3072], type:FLOAT32
  T#1_208(arith.constant376) shape:[1, 1, 3072], type:FLOAT32
  T#1_209(arith.constant377) shape:[1, 1, 3072], type:FLOAT32
  T#1_210(arith.constant378) shape:[1, 1, 3072], type:FLOAT32
  T#1_211(arith.constant379) shape:[1, 1, 3072], type:FLOAT32
  T#1_212(arith.constant380) shape:[1, 1, 3072], type:FLOAT32
  T#1_213(arith.constant381) shape:[1, 1, 3072], type:FLOAT32
  T#1_214(arith.constant382) shape:[1, 1, 3072], type:FLOAT32
  T#1_215(arith.constant383) shape:[1, 1, 3072], type:FLOAT32
  T#1_216(arith.constant384) shape:[1, 1, 3072], type:FLOAT32
  T#1_217(arith.constant385) shape:[1, 1, 3072], type:FLOAT32
  T#1_218(arith.constant386) shape:[1, 1, 3072], type:FLOAT32
  T#1_219(arith.constant387) shape:[1, 1, 3072], type:FLOAT32
  T#1_220(arith.constant388) shape:[1, 1, 3072], type:FLOAT32
  T#1_221(arith.constant389) shape:[1, 1, 3072], type:FLOAT32
  T#1_222(arith.constant390) shape:[1, 1, 3072], type:FLOAT32
  T#1_223(arith.constant391) shape:[1, 1, 3072], type:FLOAT32
  T#1_224(arith.constant392) shape:[1, 1, 3072], type:FLOAT32
  T#1_225(arith.constant393) shape:[1, 1, 3072], type:FLOAT32
  T#1_226(arith.constant394) shape:[1, 1, 3072], type:FLOAT32
  T#1_227(arith.constant395) shape:[1, 1, 3072], type:FLOAT32
  T#1_228(arith.constant396) shape:[1, 1, 3072], type:FLOAT32
  T#1_229(arith.constant397) shape:[1, 1, 3072], type:FLOAT32
  T#1_230(arith.constant398) shape:[1, 1, 3072], type:FLOAT32
  T#1_231(arith.constant399) shape:[1, 1, 3072], type:FLOAT32
  T#1_232(arith.constant400) shape:[1, 1, 3072], type:FLOAT32
  T#1_233(arith.constant401) shape:[1, 1, 3072], type:FLOAT32
  T#1_234(arith.constant402) shape:[1, 1, 3072], type:FLOAT32
  T#1_235(arith.constant403) shape:[1, 1, 3072], type:FLOAT32
  T#1_236(arith.constant404) shape:[1, 1, 3072], type:FLOAT32
  T#1_237(arith.constant405) shape:[1, 1, 3072], type:FLOAT32
  T#1_238(arith.constant406) shape:[1, 1, 3072], type:FLOAT32
  T#1_239(arith.constant407) shape:[1, 1, 3072], type:FLOAT32
  T#1_240(arith.constant408) shape:[1, 1, 3072], type:FLOAT32
  T#1_241(arith.constant409) shape:[1, 1, 3072], type:FLOAT32
  T#1_242(arith.constant410) shape:[1, 1, 3072], type:FLOAT32
  T#1_243(arith.constant411) shape:[1, 1, 3072], type:FLOAT32
  T#1_244(arith.constant412) shape:[1, 1, 3072], type:FLOAT32
  T#1_245(arith.constant413) shape:[1, 1, 3072], type:FLOAT32
  T#1_246(arith.constant414) shape:[1, 1, 3072], type:FLOAT32
  T#1_247(arith.constant415) shape:[1, 1, 3072], type:FLOAT32
  T#1_248(arith.constant416) shape:[1, 1, 3072], type:FLOAT32
  T#1_249(arith.constant417) shape:[1, 1, 3072], type:FLOAT32
  T#1_250(arith.constant418) shape:[1, 1, 3072], type:FLOAT32
  T#1_251(arith.constant419) shape:[1, 1, 3072], type:FLOAT32
  T#1_252(arith.constant420) shape:[1, 1, 3072], type:FLOAT32
  T#1_253(arith.constant421) shape:[1, 1, 3072], type:FLOAT32
  T#1_254(arith.constant422) shape:[1, 1, 3072], type:FLOAT32
  T#1_255(arith.constant423) shape:[1, 1, 3072], type:FLOAT32
  T#1_256(arith.constant424) shape:[1, 1, 3072], type:FLOAT32
  T#1_257(arith.constant425) shape:[1, 1, 3072], type:FLOAT32
  T#1_258(arith.constant426) shape:[1, 1, 3072], type:FLOAT32
  T#1_259(arith.constant427) shape:[1, 1, 3072], type:FLOAT32
  T#1_260(arith.constant428) shape:[1280, 1, 1, 1280], type:FLOAT32
  T#1_261(arith.constant429) shape:[0], type:INT32
  T#1_262(arith.constant430) shape:[4], type:INT32
  T#1_263(arith.constant431) shape:[4], type:INT32
  T#1_264(arith.constant432) shape:[4], type:INT32
  T#1_265(arith.constant433) shape:[3], type:INT32
  T#1_266(arith.constant434) shape:[], type:FLOAT32
  T#1_267(arith.constant435) shape:[], type:INT64
  T#1_268(arith.constant436) shape:[], type:INT64
  T#1_269(arith.constant437) shape:[1, 64], type:FLOAT32
  T#1_270(arith.constant438) shape:[2], type:INT32
  T#1_271(arith.constant439) shape:[3], type:INT32
  T#1_272(arith.constant440) shape:[1], type:INT32
  T#1_273(arith.constant441) shape:[1], type:INT32
  T#1_274(arith.constant442) shape:[4], type:INT32
  T#1_275(arith.constant443) shape:[1], type:INT32
  T#1_276(arith.constant444) shape:[], type:INT32
  T#1_277(arith.constant445) shape:[], type:FLOAT32
  T#1_278(arith.constant446) shape:[], type:INT32
  T#1_279(XlaCallModule/ReadVariableOp_196;StatefulPartitionedCall_1) shape:[128256, 3072], type:FLOAT32
  T#1_280(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;3) shape:[1024], type:INT32
  T#1_281(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;4) shape:[1024, 3072], type:FLOAT32
  T#1_282(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;5) shape:[1, 1024, 3072], type:FLOAT32
  T#1_283(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;17) shape:[1024], type:FLOAT32
  T#1_284(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;18) shape:[1024, 1], type:FLOAT32
  T#1_285(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;19) shape:[1024, 64], type:FLOAT32
  T#1_286(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;20) shape:[1024, 64], type:FLOAT32
  T#1_287(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;21) shape:[1024, 64], type:FLOAT32
  T#1_288(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;22) shape:[1024], type:BOOL
  T#1_289(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;23) shape:[1024], type:INT32
  T#1_290(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;24) shape:[1024], type:INT32
  T#1_291(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;25) shape:[1024, 1], type:INT32
  T#1_292(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;26) shape:[1024, 1], type:INT64
  T#1_293(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;27) shape:[1024, 1], type:BOOL
  T#1_294(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;28) shape:[1024, 1], type:BOOL
  T#1_295(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;29) shape:[1024, 1], type:BOOL
  T#1_296(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;30) shape:[1024], type:BOOL
  T#1_297(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;31) shape:[1024, 1, 1, 1280], type:FLOAT32
  T#1_298(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;32) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#1_299(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;33) shape:[1, 1, 1024, 1], type:BOOL
  T#1_300(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module;34) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#1_301(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_302(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;5) shape:[1, 1024], type:FLOAT32
  T#1_303(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;6) shape:[1, 1024], type:FLOAT32
  T#1_304(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024], type:FLOAT32
  T#1_305(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024, 1], type:FLOAT32
  T#1_306(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024, 1], type:FLOAT32
  T#1_307(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_308(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024, 3072], type:FLOAT32
  T#1_309(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_310(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_311(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_312(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_313(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_314(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_315(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_316(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_317(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_318(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_319(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_320(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_321(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_322(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_323(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_324(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_325(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_326(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_327(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_328(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_329(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_330(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_331(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_332(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_333(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_334(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_335(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_336(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_337(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[1], type:INT32
  T#1_338(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[], type:INT32
  T#1_339(tfl.pack1) shape:[4], type:INT32
  T#1_340(StatefulPartitionedCall_1:0) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_341(StatefulPartitionedCall_1:28) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_342(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_343(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_344(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_345(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/torch.nn.modules.sparse.Embedding_tok_embedding;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_346(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_347(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_348(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_349(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_350(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_351(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_352(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_353(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_354(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_355(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_356(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_357(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_358(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_359(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_360(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_361(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_362(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_363(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_364(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_365(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_366(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_367(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_368(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_369(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_370(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_371(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_372(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_373(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_374(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_375(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_376(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_377(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_378(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_379(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_380(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_381(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_382(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_383(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_384(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_385(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_386(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_387(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_388(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_389(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_390(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_391(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_392(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_393(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_394(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_395(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_396(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_397(StatefulPartitionedCall_1:1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_398(StatefulPartitionedCall_1:29) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_399(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_400(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_401(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_402(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_403(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_404(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_405(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_406(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_407(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_408(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_409(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_410(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_411(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_412(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_413(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_414(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_415(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_416(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_417(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_418(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_419(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_420(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_421(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_422(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_423(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_424(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_425(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_426(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_427(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_428(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_429(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_430(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_431(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_432(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_433(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_434(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_435(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_436(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_437(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_438(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_439(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_440(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_441(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_442(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_443(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_444(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_445(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_446(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_447(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_448(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_449(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_450(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_451(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_452(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_453(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_454(StatefulPartitionedCall_1:12) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_455(StatefulPartitionedCall_1:40) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_456(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_457(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_458(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_459(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_460(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_461(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_462(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_463(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_464(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_465(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_466(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_467(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_468(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_469(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_470(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_471(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_472(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_473(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_474(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_475(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_476(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_477(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_478(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_479(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_480(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_481(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_482(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_483(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_484(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_485(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_486(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_487(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_488(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_489(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_490(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_491(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_492(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_493(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_494(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_495(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_496(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_497(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_498(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_499(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_500(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_501(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_502(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_503(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_504(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_505(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_506(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_507(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_508(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_509(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_510(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_511(StatefulPartitionedCall_1:21) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_512(StatefulPartitionedCall_1:49) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_513(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_514(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_515(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_516(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_517(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_518(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_519(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_520(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_521(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_522(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_523(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_524(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_525(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_526(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_527(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_528(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_529(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_530(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_531(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_532(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_533(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_534(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_535(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_536(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_537(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_538(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_539(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_540(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_541(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_542(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_543(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_544(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_545(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_546(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_547(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_548(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_549(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_550(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_551(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_552(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_553(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_554(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_555(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_556(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_557(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_558(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_559(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_560(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_561(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_562(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_563(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_564(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_565(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_566(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_567(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_568(StatefulPartitionedCall_1:22) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_569(StatefulPartitionedCall_1:50) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_570(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_571(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_572(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_573(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_574(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_575(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_576(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_577(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_578(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_579(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_580(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_581(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_582(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_583(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_584(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_585(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_586(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_587(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_588(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_589(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_590(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_591(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_592(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_593(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_594(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_595(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_596(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_597(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_598(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_599(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_600(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_601(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_602(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_603(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_604(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_605(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_606(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_607(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_608(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_609(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_610(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_611(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_612(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_613(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_614(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_615(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_616(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_617(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_618(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_619(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_620(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_621(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_622(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_623(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_624(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_625(StatefulPartitionedCall_1:23) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_626(StatefulPartitionedCall_1:51) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_627(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_628(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_629(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_630(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_631(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_632(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_633(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_634(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_635(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_636(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_637(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_638(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_639(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_640(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_641(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_642(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_643(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_644(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_645(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_646(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_647(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_648(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_649(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_650(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_651(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_652(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_653(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_654(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_655(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_656(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_657(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_658(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_659(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_660(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_661(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_662(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_663(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_664(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_665(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_666(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_667(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_668(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_669(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_670(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_671(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_672(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_673(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_674(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_675(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_676(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_677(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_678(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_679(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_680(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_681(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_682(StatefulPartitionedCall_1:24) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_683(StatefulPartitionedCall_1:52) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_684(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_685(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_686(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_687(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_688(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_689(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_690(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_691(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_692(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_693(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_694(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_695(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_696(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_697(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_698(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_699(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_700(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_701(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_702(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_703(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_704(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_705(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_706(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_707(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_708(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_709(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_710(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_711(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_712(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_713(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_714(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_715(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_716(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_717(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_718(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_719(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_720(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_721(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_722(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_723(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_724(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_725(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_726(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_727(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_728(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_729(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_730(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_731(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_732(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_733(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_734(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_735(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_736(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_737(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_738(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_739(StatefulPartitionedCall_1:25) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_740(StatefulPartitionedCall_1:53) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_741(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_742(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_743(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_744(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_745(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_746(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_747(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_748(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_749(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_750(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_751(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_752(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_753(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_754(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_755(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_756(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_757(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_758(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_759(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_760(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_761(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_762(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_763(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_764(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_765(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_766(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_767(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_768(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_769(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_770(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_771(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_772(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_773(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_774(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_775(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_776(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_777(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_778(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_779(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_780(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_781(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_782(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_783(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_784(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_785(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_786(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_787(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_788(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_789(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_790(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_791(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_792(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_793(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_794(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_795(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_796(StatefulPartitionedCall_1:26) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_797(StatefulPartitionedCall_1:54) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_798(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_799(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_800(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_801(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_802(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_803(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_804(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_805(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_806(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_807(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_808(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_809(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_810(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_811(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_812(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_813(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_814(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_815(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_816(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_817(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_818(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_819(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_820(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_821(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_822(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_823(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_824(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_825(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_826(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_827(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_828(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_829(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_830(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_831(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_832(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_833(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_834(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_835(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_836(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_837(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_838(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_839(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_840(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_841(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_842(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_843(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_844(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_845(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_846(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_847(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_848(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_849(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_850(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_851(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_852(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_853(StatefulPartitionedCall_1:27) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_854(StatefulPartitionedCall_1:55) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_855(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_856(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_857(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_858(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_859(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_860(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_861(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_862(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_863(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_864(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_865(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_866(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_867(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_868(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_869(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_870(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_871(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_872(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_873(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_874(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_875(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_876(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_877(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_878(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_879(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_880(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_881(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_882(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_883(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_884(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_885(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_886(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_887(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_888(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_889(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_890(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_891(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_892(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_893(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_894(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_895(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_896(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_897(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_898(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_899(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_900(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_901(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_902(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_903(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_904(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_905(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_906(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_907(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_908(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_909(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_910(StatefulPartitionedCall_1:2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_911(StatefulPartitionedCall_1:30) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_912(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_913(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_914(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_915(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_916(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_917(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_918(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_919(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_920(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_921(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_922(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_923(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_924(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_925(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_926(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_927(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_928(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_929(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_930(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_931(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_932(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_933(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_934(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_935(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_936(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_937(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_938(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_939(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_940(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_941(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_942(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_943(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_944(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_945(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_946(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_947(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_948(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_949(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_950(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_951(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_952(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_953(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_954(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_955(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_956(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_957(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_958(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_959(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_960(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_961(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_962(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_963(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_964(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_965(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_966(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_967(StatefulPartitionedCall_1:3) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_968(StatefulPartitionedCall_1:31) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_969(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_970(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_971(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_972(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_973(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_974(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_975(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_976(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_977(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_978(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_979(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_980(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_981(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_982(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_983(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_984(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_985(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_986(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_987(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_988(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_989(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_990(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_991(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_992(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_993(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_994(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_995(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_996(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_997(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_998(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_999(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1000(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1001(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1002(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1003(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1004(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1005(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1006(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1007(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1008(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1009(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1010(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1011(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1012(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1013(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1014(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1015(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1016(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1017(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1018(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1019(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1020(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1021(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1022(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1023(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1024(StatefulPartitionedCall_1:4) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1025(StatefulPartitionedCall_1:32) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1026(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1027(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1028(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1029(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1030(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1031(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1032(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1033(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1034(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1035(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1036(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1037(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1038(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1039(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1040(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1041(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1042(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1043(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1044(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1045(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1046(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1047(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1048(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1049(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1050(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1051(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1052(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1053(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1054(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1055(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1056(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1057(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1058(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1059(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1060(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1061(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1062(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1063(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1064(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1065(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1066(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1067(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1068(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1069(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1070(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1071(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1072(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1073(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1074(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1075(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1076(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1077(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1078(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1079(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1080(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1081(StatefulPartitionedCall_1:5) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1082(StatefulPartitionedCall_1:33) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1083(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1084(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1085(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1086(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1087(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1088(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1089(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1090(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1091(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1092(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1093(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1094(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1095(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1096(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1097(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1098(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1099(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1100(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1101(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1102(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1103(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1104(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1105(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1106(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1107(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1108(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1109(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1110(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1111(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1112(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1113(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1114(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1115(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1116(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1117(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1118(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1119(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1120(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1121(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1122(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1123(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1124(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1125(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1126(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1127(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1128(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1129(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1130(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1131(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1132(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1133(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1134(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1135(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1136(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1137(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1138(StatefulPartitionedCall_1:6) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1139(StatefulPartitionedCall_1:34) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1140(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1141(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1142(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1143(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1144(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1145(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1146(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1147(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1148(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1149(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1150(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1151(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1152(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1153(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1154(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1155(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1156(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1157(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1158(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1159(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1160(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1161(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1162(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1163(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1164(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1165(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1166(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1167(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1168(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1169(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1170(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1171(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1172(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1173(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1174(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1175(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1176(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1177(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1178(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1179(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1180(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1181(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1182(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1183(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1184(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1185(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1186(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1187(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1188(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1189(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1190(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1191(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1192(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1193(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1194(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1195(StatefulPartitionedCall_1:7) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1196(StatefulPartitionedCall_1:35) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1197(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1198(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1199(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1200(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1201(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1202(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1203(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1204(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1205(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1206(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1207(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1208(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1209(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1210(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1211(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1212(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1213(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1214(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1215(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1216(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1217(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1218(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1219(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1220(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1221(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1222(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1223(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1224(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1225(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1226(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1227(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1228(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1229(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1230(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1231(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1232(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1233(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1234(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1235(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1236(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1237(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1238(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1239(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1240(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1241(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1242(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1243(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1244(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1245(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1246(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1247(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1248(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1249(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1250(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1251(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1252(StatefulPartitionedCall_1:8) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1253(StatefulPartitionedCall_1:36) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1254(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1255(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1256(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1257(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1258(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1259(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1260(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1261(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1262(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1263(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1264(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1265(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1266(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1267(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1268(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1269(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1270(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1271(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1272(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1273(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1274(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1275(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1276(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1277(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1278(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1279(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1280(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1281(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1282(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1283(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1284(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1285(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1286(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1287(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1288(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1289(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1290(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1291(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1292(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1293(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1294(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1295(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1296(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1297(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1298(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1299(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1300(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1301(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1302(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1303(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1304(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1305(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1306(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1307(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1308(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1309(StatefulPartitionedCall_1:9) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1310(StatefulPartitionedCall_1:37) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1311(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1312(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1313(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1314(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1315(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1316(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1317(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1318(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1319(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1320(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1321(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1322(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1323(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1324(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1325(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1326(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1327(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1328(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1329(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1330(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1331(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1332(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1333(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1334(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1335(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1336(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1337(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1338(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1339(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1340(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1341(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1342(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1343(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1344(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1345(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1346(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1347(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1348(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1349(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1350(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1351(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1352(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1353(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1354(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1355(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1356(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1357(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1358(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1359(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1360(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1361(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1362(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1363(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1364(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1365(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1366(StatefulPartitionedCall_1:10) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1367(StatefulPartitionedCall_1:38) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1368(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1369(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1370(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1371(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1372(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1373(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1374(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1375(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1376(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1377(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1378(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1379(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1380(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1381(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1382(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1383(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1384(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1385(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1386(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1387(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1388(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1389(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1390(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1391(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1392(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1393(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1394(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1395(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1396(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1397(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1398(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1399(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1400(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1401(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1402(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1403(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1404(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1405(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1406(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1407(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1408(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1409(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1410(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1411(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1412(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1413(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1414(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1415(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1416(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1417(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1418(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1419(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1420(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1421(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1422(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1423(StatefulPartitionedCall_1:11) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1424(StatefulPartitionedCall_1:39) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1425(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1426(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1427(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1428(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1429(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1430(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1431(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1432(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1433(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1434(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1435(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1436(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1437(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1438(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1439(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1440(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1441(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1442(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1443(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1444(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1445(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1446(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1447(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1448(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1449(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1450(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1451(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1452(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1453(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1454(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1455(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1456(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1457(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1458(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1459(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1460(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1461(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1462(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1463(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1464(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1465(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1466(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1467(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1468(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1469(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1470(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1471(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1472(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1473(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1474(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1475(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1476(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1477(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1478(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1479(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1480(StatefulPartitionedCall_1:13) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1481(StatefulPartitionedCall_1:41) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1482(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1483(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1484(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1485(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1486(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1487(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1488(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1489(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1490(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1491(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1492(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1493(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1494(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1495(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1496(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1497(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1498(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1499(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1500(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1501(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1502(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1503(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1504(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1505(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1506(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1507(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1508(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1509(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1510(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1511(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1512(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1513(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1514(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1515(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1516(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1517(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1518(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1519(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1520(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1521(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1522(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1523(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1524(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1525(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1526(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1527(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1528(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1529(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1530(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1531(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1532(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1533(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1534(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1535(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1536(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1537(StatefulPartitionedCall_1:14) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1538(StatefulPartitionedCall_1:42) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1539(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1540(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1541(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1542(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1543(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1544(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1545(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1546(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1547(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1548(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1549(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1550(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1551(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1552(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1553(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1554(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1555(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1556(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1557(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1558(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1559(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1560(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1561(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1562(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1563(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1564(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1565(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1566(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1567(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1568(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1569(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1570(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1571(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1572(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1573(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1574(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1575(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1576(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1577(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1578(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1579(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1580(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1581(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1582(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1583(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1584(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1585(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1586(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1587(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1588(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1589(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1590(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1591(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1592(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1593(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1594(StatefulPartitionedCall_1:15) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1595(StatefulPartitionedCall_1:43) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1596(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1597(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1598(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1599(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1600(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1601(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1602(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1603(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1604(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1605(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1606(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1607(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1608(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1609(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1610(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1611(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1612(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1613(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1614(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1615(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1616(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1617(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1618(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1619(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1620(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1621(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1622(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1623(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1624(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1625(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1626(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1627(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1628(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1629(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1630(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1631(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1632(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1633(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1634(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1635(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1636(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1637(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1638(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1639(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1640(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1641(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1642(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1643(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1644(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1645(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1646(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1647(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1648(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1649(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1650(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1651(StatefulPartitionedCall_1:16) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1652(StatefulPartitionedCall_1:44) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1653(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1654(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1655(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1656(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1657(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1658(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1659(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1660(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1661(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1662(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1663(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1664(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1665(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1666(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1667(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1668(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1669(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1670(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1671(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1672(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1673(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1674(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1675(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1676(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1677(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1678(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1679(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1680(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1681(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1682(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1683(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1684(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1685(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1686(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1687(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1688(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1689(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1690(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1691(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1692(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1693(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1694(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1695(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1696(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1697(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1698(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1699(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1700(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1701(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1702(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1703(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1704(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1705(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1706(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1707(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1708(StatefulPartitionedCall_1:17) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1709(StatefulPartitionedCall_1:45) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1710(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1711(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1712(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1713(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1714(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1715(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1716(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1717(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1718(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1719(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1720(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1721(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1722(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1723(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1724(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1725(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1726(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1727(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1728(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1729(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1730(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1731(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1732(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1733(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1734(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1735(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1736(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1737(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1738(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1739(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1740(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1741(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1742(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1743(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1744(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1745(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1746(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1747(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1748(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1749(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1750(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1751(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1752(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1753(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1754(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1755(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1756(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1757(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1758(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1759(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1760(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1761(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1762(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1763(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1764(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1765(StatefulPartitionedCall_1:18) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1766(StatefulPartitionedCall_1:46) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1767(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1768(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1769(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1770(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1771(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1772(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1773(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1774(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1775(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1776(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1777(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1778(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1779(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1780(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1781(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1782(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1783(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1784(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1785(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1786(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1787(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1788(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1789(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1790(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1791(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1792(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1793(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1794(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1795(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1796(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 384], type:FLOAT32
  T#1_1797(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1798(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1799(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1800(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1801(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1802(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1803(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1804(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1805(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1806(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1807(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1808(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 24, 1024, 64], type:FLOAT32
  T#1_1809(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 24, 1024, 128], type:FLOAT32
  T#1_1810(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1811(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1812(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1813(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1814(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;12) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1815(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;13) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1816(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1817(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;14) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1818(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;15) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1819(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1820(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1821(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1822(StatefulPartitionedCall_1:19) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1823(StatefulPartitionedCall_1:47) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1824(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 1024, 24, 128], type:FLOAT32
  T#1_1825(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1826(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_output_projection;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1827(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26;2) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1828(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1829(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1830(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1831(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1832(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1833(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1834(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1835(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.normalization.RMSNorm_post_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1836(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1837(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;3) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1838(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;4) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1839(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w3;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w1;1) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1840(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff;5) shape:[1, 1024, 8192], type:FLOAT32
  T#1_1841(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.feed_forward.GatedFeedForward_ff/torch.nn.modules.linear.Linear_w2;1) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1842(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26;3) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1843(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;7) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1844(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;8) shape:[1, 1024], type:FLOAT32
  T#1_1845(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;9) shape:[1, 1024], type:FLOAT32
  T#1_1846(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;10) shape:[1, 1024], type:FLOAT32
  T#1_1847(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;11) shape:[1, 1024, 1], type:FLOAT32
  T#1_1848(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;12) shape:[1, 1024, 1], type:FLOAT32
  T#1_1849(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;13) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1850(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.normalization.RMSNorm_pre_atten_norm;14) shape:[1, 1024, 3072], type:FLOAT32
  T#1_1851(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;2) shape:[1, 1024, 5120], type:FLOAT32
  T#1_1852(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func/torch.nn.modules.linear.Linear_qkv_projection;3) shape:[1, 1024, 8, 640], type:FLOAT32
  T#1_1853(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;18) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1854(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;19) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1855(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;20) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1856(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;21) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1857(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;22) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1858(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;8) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1859(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;9) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1860(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;23) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1861(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;10) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1862(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;;ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;11) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1863(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;24) shape:[1, 8, 1024, 64], type:FLOAT32
  T#1_1864(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;25) shape:[1, 8, 1024, 128], type:FLOAT32
  T#1_1865(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;26) shape:[1, 1024, 8, 128], type:FLOAT32
  T#1_1866(StatefulPartitionedCall_1:20) shape:[1, 1280, 8, 128], type:FLOAT32
  T#1_1867(StatefulPartitionedCall_1:48) shape:[1, 1280, 8, 128], type:FLOAT32

Subgraph#2 odml.scaled_dot_product_attention.impl(T#2_0, T#2_1, T#2_2, T#2_3) -> [T#2_33]
  Op#0 TRANSPOSE(T#2_1, T#2_12) -> [T#2_15]
  Op#1 TRANSPOSE(T#2_2, T#2_12) -> [T#2_16]
  Op#2 RESHAPE(T#2_15, T#2_11) -> [T#2_17]
  Op#3 BROADCAST_TO(T#2_17, T#2_13) -> [T#2_18]
  Op#4 RESHAPE(T#2_18, T#2_10) -> [T#2_19]
  Op#5 RESHAPE(T#2_16, T#2_11) -> [T#2_20]
  Op#6 BROADCAST_TO(T#2_20, T#2_13) -> [T#2_21]
  Op#7 MUL(T#2_0, T#2_14) -> [T#2_22]
  Op#8 MUL(T#2_19, T#2_14) -> [T#2_23]
  Op#9 TRANSPOSE(T#2_23, T#2_9) -> [T#2_24]
  Op#10 RESHAPE(T#2_22, T#2_8) -> [T#2_25]
  Op#11 RESHAPE(T#2_24, T#2_7) -> [T#2_26]
  Op#12 BATCH_MATMUL(T#2_25, T#2_26) -> [T#2_27]
  Op#13 ADD(T#2_27, T#2_3) -> [T#2_28]
  Op#14 SOFTMAX(T#2_28) -> [T#2_29]
  Op#15 RESHAPE(T#2_29, T#2_6) -> [T#2_30]
  Op#16 RESHAPE(T#2_21, T#2_5) -> [T#2_31]
  Op#17 BATCH_MATMUL(T#2_30, T#2_31) -> [T#2_32]
  Op#18 RESHAPE(T#2_32, T#2_4) -> [T#2_33]

Tensors of Subgraph#2
  T#2_0(odml.scaled_dot_product_attention.impl_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#2_1(odml.scaled_dot_product_attention.impl_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#2_2(odml.scaled_dot_product_attention.impl_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#2_3(odml.scaled_dot_product_attention.impl_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#2_4(arith.constant447) shape:[4], type:INT32
  T#2_5(arith.constant448) shape:[3], type:INT32
  T#2_6(arith.constant449) shape:[3], type:INT32
  T#2_7(arith.constant450) shape:[3], type:INT32
  T#2_8(arith.constant451) shape:[3], type:INT32
  T#2_9(arith.constant452) shape:[4], type:INT32
  T#2_10(arith.constant453) shape:[4], type:INT32
  T#2_11(arith.constant454) shape:[5], type:INT32
  T#2_12(arith.constant455) shape:[4], type:INT32
  T#2_13(arith.constant456) shape:[5], type:INT64
  T#2_14(arith.constant457) shape:[], type:FLOAT32
  T#2_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;27) shape:[1, 8, 1280, 128], type:FLOAT32
  T#2_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;28) shape:[1, 8, 1280, 128], type:FLOAT32
  T#2_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;29) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#2_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;30) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#2_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;31) shape:[1, 24, 1280, 128], type:FLOAT32
  T#2_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;32) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#2_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;33) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#2_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;34) shape:[1, 1, 24, 128], type:FLOAT32
  T#2_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;35) shape:[1, 24, 1280, 128], type:FLOAT32
  T#2_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;36) shape:[1, 24, 128, 1280], type:FLOAT32
  T#2_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[24, 1, 128], type:FLOAT32
  T#2_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[24, 128, 1280], type:FLOAT32
  T#2_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[24, 1, 1280], type:FLOAT32
  T#2_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 24, 1, 1280], type:FLOAT32
  T#2_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1, 1280], type:FLOAT32
  T#2_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[24, 1, 1280], type:FLOAT32
  T#2_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[24, 1280, 128], type:FLOAT32
  T#2_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[24, 1, 128], type:FLOAT32
  T#2_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_27/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#3 odml.scaled_dot_product_attention.impl_0(T#3_0, T#3_1, T#3_2, T#3_3) -> [T#3_33]
  Op#0 TRANSPOSE(T#3_1, T#3_12) -> [T#3_15]
  Op#1 TRANSPOSE(T#3_2, T#3_12) -> [T#3_16]
  Op#2 RESHAPE(T#3_15, T#3_11) -> [T#3_17]
  Op#3 BROADCAST_TO(T#3_17, T#3_13) -> [T#3_18]
  Op#4 RESHAPE(T#3_18, T#3_10) -> [T#3_19]
  Op#5 RESHAPE(T#3_16, T#3_11) -> [T#3_20]
  Op#6 BROADCAST_TO(T#3_20, T#3_13) -> [T#3_21]
  Op#7 MUL(T#3_0, T#3_14) -> [T#3_22]
  Op#8 MUL(T#3_19, T#3_14) -> [T#3_23]
  Op#9 TRANSPOSE(T#3_23, T#3_9) -> [T#3_24]
  Op#10 RESHAPE(T#3_22, T#3_8) -> [T#3_25]
  Op#11 RESHAPE(T#3_24, T#3_7) -> [T#3_26]
  Op#12 BATCH_MATMUL(T#3_25, T#3_26) -> [T#3_27]
  Op#13 ADD(T#3_27, T#3_3) -> [T#3_28]
  Op#14 SOFTMAX(T#3_28) -> [T#3_29]
  Op#15 RESHAPE(T#3_29, T#3_6) -> [T#3_30]
  Op#16 RESHAPE(T#3_21, T#3_5) -> [T#3_31]
  Op#17 BATCH_MATMUL(T#3_30, T#3_31) -> [T#3_32]
  Op#18 RESHAPE(T#3_32, T#3_4) -> [T#3_33]

Tensors of Subgraph#3
  T#3_0(odml.scaled_dot_product_attention.impl_0_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#3_1(odml.scaled_dot_product_attention.impl_0_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#3_2(odml.scaled_dot_product_attention.impl_0_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#3_3(odml.scaled_dot_product_attention.impl_0_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#3_4(arith.constant458) shape:[4], type:INT32
  T#3_5(arith.constant459) shape:[3], type:INT32
  T#3_6(arith.constant460) shape:[3], type:INT32
  T#3_7(arith.constant461) shape:[3], type:INT32
  T#3_8(arith.constant462) shape:[3], type:INT32
  T#3_9(arith.constant463) shape:[4], type:INT32
  T#3_10(arith.constant464) shape:[4], type:INT32
  T#3_11(arith.constant465) shape:[5], type:INT32
  T#3_12(arith.constant466) shape:[4], type:INT32
  T#3_13(arith.constant467) shape:[5], type:INT64
  T#3_14(arith.constant468) shape:[], type:FLOAT32
  T#3_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#3_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#3_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#3_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#3_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#3_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#3_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#3_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#3_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#3_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#3_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#3_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#3_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#3_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#3_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#3_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#3_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#3_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#3_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#4 odml.scaled_dot_product_attention.impl_1(T#4_0, T#4_1, T#4_2, T#4_3) -> [T#4_33]
  Op#0 TRANSPOSE(T#4_1, T#4_12) -> [T#4_15]
  Op#1 TRANSPOSE(T#4_2, T#4_12) -> [T#4_16]
  Op#2 RESHAPE(T#4_15, T#4_11) -> [T#4_17]
  Op#3 BROADCAST_TO(T#4_17, T#4_13) -> [T#4_18]
  Op#4 RESHAPE(T#4_18, T#4_10) -> [T#4_19]
  Op#5 RESHAPE(T#4_16, T#4_11) -> [T#4_20]
  Op#6 BROADCAST_TO(T#4_20, T#4_13) -> [T#4_21]
  Op#7 MUL(T#4_0, T#4_14) -> [T#4_22]
  Op#8 MUL(T#4_19, T#4_14) -> [T#4_23]
  Op#9 TRANSPOSE(T#4_23, T#4_9) -> [T#4_24]
  Op#10 RESHAPE(T#4_22, T#4_8) -> [T#4_25]
  Op#11 RESHAPE(T#4_24, T#4_7) -> [T#4_26]
  Op#12 BATCH_MATMUL(T#4_25, T#4_26) -> [T#4_27]
  Op#13 ADD(T#4_27, T#4_3) -> [T#4_28]
  Op#14 SOFTMAX(T#4_28) -> [T#4_29]
  Op#15 RESHAPE(T#4_29, T#4_6) -> [T#4_30]
  Op#16 RESHAPE(T#4_21, T#4_5) -> [T#4_31]
  Op#17 BATCH_MATMUL(T#4_30, T#4_31) -> [T#4_32]
  Op#18 RESHAPE(T#4_32, T#4_4) -> [T#4_33]

Tensors of Subgraph#4
  T#4_0(odml.scaled_dot_product_attention.impl_1_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#4_1(odml.scaled_dot_product_attention.impl_1_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#4_2(odml.scaled_dot_product_attention.impl_1_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#4_3(odml.scaled_dot_product_attention.impl_1_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#4_4(arith.constant469) shape:[4], type:INT32
  T#4_5(arith.constant470) shape:[3], type:INT32
  T#4_6(arith.constant471) shape:[3], type:INT32
  T#4_7(arith.constant472) shape:[3], type:INT32
  T#4_8(arith.constant473) shape:[3], type:INT32
  T#4_9(arith.constant474) shape:[4], type:INT32
  T#4_10(arith.constant475) shape:[4], type:INT32
  T#4_11(arith.constant476) shape:[5], type:INT32
  T#4_12(arith.constant477) shape:[4], type:INT32
  T#4_13(arith.constant478) shape:[5], type:INT64
  T#4_14(arith.constant479) shape:[], type:FLOAT32
  T#4_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#4_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#4_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#4_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#4_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#4_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#4_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#4_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#4_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#4_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#4_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#4_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#4_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#4_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#4_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#4_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#4_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#4_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#4_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#5 odml.scaled_dot_product_attention.impl_2(T#5_0, T#5_1, T#5_2, T#5_3) -> [T#5_33]
  Op#0 TRANSPOSE(T#5_1, T#5_12) -> [T#5_15]
  Op#1 TRANSPOSE(T#5_2, T#5_12) -> [T#5_16]
  Op#2 RESHAPE(T#5_15, T#5_11) -> [T#5_17]
  Op#3 BROADCAST_TO(T#5_17, T#5_13) -> [T#5_18]
  Op#4 RESHAPE(T#5_18, T#5_10) -> [T#5_19]
  Op#5 RESHAPE(T#5_16, T#5_11) -> [T#5_20]
  Op#6 BROADCAST_TO(T#5_20, T#5_13) -> [T#5_21]
  Op#7 MUL(T#5_0, T#5_14) -> [T#5_22]
  Op#8 MUL(T#5_19, T#5_14) -> [T#5_23]
  Op#9 TRANSPOSE(T#5_23, T#5_9) -> [T#5_24]
  Op#10 RESHAPE(T#5_22, T#5_8) -> [T#5_25]
  Op#11 RESHAPE(T#5_24, T#5_7) -> [T#5_26]
  Op#12 BATCH_MATMUL(T#5_25, T#5_26) -> [T#5_27]
  Op#13 ADD(T#5_27, T#5_3) -> [T#5_28]
  Op#14 SOFTMAX(T#5_28) -> [T#5_29]
  Op#15 RESHAPE(T#5_29, T#5_6) -> [T#5_30]
  Op#16 RESHAPE(T#5_21, T#5_5) -> [T#5_31]
  Op#17 BATCH_MATMUL(T#5_30, T#5_31) -> [T#5_32]
  Op#18 RESHAPE(T#5_32, T#5_4) -> [T#5_33]

Tensors of Subgraph#5
  T#5_0(odml.scaled_dot_product_attention.impl_2_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#5_1(odml.scaled_dot_product_attention.impl_2_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#5_2(odml.scaled_dot_product_attention.impl_2_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#5_3(odml.scaled_dot_product_attention.impl_2_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#5_4(arith.constant480) shape:[4], type:INT32
  T#5_5(arith.constant481) shape:[3], type:INT32
  T#5_6(arith.constant482) shape:[3], type:INT32
  T#5_7(arith.constant483) shape:[3], type:INT32
  T#5_8(arith.constant484) shape:[3], type:INT32
  T#5_9(arith.constant485) shape:[4], type:INT32
  T#5_10(arith.constant486) shape:[4], type:INT32
  T#5_11(arith.constant487) shape:[5], type:INT32
  T#5_12(arith.constant488) shape:[4], type:INT32
  T#5_13(arith.constant489) shape:[5], type:INT64
  T#5_14(arith.constant490) shape:[], type:FLOAT32
  T#5_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#5_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#5_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#5_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#5_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#5_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#5_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#5_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#5_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#5_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#5_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#5_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#5_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#5_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#5_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#5_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#5_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#5_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#5_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#6 odml.scaled_dot_product_attention.impl_3(T#6_0, T#6_1, T#6_2, T#6_3) -> [T#6_33]
  Op#0 TRANSPOSE(T#6_1, T#6_12) -> [T#6_15]
  Op#1 TRANSPOSE(T#6_2, T#6_12) -> [T#6_16]
  Op#2 RESHAPE(T#6_15, T#6_11) -> [T#6_17]
  Op#3 BROADCAST_TO(T#6_17, T#6_13) -> [T#6_18]
  Op#4 RESHAPE(T#6_18, T#6_10) -> [T#6_19]
  Op#5 RESHAPE(T#6_16, T#6_11) -> [T#6_20]
  Op#6 BROADCAST_TO(T#6_20, T#6_13) -> [T#6_21]
  Op#7 MUL(T#6_0, T#6_14) -> [T#6_22]
  Op#8 MUL(T#6_19, T#6_14) -> [T#6_23]
  Op#9 TRANSPOSE(T#6_23, T#6_9) -> [T#6_24]
  Op#10 RESHAPE(T#6_22, T#6_8) -> [T#6_25]
  Op#11 RESHAPE(T#6_24, T#6_7) -> [T#6_26]
  Op#12 BATCH_MATMUL(T#6_25, T#6_26) -> [T#6_27]
  Op#13 ADD(T#6_27, T#6_3) -> [T#6_28]
  Op#14 SOFTMAX(T#6_28) -> [T#6_29]
  Op#15 RESHAPE(T#6_29, T#6_6) -> [T#6_30]
  Op#16 RESHAPE(T#6_21, T#6_5) -> [T#6_31]
  Op#17 BATCH_MATMUL(T#6_30, T#6_31) -> [T#6_32]
  Op#18 RESHAPE(T#6_32, T#6_4) -> [T#6_33]

Tensors of Subgraph#6
  T#6_0(odml.scaled_dot_product_attention.impl_3_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#6_1(odml.scaled_dot_product_attention.impl_3_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#6_2(odml.scaled_dot_product_attention.impl_3_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#6_3(odml.scaled_dot_product_attention.impl_3_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#6_4(arith.constant491) shape:[4], type:INT32
  T#6_5(arith.constant492) shape:[3], type:INT32
  T#6_6(arith.constant493) shape:[3], type:INT32
  T#6_7(arith.constant494) shape:[3], type:INT32
  T#6_8(arith.constant495) shape:[3], type:INT32
  T#6_9(arith.constant496) shape:[4], type:INT32
  T#6_10(arith.constant497) shape:[4], type:INT32
  T#6_11(arith.constant498) shape:[5], type:INT32
  T#6_12(arith.constant499) shape:[4], type:INT32
  T#6_13(arith.constant500) shape:[5], type:INT64
  T#6_14(arith.constant501) shape:[], type:FLOAT32
  T#6_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#6_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#6_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#6_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#6_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#6_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#6_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#6_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#6_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#6_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#6_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#6_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#6_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#6_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#6_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#6_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#6_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#6_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#6_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#7 odml.scaled_dot_product_attention.impl_4(T#7_0, T#7_1, T#7_2, T#7_3) -> [T#7_33]
  Op#0 TRANSPOSE(T#7_1, T#7_12) -> [T#7_15]
  Op#1 TRANSPOSE(T#7_2, T#7_12) -> [T#7_16]
  Op#2 RESHAPE(T#7_15, T#7_11) -> [T#7_17]
  Op#3 BROADCAST_TO(T#7_17, T#7_13) -> [T#7_18]
  Op#4 RESHAPE(T#7_18, T#7_10) -> [T#7_19]
  Op#5 RESHAPE(T#7_16, T#7_11) -> [T#7_20]
  Op#6 BROADCAST_TO(T#7_20, T#7_13) -> [T#7_21]
  Op#7 MUL(T#7_0, T#7_14) -> [T#7_22]
  Op#8 MUL(T#7_19, T#7_14) -> [T#7_23]
  Op#9 TRANSPOSE(T#7_23, T#7_9) -> [T#7_24]
  Op#10 RESHAPE(T#7_22, T#7_8) -> [T#7_25]
  Op#11 RESHAPE(T#7_24, T#7_7) -> [T#7_26]
  Op#12 BATCH_MATMUL(T#7_25, T#7_26) -> [T#7_27]
  Op#13 ADD(T#7_27, T#7_3) -> [T#7_28]
  Op#14 SOFTMAX(T#7_28) -> [T#7_29]
  Op#15 RESHAPE(T#7_29, T#7_6) -> [T#7_30]
  Op#16 RESHAPE(T#7_21, T#7_5) -> [T#7_31]
  Op#17 BATCH_MATMUL(T#7_30, T#7_31) -> [T#7_32]
  Op#18 RESHAPE(T#7_32, T#7_4) -> [T#7_33]

Tensors of Subgraph#7
  T#7_0(odml.scaled_dot_product_attention.impl_4_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#7_1(odml.scaled_dot_product_attention.impl_4_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#7_2(odml.scaled_dot_product_attention.impl_4_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#7_3(odml.scaled_dot_product_attention.impl_4_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#7_4(arith.constant502) shape:[4], type:INT32
  T#7_5(arith.constant503) shape:[3], type:INT32
  T#7_6(arith.constant504) shape:[3], type:INT32
  T#7_7(arith.constant505) shape:[3], type:INT32
  T#7_8(arith.constant506) shape:[3], type:INT32
  T#7_9(arith.constant507) shape:[4], type:INT32
  T#7_10(arith.constant508) shape:[4], type:INT32
  T#7_11(arith.constant509) shape:[5], type:INT32
  T#7_12(arith.constant510) shape:[4], type:INT32
  T#7_13(arith.constant511) shape:[5], type:INT64
  T#7_14(arith.constant512) shape:[], type:FLOAT32
  T#7_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#7_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#7_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#7_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#7_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#7_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#7_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#7_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#7_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#7_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#7_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#7_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#7_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#7_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#7_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#7_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#7_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#7_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#7_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#8 odml.scaled_dot_product_attention.impl_5(T#8_0, T#8_1, T#8_2, T#8_3) -> [T#8_33]
  Op#0 TRANSPOSE(T#8_1, T#8_12) -> [T#8_15]
  Op#1 TRANSPOSE(T#8_2, T#8_12) -> [T#8_16]
  Op#2 RESHAPE(T#8_15, T#8_11) -> [T#8_17]
  Op#3 BROADCAST_TO(T#8_17, T#8_13) -> [T#8_18]
  Op#4 RESHAPE(T#8_18, T#8_10) -> [T#8_19]
  Op#5 RESHAPE(T#8_16, T#8_11) -> [T#8_20]
  Op#6 BROADCAST_TO(T#8_20, T#8_13) -> [T#8_21]
  Op#7 MUL(T#8_0, T#8_14) -> [T#8_22]
  Op#8 MUL(T#8_19, T#8_14) -> [T#8_23]
  Op#9 TRANSPOSE(T#8_23, T#8_9) -> [T#8_24]
  Op#10 RESHAPE(T#8_22, T#8_8) -> [T#8_25]
  Op#11 RESHAPE(T#8_24, T#8_7) -> [T#8_26]
  Op#12 BATCH_MATMUL(T#8_25, T#8_26) -> [T#8_27]
  Op#13 ADD(T#8_27, T#8_3) -> [T#8_28]
  Op#14 SOFTMAX(T#8_28) -> [T#8_29]
  Op#15 RESHAPE(T#8_29, T#8_6) -> [T#8_30]
  Op#16 RESHAPE(T#8_21, T#8_5) -> [T#8_31]
  Op#17 BATCH_MATMUL(T#8_30, T#8_31) -> [T#8_32]
  Op#18 RESHAPE(T#8_32, T#8_4) -> [T#8_33]

Tensors of Subgraph#8
  T#8_0(odml.scaled_dot_product_attention.impl_5_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#8_1(odml.scaled_dot_product_attention.impl_5_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#8_2(odml.scaled_dot_product_attention.impl_5_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#8_3(odml.scaled_dot_product_attention.impl_5_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#8_4(arith.constant513) shape:[4], type:INT32
  T#8_5(arith.constant514) shape:[3], type:INT32
  T#8_6(arith.constant515) shape:[3], type:INT32
  T#8_7(arith.constant516) shape:[3], type:INT32
  T#8_8(arith.constant517) shape:[3], type:INT32
  T#8_9(arith.constant518) shape:[4], type:INT32
  T#8_10(arith.constant519) shape:[4], type:INT32
  T#8_11(arith.constant520) shape:[5], type:INT32
  T#8_12(arith.constant521) shape:[4], type:INT32
  T#8_13(arith.constant522) shape:[5], type:INT64
  T#8_14(arith.constant523) shape:[], type:FLOAT32
  T#8_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#8_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#8_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#8_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#8_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#8_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#8_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#8_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#8_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#8_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#8_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#8_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#8_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#8_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#8_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#8_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#8_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#8_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#8_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#9 odml.scaled_dot_product_attention.impl_6(T#9_0, T#9_1, T#9_2, T#9_3) -> [T#9_33]
  Op#0 TRANSPOSE(T#9_1, T#9_12) -> [T#9_15]
  Op#1 TRANSPOSE(T#9_2, T#9_12) -> [T#9_16]
  Op#2 RESHAPE(T#9_15, T#9_11) -> [T#9_17]
  Op#3 BROADCAST_TO(T#9_17, T#9_13) -> [T#9_18]
  Op#4 RESHAPE(T#9_18, T#9_10) -> [T#9_19]
  Op#5 RESHAPE(T#9_16, T#9_11) -> [T#9_20]
  Op#6 BROADCAST_TO(T#9_20, T#9_13) -> [T#9_21]
  Op#7 MUL(T#9_0, T#9_14) -> [T#9_22]
  Op#8 MUL(T#9_19, T#9_14) -> [T#9_23]
  Op#9 TRANSPOSE(T#9_23, T#9_9) -> [T#9_24]
  Op#10 RESHAPE(T#9_22, T#9_8) -> [T#9_25]
  Op#11 RESHAPE(T#9_24, T#9_7) -> [T#9_26]
  Op#12 BATCH_MATMUL(T#9_25, T#9_26) -> [T#9_27]
  Op#13 ADD(T#9_27, T#9_3) -> [T#9_28]
  Op#14 SOFTMAX(T#9_28) -> [T#9_29]
  Op#15 RESHAPE(T#9_29, T#9_6) -> [T#9_30]
  Op#16 RESHAPE(T#9_21, T#9_5) -> [T#9_31]
  Op#17 BATCH_MATMUL(T#9_30, T#9_31) -> [T#9_32]
  Op#18 RESHAPE(T#9_32, T#9_4) -> [T#9_33]

Tensors of Subgraph#9
  T#9_0(odml.scaled_dot_product_attention.impl_6_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#9_1(odml.scaled_dot_product_attention.impl_6_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#9_2(odml.scaled_dot_product_attention.impl_6_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#9_3(odml.scaled_dot_product_attention.impl_6_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#9_4(arith.constant524) shape:[4], type:INT32
  T#9_5(arith.constant525) shape:[3], type:INT32
  T#9_6(arith.constant526) shape:[3], type:INT32
  T#9_7(arith.constant527) shape:[3], type:INT32
  T#9_8(arith.constant528) shape:[3], type:INT32
  T#9_9(arith.constant529) shape:[4], type:INT32
  T#9_10(arith.constant530) shape:[4], type:INT32
  T#9_11(arith.constant531) shape:[5], type:INT32
  T#9_12(arith.constant532) shape:[4], type:INT32
  T#9_13(arith.constant533) shape:[5], type:INT64
  T#9_14(arith.constant534) shape:[], type:FLOAT32
  T#9_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#9_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#9_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#9_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#9_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#9_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#9_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#9_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#9_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#9_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#9_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#9_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#9_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#9_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#9_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#9_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#9_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#9_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#9_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#10 odml.scaled_dot_product_attention.impl_7(T#10_0, T#10_1, T#10_2, T#10_3) -> [T#10_33]
  Op#0 TRANSPOSE(T#10_1, T#10_12) -> [T#10_15]
  Op#1 TRANSPOSE(T#10_2, T#10_12) -> [T#10_16]
  Op#2 RESHAPE(T#10_15, T#10_11) -> [T#10_17]
  Op#3 BROADCAST_TO(T#10_17, T#10_13) -> [T#10_18]
  Op#4 RESHAPE(T#10_18, T#10_10) -> [T#10_19]
  Op#5 RESHAPE(T#10_16, T#10_11) -> [T#10_20]
  Op#6 BROADCAST_TO(T#10_20, T#10_13) -> [T#10_21]
  Op#7 MUL(T#10_0, T#10_14) -> [T#10_22]
  Op#8 MUL(T#10_19, T#10_14) -> [T#10_23]
  Op#9 TRANSPOSE(T#10_23, T#10_9) -> [T#10_24]
  Op#10 RESHAPE(T#10_22, T#10_8) -> [T#10_25]
  Op#11 RESHAPE(T#10_24, T#10_7) -> [T#10_26]
  Op#12 BATCH_MATMUL(T#10_25, T#10_26) -> [T#10_27]
  Op#13 ADD(T#10_27, T#10_3) -> [T#10_28]
  Op#14 SOFTMAX(T#10_28) -> [T#10_29]
  Op#15 RESHAPE(T#10_29, T#10_6) -> [T#10_30]
  Op#16 RESHAPE(T#10_21, T#10_5) -> [T#10_31]
  Op#17 BATCH_MATMUL(T#10_30, T#10_31) -> [T#10_32]
  Op#18 RESHAPE(T#10_32, T#10_4) -> [T#10_33]

Tensors of Subgraph#10
  T#10_0(odml.scaled_dot_product_attention.impl_7_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#10_1(odml.scaled_dot_product_attention.impl_7_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#10_2(odml.scaled_dot_product_attention.impl_7_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#10_3(odml.scaled_dot_product_attention.impl_7_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#10_4(arith.constant535) shape:[4], type:INT32
  T#10_5(arith.constant536) shape:[3], type:INT32
  T#10_6(arith.constant537) shape:[3], type:INT32
  T#10_7(arith.constant538) shape:[3], type:INT32
  T#10_8(arith.constant539) shape:[3], type:INT32
  T#10_9(arith.constant540) shape:[4], type:INT32
  T#10_10(arith.constant541) shape:[4], type:INT32
  T#10_11(arith.constant542) shape:[5], type:INT32
  T#10_12(arith.constant543) shape:[4], type:INT32
  T#10_13(arith.constant544) shape:[5], type:INT64
  T#10_14(arith.constant545) shape:[], type:FLOAT32
  T#10_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#10_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#10_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#10_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#10_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#10_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#10_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#10_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#10_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#10_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#10_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#10_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#10_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#10_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#10_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#10_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#10_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#10_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#10_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#11 odml.scaled_dot_product_attention.impl_8(T#11_0, T#11_1, T#11_2, T#11_3) -> [T#11_33]
  Op#0 TRANSPOSE(T#11_1, T#11_12) -> [T#11_15]
  Op#1 TRANSPOSE(T#11_2, T#11_12) -> [T#11_16]
  Op#2 RESHAPE(T#11_15, T#11_11) -> [T#11_17]
  Op#3 BROADCAST_TO(T#11_17, T#11_13) -> [T#11_18]
  Op#4 RESHAPE(T#11_18, T#11_10) -> [T#11_19]
  Op#5 RESHAPE(T#11_16, T#11_11) -> [T#11_20]
  Op#6 BROADCAST_TO(T#11_20, T#11_13) -> [T#11_21]
  Op#7 MUL(T#11_0, T#11_14) -> [T#11_22]
  Op#8 MUL(T#11_19, T#11_14) -> [T#11_23]
  Op#9 TRANSPOSE(T#11_23, T#11_9) -> [T#11_24]
  Op#10 RESHAPE(T#11_22, T#11_8) -> [T#11_25]
  Op#11 RESHAPE(T#11_24, T#11_7) -> [T#11_26]
  Op#12 BATCH_MATMUL(T#11_25, T#11_26) -> [T#11_27]
  Op#13 ADD(T#11_27, T#11_3) -> [T#11_28]
  Op#14 SOFTMAX(T#11_28) -> [T#11_29]
  Op#15 RESHAPE(T#11_29, T#11_6) -> [T#11_30]
  Op#16 RESHAPE(T#11_21, T#11_5) -> [T#11_31]
  Op#17 BATCH_MATMUL(T#11_30, T#11_31) -> [T#11_32]
  Op#18 RESHAPE(T#11_32, T#11_4) -> [T#11_33]

Tensors of Subgraph#11
  T#11_0(odml.scaled_dot_product_attention.impl_8_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#11_1(odml.scaled_dot_product_attention.impl_8_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#11_2(odml.scaled_dot_product_attention.impl_8_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#11_3(odml.scaled_dot_product_attention.impl_8_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#11_4(arith.constant546) shape:[4], type:INT32
  T#11_5(arith.constant547) shape:[3], type:INT32
  T#11_6(arith.constant548) shape:[3], type:INT32
  T#11_7(arith.constant549) shape:[3], type:INT32
  T#11_8(arith.constant550) shape:[3], type:INT32
  T#11_9(arith.constant551) shape:[4], type:INT32
  T#11_10(arith.constant552) shape:[4], type:INT32
  T#11_11(arith.constant553) shape:[5], type:INT32
  T#11_12(arith.constant554) shape:[4], type:INT32
  T#11_13(arith.constant555) shape:[5], type:INT64
  T#11_14(arith.constant556) shape:[], type:FLOAT32
  T#11_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#11_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#11_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#11_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#11_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#11_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#11_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#11_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#11_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#11_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#11_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#11_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#11_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#11_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#11_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#11_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#11_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#11_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#11_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#12 odml.scaled_dot_product_attention.impl_9(T#12_0, T#12_1, T#12_2, T#12_3) -> [T#12_33]
  Op#0 TRANSPOSE(T#12_1, T#12_12) -> [T#12_15]
  Op#1 TRANSPOSE(T#12_2, T#12_12) -> [T#12_16]
  Op#2 RESHAPE(T#12_15, T#12_11) -> [T#12_17]
  Op#3 BROADCAST_TO(T#12_17, T#12_13) -> [T#12_18]
  Op#4 RESHAPE(T#12_18, T#12_10) -> [T#12_19]
  Op#5 RESHAPE(T#12_16, T#12_11) -> [T#12_20]
  Op#6 BROADCAST_TO(T#12_20, T#12_13) -> [T#12_21]
  Op#7 MUL(T#12_0, T#12_14) -> [T#12_22]
  Op#8 MUL(T#12_19, T#12_14) -> [T#12_23]
  Op#9 TRANSPOSE(T#12_23, T#12_9) -> [T#12_24]
  Op#10 RESHAPE(T#12_22, T#12_8) -> [T#12_25]
  Op#11 RESHAPE(T#12_24, T#12_7) -> [T#12_26]
  Op#12 BATCH_MATMUL(T#12_25, T#12_26) -> [T#12_27]
  Op#13 ADD(T#12_27, T#12_3) -> [T#12_28]
  Op#14 SOFTMAX(T#12_28) -> [T#12_29]
  Op#15 RESHAPE(T#12_29, T#12_6) -> [T#12_30]
  Op#16 RESHAPE(T#12_21, T#12_5) -> [T#12_31]
  Op#17 BATCH_MATMUL(T#12_30, T#12_31) -> [T#12_32]
  Op#18 RESHAPE(T#12_32, T#12_4) -> [T#12_33]

Tensors of Subgraph#12
  T#12_0(odml.scaled_dot_product_attention.impl_9_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#12_1(odml.scaled_dot_product_attention.impl_9_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#12_2(odml.scaled_dot_product_attention.impl_9_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#12_3(odml.scaled_dot_product_attention.impl_9_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#12_4(arith.constant557) shape:[4], type:INT32
  T#12_5(arith.constant558) shape:[3], type:INT32
  T#12_6(arith.constant559) shape:[3], type:INT32
  T#12_7(arith.constant560) shape:[3], type:INT32
  T#12_8(arith.constant561) shape:[3], type:INT32
  T#12_9(arith.constant562) shape:[4], type:INT32
  T#12_10(arith.constant563) shape:[4], type:INT32
  T#12_11(arith.constant564) shape:[5], type:INT32
  T#12_12(arith.constant565) shape:[4], type:INT32
  T#12_13(arith.constant566) shape:[5], type:INT64
  T#12_14(arith.constant567) shape:[], type:FLOAT32
  T#12_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#12_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#12_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#12_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#12_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#12_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#12_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#12_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#12_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#12_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#12_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#12_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#12_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#12_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#12_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#12_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#12_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#12_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#12_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#13 odml.scaled_dot_product_attention.impl_10(T#13_0, T#13_1, T#13_2, T#13_3) -> [T#13_33]
  Op#0 TRANSPOSE(T#13_1, T#13_12) -> [T#13_15]
  Op#1 TRANSPOSE(T#13_2, T#13_12) -> [T#13_16]
  Op#2 RESHAPE(T#13_15, T#13_11) -> [T#13_17]
  Op#3 BROADCAST_TO(T#13_17, T#13_13) -> [T#13_18]
  Op#4 RESHAPE(T#13_18, T#13_10) -> [T#13_19]
  Op#5 RESHAPE(T#13_16, T#13_11) -> [T#13_20]
  Op#6 BROADCAST_TO(T#13_20, T#13_13) -> [T#13_21]
  Op#7 MUL(T#13_0, T#13_14) -> [T#13_22]
  Op#8 MUL(T#13_19, T#13_14) -> [T#13_23]
  Op#9 TRANSPOSE(T#13_23, T#13_9) -> [T#13_24]
  Op#10 RESHAPE(T#13_22, T#13_8) -> [T#13_25]
  Op#11 RESHAPE(T#13_24, T#13_7) -> [T#13_26]
  Op#12 BATCH_MATMUL(T#13_25, T#13_26) -> [T#13_27]
  Op#13 ADD(T#13_27, T#13_3) -> [T#13_28]
  Op#14 SOFTMAX(T#13_28) -> [T#13_29]
  Op#15 RESHAPE(T#13_29, T#13_6) -> [T#13_30]
  Op#16 RESHAPE(T#13_21, T#13_5) -> [T#13_31]
  Op#17 BATCH_MATMUL(T#13_30, T#13_31) -> [T#13_32]
  Op#18 RESHAPE(T#13_32, T#13_4) -> [T#13_33]

Tensors of Subgraph#13
  T#13_0(odml.scaled_dot_product_attention.impl_10_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#13_1(odml.scaled_dot_product_attention.impl_10_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#13_2(odml.scaled_dot_product_attention.impl_10_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#13_3(odml.scaled_dot_product_attention.impl_10_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#13_4(arith.constant568) shape:[4], type:INT32
  T#13_5(arith.constant569) shape:[3], type:INT32
  T#13_6(arith.constant570) shape:[3], type:INT32
  T#13_7(arith.constant571) shape:[3], type:INT32
  T#13_8(arith.constant572) shape:[3], type:INT32
  T#13_9(arith.constant573) shape:[4], type:INT32
  T#13_10(arith.constant574) shape:[4], type:INT32
  T#13_11(arith.constant575) shape:[5], type:INT32
  T#13_12(arith.constant576) shape:[4], type:INT32
  T#13_13(arith.constant577) shape:[5], type:INT64
  T#13_14(arith.constant578) shape:[], type:FLOAT32
  T#13_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#13_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#13_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#13_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#13_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#13_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#13_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#13_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#13_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#13_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#13_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#13_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#13_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#13_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#13_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#13_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#13_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#13_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#13_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#14 odml.scaled_dot_product_attention.impl_11(T#14_0, T#14_1, T#14_2, T#14_3) -> [T#14_33]
  Op#0 TRANSPOSE(T#14_1, T#14_12) -> [T#14_15]
  Op#1 TRANSPOSE(T#14_2, T#14_12) -> [T#14_16]
  Op#2 RESHAPE(T#14_15, T#14_11) -> [T#14_17]
  Op#3 BROADCAST_TO(T#14_17, T#14_13) -> [T#14_18]
  Op#4 RESHAPE(T#14_18, T#14_10) -> [T#14_19]
  Op#5 RESHAPE(T#14_16, T#14_11) -> [T#14_20]
  Op#6 BROADCAST_TO(T#14_20, T#14_13) -> [T#14_21]
  Op#7 MUL(T#14_0, T#14_14) -> [T#14_22]
  Op#8 MUL(T#14_19, T#14_14) -> [T#14_23]
  Op#9 TRANSPOSE(T#14_23, T#14_9) -> [T#14_24]
  Op#10 RESHAPE(T#14_22, T#14_8) -> [T#14_25]
  Op#11 RESHAPE(T#14_24, T#14_7) -> [T#14_26]
  Op#12 BATCH_MATMUL(T#14_25, T#14_26) -> [T#14_27]
  Op#13 ADD(T#14_27, T#14_3) -> [T#14_28]
  Op#14 SOFTMAX(T#14_28) -> [T#14_29]
  Op#15 RESHAPE(T#14_29, T#14_6) -> [T#14_30]
  Op#16 RESHAPE(T#14_21, T#14_5) -> [T#14_31]
  Op#17 BATCH_MATMUL(T#14_30, T#14_31) -> [T#14_32]
  Op#18 RESHAPE(T#14_32, T#14_4) -> [T#14_33]

Tensors of Subgraph#14
  T#14_0(odml.scaled_dot_product_attention.impl_11_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#14_1(odml.scaled_dot_product_attention.impl_11_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#14_2(odml.scaled_dot_product_attention.impl_11_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#14_3(odml.scaled_dot_product_attention.impl_11_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#14_4(arith.constant579) shape:[4], type:INT32
  T#14_5(arith.constant580) shape:[3], type:INT32
  T#14_6(arith.constant581) shape:[3], type:INT32
  T#14_7(arith.constant582) shape:[3], type:INT32
  T#14_8(arith.constant583) shape:[3], type:INT32
  T#14_9(arith.constant584) shape:[4], type:INT32
  T#14_10(arith.constant585) shape:[4], type:INT32
  T#14_11(arith.constant586) shape:[5], type:INT32
  T#14_12(arith.constant587) shape:[4], type:INT32
  T#14_13(arith.constant588) shape:[5], type:INT64
  T#14_14(arith.constant589) shape:[], type:FLOAT32
  T#14_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#14_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#14_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#14_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#14_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#14_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#14_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#14_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#14_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#14_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#14_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#14_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#14_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#14_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#14_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#14_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#14_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#14_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#14_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#15 odml.scaled_dot_product_attention.impl_12(T#15_0, T#15_1, T#15_2, T#15_3) -> [T#15_33]
  Op#0 TRANSPOSE(T#15_1, T#15_12) -> [T#15_15]
  Op#1 TRANSPOSE(T#15_2, T#15_12) -> [T#15_16]
  Op#2 RESHAPE(T#15_15, T#15_11) -> [T#15_17]
  Op#3 BROADCAST_TO(T#15_17, T#15_13) -> [T#15_18]
  Op#4 RESHAPE(T#15_18, T#15_10) -> [T#15_19]
  Op#5 RESHAPE(T#15_16, T#15_11) -> [T#15_20]
  Op#6 BROADCAST_TO(T#15_20, T#15_13) -> [T#15_21]
  Op#7 MUL(T#15_0, T#15_14) -> [T#15_22]
  Op#8 MUL(T#15_19, T#15_14) -> [T#15_23]
  Op#9 TRANSPOSE(T#15_23, T#15_9) -> [T#15_24]
  Op#10 RESHAPE(T#15_22, T#15_8) -> [T#15_25]
  Op#11 RESHAPE(T#15_24, T#15_7) -> [T#15_26]
  Op#12 BATCH_MATMUL(T#15_25, T#15_26) -> [T#15_27]
  Op#13 ADD(T#15_27, T#15_3) -> [T#15_28]
  Op#14 SOFTMAX(T#15_28) -> [T#15_29]
  Op#15 RESHAPE(T#15_29, T#15_6) -> [T#15_30]
  Op#16 RESHAPE(T#15_21, T#15_5) -> [T#15_31]
  Op#17 BATCH_MATMUL(T#15_30, T#15_31) -> [T#15_32]
  Op#18 RESHAPE(T#15_32, T#15_4) -> [T#15_33]

Tensors of Subgraph#15
  T#15_0(odml.scaled_dot_product_attention.impl_12_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#15_1(odml.scaled_dot_product_attention.impl_12_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#15_2(odml.scaled_dot_product_attention.impl_12_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#15_3(odml.scaled_dot_product_attention.impl_12_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#15_4(arith.constant590) shape:[4], type:INT32
  T#15_5(arith.constant591) shape:[3], type:INT32
  T#15_6(arith.constant592) shape:[3], type:INT32
  T#15_7(arith.constant593) shape:[3], type:INT32
  T#15_8(arith.constant594) shape:[3], type:INT32
  T#15_9(arith.constant595) shape:[4], type:INT32
  T#15_10(arith.constant596) shape:[4], type:INT32
  T#15_11(arith.constant597) shape:[5], type:INT32
  T#15_12(arith.constant598) shape:[4], type:INT32
  T#15_13(arith.constant599) shape:[5], type:INT64
  T#15_14(arith.constant600) shape:[], type:FLOAT32
  T#15_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#15_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#15_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#15_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#15_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#15_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#15_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#15_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#15_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#15_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#15_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#15_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#15_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#15_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#15_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#15_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#15_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#15_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#15_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#16 odml.scaled_dot_product_attention.impl_13(T#16_0, T#16_1, T#16_2, T#16_3) -> [T#16_33]
  Op#0 TRANSPOSE(T#16_1, T#16_12) -> [T#16_15]
  Op#1 TRANSPOSE(T#16_2, T#16_12) -> [T#16_16]
  Op#2 RESHAPE(T#16_15, T#16_11) -> [T#16_17]
  Op#3 BROADCAST_TO(T#16_17, T#16_13) -> [T#16_18]
  Op#4 RESHAPE(T#16_18, T#16_10) -> [T#16_19]
  Op#5 RESHAPE(T#16_16, T#16_11) -> [T#16_20]
  Op#6 BROADCAST_TO(T#16_20, T#16_13) -> [T#16_21]
  Op#7 MUL(T#16_0, T#16_14) -> [T#16_22]
  Op#8 MUL(T#16_19, T#16_14) -> [T#16_23]
  Op#9 TRANSPOSE(T#16_23, T#16_9) -> [T#16_24]
  Op#10 RESHAPE(T#16_22, T#16_8) -> [T#16_25]
  Op#11 RESHAPE(T#16_24, T#16_7) -> [T#16_26]
  Op#12 BATCH_MATMUL(T#16_25, T#16_26) -> [T#16_27]
  Op#13 ADD(T#16_27, T#16_3) -> [T#16_28]
  Op#14 SOFTMAX(T#16_28) -> [T#16_29]
  Op#15 RESHAPE(T#16_29, T#16_6) -> [T#16_30]
  Op#16 RESHAPE(T#16_21, T#16_5) -> [T#16_31]
  Op#17 BATCH_MATMUL(T#16_30, T#16_31) -> [T#16_32]
  Op#18 RESHAPE(T#16_32, T#16_4) -> [T#16_33]

Tensors of Subgraph#16
  T#16_0(odml.scaled_dot_product_attention.impl_13_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#16_1(odml.scaled_dot_product_attention.impl_13_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#16_2(odml.scaled_dot_product_attention.impl_13_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#16_3(odml.scaled_dot_product_attention.impl_13_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#16_4(arith.constant601) shape:[4], type:INT32
  T#16_5(arith.constant602) shape:[3], type:INT32
  T#16_6(arith.constant603) shape:[3], type:INT32
  T#16_7(arith.constant604) shape:[3], type:INT32
  T#16_8(arith.constant605) shape:[3], type:INT32
  T#16_9(arith.constant606) shape:[4], type:INT32
  T#16_10(arith.constant607) shape:[4], type:INT32
  T#16_11(arith.constant608) shape:[5], type:INT32
  T#16_12(arith.constant609) shape:[4], type:INT32
  T#16_13(arith.constant610) shape:[5], type:INT64
  T#16_14(arith.constant611) shape:[], type:FLOAT32
  T#16_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#16_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#16_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#16_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#16_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#16_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#16_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#16_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#16_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#16_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#16_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#16_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#16_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#16_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#16_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#16_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#16_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#16_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#16_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#17 odml.scaled_dot_product_attention.impl_14(T#17_0, T#17_1, T#17_2, T#17_3) -> [T#17_33]
  Op#0 TRANSPOSE(T#17_1, T#17_12) -> [T#17_15]
  Op#1 TRANSPOSE(T#17_2, T#17_12) -> [T#17_16]
  Op#2 RESHAPE(T#17_15, T#17_11) -> [T#17_17]
  Op#3 BROADCAST_TO(T#17_17, T#17_13) -> [T#17_18]
  Op#4 RESHAPE(T#17_18, T#17_10) -> [T#17_19]
  Op#5 RESHAPE(T#17_16, T#17_11) -> [T#17_20]
  Op#6 BROADCAST_TO(T#17_20, T#17_13) -> [T#17_21]
  Op#7 MUL(T#17_0, T#17_14) -> [T#17_22]
  Op#8 MUL(T#17_19, T#17_14) -> [T#17_23]
  Op#9 TRANSPOSE(T#17_23, T#17_9) -> [T#17_24]
  Op#10 RESHAPE(T#17_22, T#17_8) -> [T#17_25]
  Op#11 RESHAPE(T#17_24, T#17_7) -> [T#17_26]
  Op#12 BATCH_MATMUL(T#17_25, T#17_26) -> [T#17_27]
  Op#13 ADD(T#17_27, T#17_3) -> [T#17_28]
  Op#14 SOFTMAX(T#17_28) -> [T#17_29]
  Op#15 RESHAPE(T#17_29, T#17_6) -> [T#17_30]
  Op#16 RESHAPE(T#17_21, T#17_5) -> [T#17_31]
  Op#17 BATCH_MATMUL(T#17_30, T#17_31) -> [T#17_32]
  Op#18 RESHAPE(T#17_32, T#17_4) -> [T#17_33]

Tensors of Subgraph#17
  T#17_0(odml.scaled_dot_product_attention.impl_14_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#17_1(odml.scaled_dot_product_attention.impl_14_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#17_2(odml.scaled_dot_product_attention.impl_14_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#17_3(odml.scaled_dot_product_attention.impl_14_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#17_4(arith.constant612) shape:[4], type:INT32
  T#17_5(arith.constant613) shape:[3], type:INT32
  T#17_6(arith.constant614) shape:[3], type:INT32
  T#17_7(arith.constant615) shape:[3], type:INT32
  T#17_8(arith.constant616) shape:[3], type:INT32
  T#17_9(arith.constant617) shape:[4], type:INT32
  T#17_10(arith.constant618) shape:[4], type:INT32
  T#17_11(arith.constant619) shape:[5], type:INT32
  T#17_12(arith.constant620) shape:[4], type:INT32
  T#17_13(arith.constant621) shape:[5], type:INT64
  T#17_14(arith.constant622) shape:[], type:FLOAT32
  T#17_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#17_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#17_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#17_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#17_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#17_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#17_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#17_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1, 24, 128], type:FLOAT32
  T#17_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1280, 128], type:FLOAT32
  T#17_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 128, 1280], type:FLOAT32
  T#17_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[24, 1, 128], type:FLOAT32
  T#17_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 128, 1280], type:FLOAT32
  T#17_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 1, 1280], type:FLOAT32
  T#17_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[1, 24, 1, 1280], type:FLOAT32
  T#17_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1, 1280], type:FLOAT32
  T#17_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[24, 1, 1280], type:FLOAT32
  T#17_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1280, 128], type:FLOAT32
  T#17_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1, 128], type:FLOAT32
  T#17_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#18 odml.scaled_dot_product_attention.impl_15(T#18_0, T#18_1, T#18_2, T#18_3) -> [T#18_33]
  Op#0 TRANSPOSE(T#18_1, T#18_12) -> [T#18_15]
  Op#1 TRANSPOSE(T#18_2, T#18_12) -> [T#18_16]
  Op#2 RESHAPE(T#18_15, T#18_11) -> [T#18_17]
  Op#3 BROADCAST_TO(T#18_17, T#18_13) -> [T#18_18]
  Op#4 RESHAPE(T#18_18, T#18_10) -> [T#18_19]
  Op#5 RESHAPE(T#18_16, T#18_11) -> [T#18_20]
  Op#6 BROADCAST_TO(T#18_20, T#18_13) -> [T#18_21]
  Op#7 MUL(T#18_0, T#18_14) -> [T#18_22]
  Op#8 MUL(T#18_19, T#18_14) -> [T#18_23]
  Op#9 TRANSPOSE(T#18_23, T#18_9) -> [T#18_24]
  Op#10 RESHAPE(T#18_22, T#18_8) -> [T#18_25]
  Op#11 RESHAPE(T#18_24, T#18_7) -> [T#18_26]
  Op#12 BATCH_MATMUL(T#18_25, T#18_26) -> [T#18_27]
  Op#13 ADD(T#18_27, T#18_3) -> [T#18_28]
  Op#14 SOFTMAX(T#18_28) -> [T#18_29]
  Op#15 RESHAPE(T#18_29, T#18_6) -> [T#18_30]
  Op#16 RESHAPE(T#18_21, T#18_5) -> [T#18_31]
  Op#17 BATCH_MATMUL(T#18_30, T#18_31) -> [T#18_32]
  Op#18 RESHAPE(T#18_32, T#18_4) -> [T#18_33]

Tensors of Subgraph#18
  T#18_0(odml.scaled_dot_product_attention.impl_15_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#18_1(odml.scaled_dot_product_attention.impl_15_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#18_2(odml.scaled_dot_product_attention.impl_15_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#18_3(odml.scaled_dot_product_attention.impl_15_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#18_4(arith.constant623) shape:[4], type:INT32
  T#18_5(arith.constant624) shape:[3], type:INT32
  T#18_6(arith.constant625) shape:[3], type:INT32
  T#18_7(arith.constant626) shape:[3], type:INT32
  T#18_8(arith.constant627) shape:[3], type:INT32
  T#18_9(arith.constant628) shape:[4], type:INT32
  T#18_10(arith.constant629) shape:[4], type:INT32
  T#18_11(arith.constant630) shape:[5], type:INT32
  T#18_12(arith.constant631) shape:[4], type:INT32
  T#18_13(arith.constant632) shape:[5], type:INT64
  T#18_14(arith.constant633) shape:[], type:FLOAT32
  T#18_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#18_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#18_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#18_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#18_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#18_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#18_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#18_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#18_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#18_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#18_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#18_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#18_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#18_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#18_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#18_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#18_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#18_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#18_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#19 odml.scaled_dot_product_attention.impl_16(T#19_0, T#19_1, T#19_2, T#19_3) -> [T#19_33]
  Op#0 TRANSPOSE(T#19_1, T#19_12) -> [T#19_15]
  Op#1 TRANSPOSE(T#19_2, T#19_12) -> [T#19_16]
  Op#2 RESHAPE(T#19_15, T#19_11) -> [T#19_17]
  Op#3 BROADCAST_TO(T#19_17, T#19_13) -> [T#19_18]
  Op#4 RESHAPE(T#19_18, T#19_10) -> [T#19_19]
  Op#5 RESHAPE(T#19_16, T#19_11) -> [T#19_20]
  Op#6 BROADCAST_TO(T#19_20, T#19_13) -> [T#19_21]
  Op#7 MUL(T#19_0, T#19_14) -> [T#19_22]
  Op#8 MUL(T#19_19, T#19_14) -> [T#19_23]
  Op#9 TRANSPOSE(T#19_23, T#19_9) -> [T#19_24]
  Op#10 RESHAPE(T#19_22, T#19_8) -> [T#19_25]
  Op#11 RESHAPE(T#19_24, T#19_7) -> [T#19_26]
  Op#12 BATCH_MATMUL(T#19_25, T#19_26) -> [T#19_27]
  Op#13 ADD(T#19_27, T#19_3) -> [T#19_28]
  Op#14 SOFTMAX(T#19_28) -> [T#19_29]
  Op#15 RESHAPE(T#19_29, T#19_6) -> [T#19_30]
  Op#16 RESHAPE(T#19_21, T#19_5) -> [T#19_31]
  Op#17 BATCH_MATMUL(T#19_30, T#19_31) -> [T#19_32]
  Op#18 RESHAPE(T#19_32, T#19_4) -> [T#19_33]

Tensors of Subgraph#19
  T#19_0(odml.scaled_dot_product_attention.impl_16_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#19_1(odml.scaled_dot_product_attention.impl_16_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#19_2(odml.scaled_dot_product_attention.impl_16_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#19_3(odml.scaled_dot_product_attention.impl_16_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#19_4(arith.constant634) shape:[4], type:INT32
  T#19_5(arith.constant635) shape:[3], type:INT32
  T#19_6(arith.constant636) shape:[3], type:INT32
  T#19_7(arith.constant637) shape:[3], type:INT32
  T#19_8(arith.constant638) shape:[3], type:INT32
  T#19_9(arith.constant639) shape:[4], type:INT32
  T#19_10(arith.constant640) shape:[4], type:INT32
  T#19_11(arith.constant641) shape:[5], type:INT32
  T#19_12(arith.constant642) shape:[4], type:INT32
  T#19_13(arith.constant643) shape:[5], type:INT64
  T#19_14(arith.constant644) shape:[], type:FLOAT32
  T#19_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#19_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#19_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#19_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#19_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#19_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#19_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#19_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#19_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#19_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#19_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#19_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#19_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#19_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#19_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#19_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#19_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#19_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#19_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#20 odml.scaled_dot_product_attention.impl_17(T#20_0, T#20_1, T#20_2, T#20_3) -> [T#20_33]
  Op#0 TRANSPOSE(T#20_1, T#20_12) -> [T#20_15]
  Op#1 TRANSPOSE(T#20_2, T#20_12) -> [T#20_16]
  Op#2 RESHAPE(T#20_15, T#20_11) -> [T#20_17]
  Op#3 BROADCAST_TO(T#20_17, T#20_13) -> [T#20_18]
  Op#4 RESHAPE(T#20_18, T#20_10) -> [T#20_19]
  Op#5 RESHAPE(T#20_16, T#20_11) -> [T#20_20]
  Op#6 BROADCAST_TO(T#20_20, T#20_13) -> [T#20_21]
  Op#7 MUL(T#20_0, T#20_14) -> [T#20_22]
  Op#8 MUL(T#20_19, T#20_14) -> [T#20_23]
  Op#9 TRANSPOSE(T#20_23, T#20_9) -> [T#20_24]
  Op#10 RESHAPE(T#20_22, T#20_8) -> [T#20_25]
  Op#11 RESHAPE(T#20_24, T#20_7) -> [T#20_26]
  Op#12 BATCH_MATMUL(T#20_25, T#20_26) -> [T#20_27]
  Op#13 ADD(T#20_27, T#20_3) -> [T#20_28]
  Op#14 SOFTMAX(T#20_28) -> [T#20_29]
  Op#15 RESHAPE(T#20_29, T#20_6) -> [T#20_30]
  Op#16 RESHAPE(T#20_21, T#20_5) -> [T#20_31]
  Op#17 BATCH_MATMUL(T#20_30, T#20_31) -> [T#20_32]
  Op#18 RESHAPE(T#20_32, T#20_4) -> [T#20_33]

Tensors of Subgraph#20
  T#20_0(odml.scaled_dot_product_attention.impl_17_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#20_1(odml.scaled_dot_product_attention.impl_17_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#20_2(odml.scaled_dot_product_attention.impl_17_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#20_3(odml.scaled_dot_product_attention.impl_17_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#20_4(arith.constant645) shape:[4], type:INT32
  T#20_5(arith.constant646) shape:[3], type:INT32
  T#20_6(arith.constant647) shape:[3], type:INT32
  T#20_7(arith.constant648) shape:[3], type:INT32
  T#20_8(arith.constant649) shape:[3], type:INT32
  T#20_9(arith.constant650) shape:[4], type:INT32
  T#20_10(arith.constant651) shape:[4], type:INT32
  T#20_11(arith.constant652) shape:[5], type:INT32
  T#20_12(arith.constant653) shape:[4], type:INT32
  T#20_13(arith.constant654) shape:[5], type:INT64
  T#20_14(arith.constant655) shape:[], type:FLOAT32
  T#20_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#20_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#20_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#20_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#20_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#20_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#20_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#20_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#20_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#20_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#20_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#20_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#20_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#20_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#20_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#20_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#20_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#20_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#20_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#21 odml.scaled_dot_product_attention.impl_18(T#21_0, T#21_1, T#21_2, T#21_3) -> [T#21_33]
  Op#0 TRANSPOSE(T#21_1, T#21_12) -> [T#21_15]
  Op#1 TRANSPOSE(T#21_2, T#21_12) -> [T#21_16]
  Op#2 RESHAPE(T#21_15, T#21_11) -> [T#21_17]
  Op#3 BROADCAST_TO(T#21_17, T#21_13) -> [T#21_18]
  Op#4 RESHAPE(T#21_18, T#21_10) -> [T#21_19]
  Op#5 RESHAPE(T#21_16, T#21_11) -> [T#21_20]
  Op#6 BROADCAST_TO(T#21_20, T#21_13) -> [T#21_21]
  Op#7 MUL(T#21_0, T#21_14) -> [T#21_22]
  Op#8 MUL(T#21_19, T#21_14) -> [T#21_23]
  Op#9 TRANSPOSE(T#21_23, T#21_9) -> [T#21_24]
  Op#10 RESHAPE(T#21_22, T#21_8) -> [T#21_25]
  Op#11 RESHAPE(T#21_24, T#21_7) -> [T#21_26]
  Op#12 BATCH_MATMUL(T#21_25, T#21_26) -> [T#21_27]
  Op#13 ADD(T#21_27, T#21_3) -> [T#21_28]
  Op#14 SOFTMAX(T#21_28) -> [T#21_29]
  Op#15 RESHAPE(T#21_29, T#21_6) -> [T#21_30]
  Op#16 RESHAPE(T#21_21, T#21_5) -> [T#21_31]
  Op#17 BATCH_MATMUL(T#21_30, T#21_31) -> [T#21_32]
  Op#18 RESHAPE(T#21_32, T#21_4) -> [T#21_33]

Tensors of Subgraph#21
  T#21_0(odml.scaled_dot_product_attention.impl_18_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#21_1(odml.scaled_dot_product_attention.impl_18_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#21_2(odml.scaled_dot_product_attention.impl_18_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#21_3(odml.scaled_dot_product_attention.impl_18_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#21_4(arith.constant656) shape:[4], type:INT32
  T#21_5(arith.constant657) shape:[3], type:INT32
  T#21_6(arith.constant658) shape:[3], type:INT32
  T#21_7(arith.constant659) shape:[3], type:INT32
  T#21_8(arith.constant660) shape:[3], type:INT32
  T#21_9(arith.constant661) shape:[4], type:INT32
  T#21_10(arith.constant662) shape:[4], type:INT32
  T#21_11(arith.constant663) shape:[5], type:INT32
  T#21_12(arith.constant664) shape:[4], type:INT32
  T#21_13(arith.constant665) shape:[5], type:INT64
  T#21_14(arith.constant666) shape:[], type:FLOAT32
  T#21_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#21_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#21_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#21_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#21_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#21_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#21_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#21_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#21_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#21_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#21_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#21_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#21_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#21_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#21_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#21_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#21_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#21_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#21_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#22 odml.scaled_dot_product_attention.impl_19(T#22_0, T#22_1, T#22_2, T#22_3) -> [T#22_33]
  Op#0 TRANSPOSE(T#22_1, T#22_12) -> [T#22_15]
  Op#1 TRANSPOSE(T#22_2, T#22_12) -> [T#22_16]
  Op#2 RESHAPE(T#22_15, T#22_11) -> [T#22_17]
  Op#3 BROADCAST_TO(T#22_17, T#22_13) -> [T#22_18]
  Op#4 RESHAPE(T#22_18, T#22_10) -> [T#22_19]
  Op#5 RESHAPE(T#22_16, T#22_11) -> [T#22_20]
  Op#6 BROADCAST_TO(T#22_20, T#22_13) -> [T#22_21]
  Op#7 MUL(T#22_0, T#22_14) -> [T#22_22]
  Op#8 MUL(T#22_19, T#22_14) -> [T#22_23]
  Op#9 TRANSPOSE(T#22_23, T#22_9) -> [T#22_24]
  Op#10 RESHAPE(T#22_22, T#22_8) -> [T#22_25]
  Op#11 RESHAPE(T#22_24, T#22_7) -> [T#22_26]
  Op#12 BATCH_MATMUL(T#22_25, T#22_26) -> [T#22_27]
  Op#13 ADD(T#22_27, T#22_3) -> [T#22_28]
  Op#14 SOFTMAX(T#22_28) -> [T#22_29]
  Op#15 RESHAPE(T#22_29, T#22_6) -> [T#22_30]
  Op#16 RESHAPE(T#22_21, T#22_5) -> [T#22_31]
  Op#17 BATCH_MATMUL(T#22_30, T#22_31) -> [T#22_32]
  Op#18 RESHAPE(T#22_32, T#22_4) -> [T#22_33]

Tensors of Subgraph#22
  T#22_0(odml.scaled_dot_product_attention.impl_19_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#22_1(odml.scaled_dot_product_attention.impl_19_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#22_2(odml.scaled_dot_product_attention.impl_19_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#22_3(odml.scaled_dot_product_attention.impl_19_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#22_4(arith.constant667) shape:[4], type:INT32
  T#22_5(arith.constant668) shape:[3], type:INT32
  T#22_6(arith.constant669) shape:[3], type:INT32
  T#22_7(arith.constant670) shape:[3], type:INT32
  T#22_8(arith.constant671) shape:[3], type:INT32
  T#22_9(arith.constant672) shape:[4], type:INT32
  T#22_10(arith.constant673) shape:[4], type:INT32
  T#22_11(arith.constant674) shape:[5], type:INT32
  T#22_12(arith.constant675) shape:[4], type:INT32
  T#22_13(arith.constant676) shape:[5], type:INT64
  T#22_14(arith.constant677) shape:[], type:FLOAT32
  T#22_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#22_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#22_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#22_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#22_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#22_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#22_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#22_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#22_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#22_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#22_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#22_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#22_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#22_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#22_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#22_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#22_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#22_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#22_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#23 odml.scaled_dot_product_attention.impl_20(T#23_0, T#23_1, T#23_2, T#23_3) -> [T#23_33]
  Op#0 TRANSPOSE(T#23_1, T#23_12) -> [T#23_15]
  Op#1 TRANSPOSE(T#23_2, T#23_12) -> [T#23_16]
  Op#2 RESHAPE(T#23_15, T#23_11) -> [T#23_17]
  Op#3 BROADCAST_TO(T#23_17, T#23_13) -> [T#23_18]
  Op#4 RESHAPE(T#23_18, T#23_10) -> [T#23_19]
  Op#5 RESHAPE(T#23_16, T#23_11) -> [T#23_20]
  Op#6 BROADCAST_TO(T#23_20, T#23_13) -> [T#23_21]
  Op#7 MUL(T#23_0, T#23_14) -> [T#23_22]
  Op#8 MUL(T#23_19, T#23_14) -> [T#23_23]
  Op#9 TRANSPOSE(T#23_23, T#23_9) -> [T#23_24]
  Op#10 RESHAPE(T#23_22, T#23_8) -> [T#23_25]
  Op#11 RESHAPE(T#23_24, T#23_7) -> [T#23_26]
  Op#12 BATCH_MATMUL(T#23_25, T#23_26) -> [T#23_27]
  Op#13 ADD(T#23_27, T#23_3) -> [T#23_28]
  Op#14 SOFTMAX(T#23_28) -> [T#23_29]
  Op#15 RESHAPE(T#23_29, T#23_6) -> [T#23_30]
  Op#16 RESHAPE(T#23_21, T#23_5) -> [T#23_31]
  Op#17 BATCH_MATMUL(T#23_30, T#23_31) -> [T#23_32]
  Op#18 RESHAPE(T#23_32, T#23_4) -> [T#23_33]

Tensors of Subgraph#23
  T#23_0(odml.scaled_dot_product_attention.impl_20_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#23_1(odml.scaled_dot_product_attention.impl_20_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#23_2(odml.scaled_dot_product_attention.impl_20_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#23_3(odml.scaled_dot_product_attention.impl_20_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#23_4(arith.constant678) shape:[4], type:INT32
  T#23_5(arith.constant679) shape:[3], type:INT32
  T#23_6(arith.constant680) shape:[3], type:INT32
  T#23_7(arith.constant681) shape:[3], type:INT32
  T#23_8(arith.constant682) shape:[3], type:INT32
  T#23_9(arith.constant683) shape:[4], type:INT32
  T#23_10(arith.constant684) shape:[4], type:INT32
  T#23_11(arith.constant685) shape:[5], type:INT32
  T#23_12(arith.constant686) shape:[4], type:INT32
  T#23_13(arith.constant687) shape:[5], type:INT64
  T#23_14(arith.constant688) shape:[], type:FLOAT32
  T#23_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#23_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#23_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#23_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#23_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#23_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#23_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#23_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#23_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#23_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#23_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#23_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#23_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#23_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#23_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#23_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#23_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#23_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#23_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#24 odml.scaled_dot_product_attention.impl_21(T#24_0, T#24_1, T#24_2, T#24_3) -> [T#24_33]
  Op#0 TRANSPOSE(T#24_1, T#24_12) -> [T#24_15]
  Op#1 TRANSPOSE(T#24_2, T#24_12) -> [T#24_16]
  Op#2 RESHAPE(T#24_15, T#24_11) -> [T#24_17]
  Op#3 BROADCAST_TO(T#24_17, T#24_13) -> [T#24_18]
  Op#4 RESHAPE(T#24_18, T#24_10) -> [T#24_19]
  Op#5 RESHAPE(T#24_16, T#24_11) -> [T#24_20]
  Op#6 BROADCAST_TO(T#24_20, T#24_13) -> [T#24_21]
  Op#7 MUL(T#24_0, T#24_14) -> [T#24_22]
  Op#8 MUL(T#24_19, T#24_14) -> [T#24_23]
  Op#9 TRANSPOSE(T#24_23, T#24_9) -> [T#24_24]
  Op#10 RESHAPE(T#24_22, T#24_8) -> [T#24_25]
  Op#11 RESHAPE(T#24_24, T#24_7) -> [T#24_26]
  Op#12 BATCH_MATMUL(T#24_25, T#24_26) -> [T#24_27]
  Op#13 ADD(T#24_27, T#24_3) -> [T#24_28]
  Op#14 SOFTMAX(T#24_28) -> [T#24_29]
  Op#15 RESHAPE(T#24_29, T#24_6) -> [T#24_30]
  Op#16 RESHAPE(T#24_21, T#24_5) -> [T#24_31]
  Op#17 BATCH_MATMUL(T#24_30, T#24_31) -> [T#24_32]
  Op#18 RESHAPE(T#24_32, T#24_4) -> [T#24_33]

Tensors of Subgraph#24
  T#24_0(odml.scaled_dot_product_attention.impl_21_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#24_1(odml.scaled_dot_product_attention.impl_21_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#24_2(odml.scaled_dot_product_attention.impl_21_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#24_3(odml.scaled_dot_product_attention.impl_21_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#24_4(arith.constant689) shape:[4], type:INT32
  T#24_5(arith.constant690) shape:[3], type:INT32
  T#24_6(arith.constant691) shape:[3], type:INT32
  T#24_7(arith.constant692) shape:[3], type:INT32
  T#24_8(arith.constant693) shape:[3], type:INT32
  T#24_9(arith.constant694) shape:[4], type:INT32
  T#24_10(arith.constant695) shape:[4], type:INT32
  T#24_11(arith.constant696) shape:[5], type:INT32
  T#24_12(arith.constant697) shape:[4], type:INT32
  T#24_13(arith.constant698) shape:[5], type:INT64
  T#24_14(arith.constant699) shape:[], type:FLOAT32
  T#24_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#24_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#24_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#24_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#24_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#24_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#24_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#24_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#24_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#24_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#24_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#24_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#24_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#24_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#24_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#24_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#24_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#24_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#24_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#25 odml.scaled_dot_product_attention.impl_22(T#25_0, T#25_1, T#25_2, T#25_3) -> [T#25_33]
  Op#0 TRANSPOSE(T#25_1, T#25_12) -> [T#25_15]
  Op#1 TRANSPOSE(T#25_2, T#25_12) -> [T#25_16]
  Op#2 RESHAPE(T#25_15, T#25_11) -> [T#25_17]
  Op#3 BROADCAST_TO(T#25_17, T#25_13) -> [T#25_18]
  Op#4 RESHAPE(T#25_18, T#25_10) -> [T#25_19]
  Op#5 RESHAPE(T#25_16, T#25_11) -> [T#25_20]
  Op#6 BROADCAST_TO(T#25_20, T#25_13) -> [T#25_21]
  Op#7 MUL(T#25_0, T#25_14) -> [T#25_22]
  Op#8 MUL(T#25_19, T#25_14) -> [T#25_23]
  Op#9 TRANSPOSE(T#25_23, T#25_9) -> [T#25_24]
  Op#10 RESHAPE(T#25_22, T#25_8) -> [T#25_25]
  Op#11 RESHAPE(T#25_24, T#25_7) -> [T#25_26]
  Op#12 BATCH_MATMUL(T#25_25, T#25_26) -> [T#25_27]
  Op#13 ADD(T#25_27, T#25_3) -> [T#25_28]
  Op#14 SOFTMAX(T#25_28) -> [T#25_29]
  Op#15 RESHAPE(T#25_29, T#25_6) -> [T#25_30]
  Op#16 RESHAPE(T#25_21, T#25_5) -> [T#25_31]
  Op#17 BATCH_MATMUL(T#25_30, T#25_31) -> [T#25_32]
  Op#18 RESHAPE(T#25_32, T#25_4) -> [T#25_33]

Tensors of Subgraph#25
  T#25_0(odml.scaled_dot_product_attention.impl_22_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#25_1(odml.scaled_dot_product_attention.impl_22_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#25_2(odml.scaled_dot_product_attention.impl_22_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#25_3(odml.scaled_dot_product_attention.impl_22_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#25_4(arith.constant700) shape:[4], type:INT32
  T#25_5(arith.constant701) shape:[3], type:INT32
  T#25_6(arith.constant702) shape:[3], type:INT32
  T#25_7(arith.constant703) shape:[3], type:INT32
  T#25_8(arith.constant704) shape:[3], type:INT32
  T#25_9(arith.constant705) shape:[4], type:INT32
  T#25_10(arith.constant706) shape:[4], type:INT32
  T#25_11(arith.constant707) shape:[5], type:INT32
  T#25_12(arith.constant708) shape:[4], type:INT32
  T#25_13(arith.constant709) shape:[5], type:INT64
  T#25_14(arith.constant710) shape:[], type:FLOAT32
  T#25_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#25_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#25_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#25_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#25_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#25_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#25_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#25_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#25_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#25_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#25_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#25_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#25_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#25_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#25_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#25_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#25_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#25_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#25_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#26 odml.scaled_dot_product_attention.impl_23(T#26_0, T#26_1, T#26_2, T#26_3) -> [T#26_33]
  Op#0 TRANSPOSE(T#26_1, T#26_12) -> [T#26_15]
  Op#1 TRANSPOSE(T#26_2, T#26_12) -> [T#26_16]
  Op#2 RESHAPE(T#26_15, T#26_11) -> [T#26_17]
  Op#3 BROADCAST_TO(T#26_17, T#26_13) -> [T#26_18]
  Op#4 RESHAPE(T#26_18, T#26_10) -> [T#26_19]
  Op#5 RESHAPE(T#26_16, T#26_11) -> [T#26_20]
  Op#6 BROADCAST_TO(T#26_20, T#26_13) -> [T#26_21]
  Op#7 MUL(T#26_0, T#26_14) -> [T#26_22]
  Op#8 MUL(T#26_19, T#26_14) -> [T#26_23]
  Op#9 TRANSPOSE(T#26_23, T#26_9) -> [T#26_24]
  Op#10 RESHAPE(T#26_22, T#26_8) -> [T#26_25]
  Op#11 RESHAPE(T#26_24, T#26_7) -> [T#26_26]
  Op#12 BATCH_MATMUL(T#26_25, T#26_26) -> [T#26_27]
  Op#13 ADD(T#26_27, T#26_3) -> [T#26_28]
  Op#14 SOFTMAX(T#26_28) -> [T#26_29]
  Op#15 RESHAPE(T#26_29, T#26_6) -> [T#26_30]
  Op#16 RESHAPE(T#26_21, T#26_5) -> [T#26_31]
  Op#17 BATCH_MATMUL(T#26_30, T#26_31) -> [T#26_32]
  Op#18 RESHAPE(T#26_32, T#26_4) -> [T#26_33]

Tensors of Subgraph#26
  T#26_0(odml.scaled_dot_product_attention.impl_23_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#26_1(odml.scaled_dot_product_attention.impl_23_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#26_2(odml.scaled_dot_product_attention.impl_23_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#26_3(odml.scaled_dot_product_attention.impl_23_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#26_4(arith.constant711) shape:[4], type:INT32
  T#26_5(arith.constant712) shape:[3], type:INT32
  T#26_6(arith.constant713) shape:[3], type:INT32
  T#26_7(arith.constant714) shape:[3], type:INT32
  T#26_8(arith.constant715) shape:[3], type:INT32
  T#26_9(arith.constant716) shape:[4], type:INT32
  T#26_10(arith.constant717) shape:[4], type:INT32
  T#26_11(arith.constant718) shape:[5], type:INT32
  T#26_12(arith.constant719) shape:[4], type:INT32
  T#26_13(arith.constant720) shape:[5], type:INT64
  T#26_14(arith.constant721) shape:[], type:FLOAT32
  T#26_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#26_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#26_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#26_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#26_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#26_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#26_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#26_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#26_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#26_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#26_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#26_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#26_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#26_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#26_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#26_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#26_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#26_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#26_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#27 odml.scaled_dot_product_attention.impl_24(T#27_0, T#27_1, T#27_2, T#27_3) -> [T#27_33]
  Op#0 TRANSPOSE(T#27_1, T#27_12) -> [T#27_15]
  Op#1 TRANSPOSE(T#27_2, T#27_12) -> [T#27_16]
  Op#2 RESHAPE(T#27_15, T#27_11) -> [T#27_17]
  Op#3 BROADCAST_TO(T#27_17, T#27_13) -> [T#27_18]
  Op#4 RESHAPE(T#27_18, T#27_10) -> [T#27_19]
  Op#5 RESHAPE(T#27_16, T#27_11) -> [T#27_20]
  Op#6 BROADCAST_TO(T#27_20, T#27_13) -> [T#27_21]
  Op#7 MUL(T#27_0, T#27_14) -> [T#27_22]
  Op#8 MUL(T#27_19, T#27_14) -> [T#27_23]
  Op#9 TRANSPOSE(T#27_23, T#27_9) -> [T#27_24]
  Op#10 RESHAPE(T#27_22, T#27_8) -> [T#27_25]
  Op#11 RESHAPE(T#27_24, T#27_7) -> [T#27_26]
  Op#12 BATCH_MATMUL(T#27_25, T#27_26) -> [T#27_27]
  Op#13 ADD(T#27_27, T#27_3) -> [T#27_28]
  Op#14 SOFTMAX(T#27_28) -> [T#27_29]
  Op#15 RESHAPE(T#27_29, T#27_6) -> [T#27_30]
  Op#16 RESHAPE(T#27_21, T#27_5) -> [T#27_31]
  Op#17 BATCH_MATMUL(T#27_30, T#27_31) -> [T#27_32]
  Op#18 RESHAPE(T#27_32, T#27_4) -> [T#27_33]

Tensors of Subgraph#27
  T#27_0(odml.scaled_dot_product_attention.impl_24_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#27_1(odml.scaled_dot_product_attention.impl_24_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#27_2(odml.scaled_dot_product_attention.impl_24_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#27_3(odml.scaled_dot_product_attention.impl_24_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#27_4(arith.constant722) shape:[4], type:INT32
  T#27_5(arith.constant723) shape:[3], type:INT32
  T#27_6(arith.constant724) shape:[3], type:INT32
  T#27_7(arith.constant725) shape:[3], type:INT32
  T#27_8(arith.constant726) shape:[3], type:INT32
  T#27_9(arith.constant727) shape:[4], type:INT32
  T#27_10(arith.constant728) shape:[4], type:INT32
  T#27_11(arith.constant729) shape:[5], type:INT32
  T#27_12(arith.constant730) shape:[4], type:INT32
  T#27_13(arith.constant731) shape:[5], type:INT64
  T#27_14(arith.constant732) shape:[], type:FLOAT32
  T#27_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#27_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#27_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#27_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#27_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#27_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#27_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#27_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#27_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#27_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#27_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#27_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#27_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#27_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#27_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#27_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#27_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#27_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#27_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#28 odml.scaled_dot_product_attention.impl_25(T#28_0, T#28_1, T#28_2, T#28_3) -> [T#28_33]
  Op#0 TRANSPOSE(T#28_1, T#28_12) -> [T#28_15]
  Op#1 TRANSPOSE(T#28_2, T#28_12) -> [T#28_16]
  Op#2 RESHAPE(T#28_15, T#28_11) -> [T#28_17]
  Op#3 BROADCAST_TO(T#28_17, T#28_13) -> [T#28_18]
  Op#4 RESHAPE(T#28_18, T#28_10) -> [T#28_19]
  Op#5 RESHAPE(T#28_16, T#28_11) -> [T#28_20]
  Op#6 BROADCAST_TO(T#28_20, T#28_13) -> [T#28_21]
  Op#7 MUL(T#28_0, T#28_14) -> [T#28_22]
  Op#8 MUL(T#28_19, T#28_14) -> [T#28_23]
  Op#9 TRANSPOSE(T#28_23, T#28_9) -> [T#28_24]
  Op#10 RESHAPE(T#28_22, T#28_8) -> [T#28_25]
  Op#11 RESHAPE(T#28_24, T#28_7) -> [T#28_26]
  Op#12 BATCH_MATMUL(T#28_25, T#28_26) -> [T#28_27]
  Op#13 ADD(T#28_27, T#28_3) -> [T#28_28]
  Op#14 SOFTMAX(T#28_28) -> [T#28_29]
  Op#15 RESHAPE(T#28_29, T#28_6) -> [T#28_30]
  Op#16 RESHAPE(T#28_21, T#28_5) -> [T#28_31]
  Op#17 BATCH_MATMUL(T#28_30, T#28_31) -> [T#28_32]
  Op#18 RESHAPE(T#28_32, T#28_4) -> [T#28_33]

Tensors of Subgraph#28
  T#28_0(odml.scaled_dot_product_attention.impl_25_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#28_1(odml.scaled_dot_product_attention.impl_25_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#28_2(odml.scaled_dot_product_attention.impl_25_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#28_3(odml.scaled_dot_product_attention.impl_25_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#28_4(arith.constant733) shape:[4], type:INT32
  T#28_5(arith.constant734) shape:[3], type:INT32
  T#28_6(arith.constant735) shape:[3], type:INT32
  T#28_7(arith.constant736) shape:[3], type:INT32
  T#28_8(arith.constant737) shape:[3], type:INT32
  T#28_9(arith.constant738) shape:[4], type:INT32
  T#28_10(arith.constant739) shape:[4], type:INT32
  T#28_11(arith.constant740) shape:[5], type:INT32
  T#28_12(arith.constant741) shape:[4], type:INT32
  T#28_13(arith.constant742) shape:[5], type:INT64
  T#28_14(arith.constant743) shape:[], type:FLOAT32
  T#28_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#28_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#28_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#28_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#28_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#28_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#28_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#28_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#28_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#28_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#28_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#28_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#28_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#28_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#28_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#28_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#28_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#28_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#28_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#29 odml.scaled_dot_product_attention.impl_26(T#29_0, T#29_1, T#29_2, T#29_3) -> [T#29_33]
  Op#0 TRANSPOSE(T#29_1, T#29_12) -> [T#29_15]
  Op#1 TRANSPOSE(T#29_2, T#29_12) -> [T#29_16]
  Op#2 RESHAPE(T#29_15, T#29_11) -> [T#29_17]
  Op#3 BROADCAST_TO(T#29_17, T#29_13) -> [T#29_18]
  Op#4 RESHAPE(T#29_18, T#29_10) -> [T#29_19]
  Op#5 RESHAPE(T#29_16, T#29_11) -> [T#29_20]
  Op#6 BROADCAST_TO(T#29_20, T#29_13) -> [T#29_21]
  Op#7 MUL(T#29_0, T#29_14) -> [T#29_22]
  Op#8 MUL(T#29_19, T#29_14) -> [T#29_23]
  Op#9 TRANSPOSE(T#29_23, T#29_9) -> [T#29_24]
  Op#10 RESHAPE(T#29_22, T#29_8) -> [T#29_25]
  Op#11 RESHAPE(T#29_24, T#29_7) -> [T#29_26]
  Op#12 BATCH_MATMUL(T#29_25, T#29_26) -> [T#29_27]
  Op#13 ADD(T#29_27, T#29_3) -> [T#29_28]
  Op#14 SOFTMAX(T#29_28) -> [T#29_29]
  Op#15 RESHAPE(T#29_29, T#29_6) -> [T#29_30]
  Op#16 RESHAPE(T#29_21, T#29_5) -> [T#29_31]
  Op#17 BATCH_MATMUL(T#29_30, T#29_31) -> [T#29_32]
  Op#18 RESHAPE(T#29_32, T#29_4) -> [T#29_33]

Tensors of Subgraph#29
  T#29_0(odml.scaled_dot_product_attention.impl_26_arg0) shape:[1, 1, 24, 128], type:FLOAT32
  T#29_1(odml.scaled_dot_product_attention.impl_26_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#29_2(odml.scaled_dot_product_attention.impl_26_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#29_3(odml.scaled_dot_product_attention.impl_26_arg3) shape:[1, 1, 1, 1280], type:FLOAT32
  T#29_4(arith.constant744) shape:[4], type:INT32
  T#29_5(arith.constant745) shape:[3], type:INT32
  T#29_6(arith.constant746) shape:[3], type:INT32
  T#29_7(arith.constant747) shape:[3], type:INT32
  T#29_8(arith.constant748) shape:[3], type:INT32
  T#29_9(arith.constant749) shape:[4], type:INT32
  T#29_10(arith.constant750) shape:[4], type:INT32
  T#29_11(arith.constant751) shape:[5], type:INT32
  T#29_12(arith.constant752) shape:[4], type:INT32
  T#29_13(arith.constant753) shape:[5], type:INT64
  T#29_14(arith.constant754) shape:[], type:FLOAT32
  T#29_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;37) shape:[1, 8, 1280, 128], type:FLOAT32
  T#29_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;38) shape:[1, 8, 1280, 128], type:FLOAT32
  T#29_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;39) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#29_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;40) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#29_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;41) shape:[1, 24, 1280, 128], type:FLOAT32
  T#29_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;42) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#29_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;43) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#29_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;44) shape:[1, 1, 24, 128], type:FLOAT32
  T#29_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;45) shape:[1, 24, 1280, 128], type:FLOAT32
  T#29_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;46) shape:[1, 24, 128, 1280], type:FLOAT32
  T#29_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;47) shape:[24, 1, 128], type:FLOAT32
  T#29_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;48) shape:[24, 128, 1280], type:FLOAT32
  T#29_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;49) shape:[24, 1, 1280], type:FLOAT32
  T#29_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;50) shape:[1, 24, 1, 1280], type:FLOAT32
  T#29_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;51) shape:[1, 24, 1, 1280], type:FLOAT32
  T#29_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;52) shape:[24, 1, 1280], type:FLOAT32
  T#29_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;53) shape:[24, 1280, 128], type:FLOAT32
  T#29_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;54) shape:[24, 1, 128], type:FLOAT32
  T#29_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;55) shape:[1, 1, 24, 128], type:FLOAT32

Subgraph#30 odml.scaled_dot_product_attention.impl_27(T#30_0, T#30_1, T#30_2, T#30_3) -> [T#30_35]
  Op#0 TRANSPOSE(T#30_1, T#30_14) -> [T#30_15]
  Op#1 TRANSPOSE(T#30_2, T#30_14) -> [T#30_16]
  Op#2 RESHAPE(T#30_15, T#30_11) -> [T#30_17]
  Op#3 BROADCAST_TO(T#30_17, T#30_12) -> [T#30_18]
  Op#4 RESHAPE(T#30_18, T#30_10) -> [T#30_19]
  Op#5 RESHAPE(T#30_16, T#30_11) -> [T#30_20]
  Op#6 BROADCAST_TO(T#30_20, T#30_12) -> [T#30_21]
  Op#7 MUL(T#30_0, T#30_13) -> [T#30_22]
  Op#8 TRANSPOSE(T#30_22, T#30_14) -> [T#30_23]
  Op#9 MUL(T#30_19, T#30_13) -> [T#30_24]
  Op#10 TRANSPOSE(T#30_24, T#30_9) -> [T#30_25]
  Op#11 RESHAPE(T#30_23, T#30_8) -> [T#30_26]
  Op#12 RESHAPE(T#30_25, T#30_7) -> [T#30_27]
  Op#13 BATCH_MATMUL(T#30_26, T#30_27) -> [T#30_28]
  Op#14 ADD(T#30_28, T#30_3) -> [T#30_29]
  Op#15 SOFTMAX(T#30_29) -> [T#30_30]
  Op#16 RESHAPE(T#30_30, T#30_6) -> [T#30_31]
  Op#17 RESHAPE(T#30_21, T#30_5) -> [T#30_32]
  Op#18 BATCH_MATMUL(T#30_31, T#30_32) -> [T#30_33]
  Op#19 RESHAPE(T#30_33, T#30_4) -> [T#30_34]
  Op#20 TRANSPOSE(T#30_34, T#30_14) -> [T#30_35]

Tensors of Subgraph#30
  T#30_0(odml.scaled_dot_product_attention.impl_27_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#30_1(odml.scaled_dot_product_attention.impl_27_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#30_2(odml.scaled_dot_product_attention.impl_27_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#30_3(odml.scaled_dot_product_attention.impl_27_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#30_4(arith.constant755) shape:[4], type:INT32
  T#30_5(arith.constant756) shape:[3], type:INT32
  T#30_6(arith.constant757) shape:[3], type:INT32
  T#30_7(arith.constant758) shape:[3], type:INT32
  T#30_8(arith.constant759) shape:[3], type:INT32
  T#30_9(arith.constant760) shape:[4], type:INT32
  T#30_10(arith.constant761) shape:[4], type:INT32
  T#30_11(arith.constant762) shape:[5], type:INT32
  T#30_12(arith.constant763) shape:[5], type:INT64
  T#30_13(arith.constant764) shape:[], type:FLOAT32
  T#30_14(arith.constant765) shape:[4], type:INT32
  T#30_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#30_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#30_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#30_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#30_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#30_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#30_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#30_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#30_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#30_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#30_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#30_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#30_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#30_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#30_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#30_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#30_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#30_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#30_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#30_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#30_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_25/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#31 odml.scaled_dot_product_attention.impl_28(T#31_0, T#31_1, T#31_2, T#31_3) -> [T#31_35]
  Op#0 TRANSPOSE(T#31_1, T#31_14) -> [T#31_15]
  Op#1 TRANSPOSE(T#31_2, T#31_14) -> [T#31_16]
  Op#2 RESHAPE(T#31_15, T#31_11) -> [T#31_17]
  Op#3 BROADCAST_TO(T#31_17, T#31_12) -> [T#31_18]
  Op#4 RESHAPE(T#31_18, T#31_10) -> [T#31_19]
  Op#5 RESHAPE(T#31_16, T#31_11) -> [T#31_20]
  Op#6 BROADCAST_TO(T#31_20, T#31_12) -> [T#31_21]
  Op#7 MUL(T#31_0, T#31_13) -> [T#31_22]
  Op#8 TRANSPOSE(T#31_22, T#31_14) -> [T#31_23]
  Op#9 MUL(T#31_19, T#31_13) -> [T#31_24]
  Op#10 TRANSPOSE(T#31_24, T#31_9) -> [T#31_25]
  Op#11 RESHAPE(T#31_23, T#31_8) -> [T#31_26]
  Op#12 RESHAPE(T#31_25, T#31_7) -> [T#31_27]
  Op#13 BATCH_MATMUL(T#31_26, T#31_27) -> [T#31_28]
  Op#14 ADD(T#31_28, T#31_3) -> [T#31_29]
  Op#15 SOFTMAX(T#31_29) -> [T#31_30]
  Op#16 RESHAPE(T#31_30, T#31_6) -> [T#31_31]
  Op#17 RESHAPE(T#31_21, T#31_5) -> [T#31_32]
  Op#18 BATCH_MATMUL(T#31_31, T#31_32) -> [T#31_33]
  Op#19 RESHAPE(T#31_33, T#31_4) -> [T#31_34]
  Op#20 TRANSPOSE(T#31_34, T#31_14) -> [T#31_35]

Tensors of Subgraph#31
  T#31_0(odml.scaled_dot_product_attention.impl_28_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#31_1(odml.scaled_dot_product_attention.impl_28_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#31_2(odml.scaled_dot_product_attention.impl_28_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#31_3(odml.scaled_dot_product_attention.impl_28_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#31_4(arith.constant766) shape:[4], type:INT32
  T#31_5(arith.constant767) shape:[3], type:INT32
  T#31_6(arith.constant768) shape:[3], type:INT32
  T#31_7(arith.constant769) shape:[3], type:INT32
  T#31_8(arith.constant770) shape:[3], type:INT32
  T#31_9(arith.constant771) shape:[4], type:INT32
  T#31_10(arith.constant772) shape:[4], type:INT32
  T#31_11(arith.constant773) shape:[5], type:INT32
  T#31_12(arith.constant774) shape:[5], type:INT64
  T#31_13(arith.constant775) shape:[], type:FLOAT32
  T#31_14(arith.constant776) shape:[4], type:INT32
  T#31_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#31_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#31_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#31_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#31_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#31_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#31_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#31_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#31_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#31_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#31_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#31_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#31_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#31_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#31_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#31_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#31_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#31_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#31_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#31_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#31_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_24/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#32 odml.scaled_dot_product_attention.impl_29(T#32_0, T#32_1, T#32_2, T#32_3) -> [T#32_35]
  Op#0 TRANSPOSE(T#32_1, T#32_14) -> [T#32_15]
  Op#1 TRANSPOSE(T#32_2, T#32_14) -> [T#32_16]
  Op#2 RESHAPE(T#32_15, T#32_11) -> [T#32_17]
  Op#3 BROADCAST_TO(T#32_17, T#32_12) -> [T#32_18]
  Op#4 RESHAPE(T#32_18, T#32_10) -> [T#32_19]
  Op#5 RESHAPE(T#32_16, T#32_11) -> [T#32_20]
  Op#6 BROADCAST_TO(T#32_20, T#32_12) -> [T#32_21]
  Op#7 MUL(T#32_0, T#32_13) -> [T#32_22]
  Op#8 TRANSPOSE(T#32_22, T#32_14) -> [T#32_23]
  Op#9 MUL(T#32_19, T#32_13) -> [T#32_24]
  Op#10 TRANSPOSE(T#32_24, T#32_9) -> [T#32_25]
  Op#11 RESHAPE(T#32_23, T#32_8) -> [T#32_26]
  Op#12 RESHAPE(T#32_25, T#32_7) -> [T#32_27]
  Op#13 BATCH_MATMUL(T#32_26, T#32_27) -> [T#32_28]
  Op#14 ADD(T#32_28, T#32_3) -> [T#32_29]
  Op#15 SOFTMAX(T#32_29) -> [T#32_30]
  Op#16 RESHAPE(T#32_30, T#32_6) -> [T#32_31]
  Op#17 RESHAPE(T#32_21, T#32_5) -> [T#32_32]
  Op#18 BATCH_MATMUL(T#32_31, T#32_32) -> [T#32_33]
  Op#19 RESHAPE(T#32_33, T#32_4) -> [T#32_34]
  Op#20 TRANSPOSE(T#32_34, T#32_14) -> [T#32_35]

Tensors of Subgraph#32
  T#32_0(odml.scaled_dot_product_attention.impl_29_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#32_1(odml.scaled_dot_product_attention.impl_29_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#32_2(odml.scaled_dot_product_attention.impl_29_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#32_3(odml.scaled_dot_product_attention.impl_29_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#32_4(arith.constant777) shape:[4], type:INT32
  T#32_5(arith.constant778) shape:[3], type:INT32
  T#32_6(arith.constant779) shape:[3], type:INT32
  T#32_7(arith.constant780) shape:[3], type:INT32
  T#32_8(arith.constant781) shape:[3], type:INT32
  T#32_9(arith.constant782) shape:[4], type:INT32
  T#32_10(arith.constant783) shape:[4], type:INT32
  T#32_11(arith.constant784) shape:[5], type:INT32
  T#32_12(arith.constant785) shape:[5], type:INT64
  T#32_13(arith.constant786) shape:[], type:FLOAT32
  T#32_14(arith.constant787) shape:[4], type:INT32
  T#32_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#32_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#32_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#32_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#32_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#32_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#32_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#32_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#32_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#32_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#32_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#32_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#32_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#32_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#32_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#32_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#32_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#32_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#32_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#32_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#32_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_22/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#33 odml.scaled_dot_product_attention.impl_30(T#33_0, T#33_1, T#33_2, T#33_3) -> [T#33_35]
  Op#0 TRANSPOSE(T#33_1, T#33_14) -> [T#33_15]
  Op#1 TRANSPOSE(T#33_2, T#33_14) -> [T#33_16]
  Op#2 RESHAPE(T#33_15, T#33_11) -> [T#33_17]
  Op#3 BROADCAST_TO(T#33_17, T#33_12) -> [T#33_18]
  Op#4 RESHAPE(T#33_18, T#33_10) -> [T#33_19]
  Op#5 RESHAPE(T#33_16, T#33_11) -> [T#33_20]
  Op#6 BROADCAST_TO(T#33_20, T#33_12) -> [T#33_21]
  Op#7 MUL(T#33_0, T#33_13) -> [T#33_22]
  Op#8 TRANSPOSE(T#33_22, T#33_14) -> [T#33_23]
  Op#9 MUL(T#33_19, T#33_13) -> [T#33_24]
  Op#10 TRANSPOSE(T#33_24, T#33_9) -> [T#33_25]
  Op#11 RESHAPE(T#33_23, T#33_8) -> [T#33_26]
  Op#12 RESHAPE(T#33_25, T#33_7) -> [T#33_27]
  Op#13 BATCH_MATMUL(T#33_26, T#33_27) -> [T#33_28]
  Op#14 ADD(T#33_28, T#33_3) -> [T#33_29]
  Op#15 SOFTMAX(T#33_29) -> [T#33_30]
  Op#16 RESHAPE(T#33_30, T#33_6) -> [T#33_31]
  Op#17 RESHAPE(T#33_21, T#33_5) -> [T#33_32]
  Op#18 BATCH_MATMUL(T#33_31, T#33_32) -> [T#33_33]
  Op#19 RESHAPE(T#33_33, T#33_4) -> [T#33_34]
  Op#20 TRANSPOSE(T#33_34, T#33_14) -> [T#33_35]

Tensors of Subgraph#33
  T#33_0(odml.scaled_dot_product_attention.impl_30_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#33_1(odml.scaled_dot_product_attention.impl_30_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#33_2(odml.scaled_dot_product_attention.impl_30_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#33_3(odml.scaled_dot_product_attention.impl_30_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#33_4(arith.constant788) shape:[4], type:INT32
  T#33_5(arith.constant789) shape:[3], type:INT32
  T#33_6(arith.constant790) shape:[3], type:INT32
  T#33_7(arith.constant791) shape:[3], type:INT32
  T#33_8(arith.constant792) shape:[3], type:INT32
  T#33_9(arith.constant793) shape:[4], type:INT32
  T#33_10(arith.constant794) shape:[4], type:INT32
  T#33_11(arith.constant795) shape:[5], type:INT32
  T#33_12(arith.constant796) shape:[5], type:INT64
  T#33_13(arith.constant797) shape:[], type:FLOAT32
  T#33_14(arith.constant798) shape:[4], type:INT32
  T#33_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#33_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#33_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#33_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#33_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#33_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#33_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#33_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#33_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#33_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#33_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#33_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#33_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#33_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#33_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#33_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#33_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#33_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#33_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#33_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#33_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_18/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#34 odml.scaled_dot_product_attention.impl_31(T#34_0, T#34_1, T#34_2, T#34_3) -> [T#34_35]
  Op#0 TRANSPOSE(T#34_1, T#34_14) -> [T#34_15]
  Op#1 TRANSPOSE(T#34_2, T#34_14) -> [T#34_16]
  Op#2 RESHAPE(T#34_15, T#34_11) -> [T#34_17]
  Op#3 BROADCAST_TO(T#34_17, T#34_12) -> [T#34_18]
  Op#4 RESHAPE(T#34_18, T#34_10) -> [T#34_19]
  Op#5 RESHAPE(T#34_16, T#34_11) -> [T#34_20]
  Op#6 BROADCAST_TO(T#34_20, T#34_12) -> [T#34_21]
  Op#7 MUL(T#34_0, T#34_13) -> [T#34_22]
  Op#8 TRANSPOSE(T#34_22, T#34_14) -> [T#34_23]
  Op#9 MUL(T#34_19, T#34_13) -> [T#34_24]
  Op#10 TRANSPOSE(T#34_24, T#34_9) -> [T#34_25]
  Op#11 RESHAPE(T#34_23, T#34_8) -> [T#34_26]
  Op#12 RESHAPE(T#34_25, T#34_7) -> [T#34_27]
  Op#13 BATCH_MATMUL(T#34_26, T#34_27) -> [T#34_28]
  Op#14 ADD(T#34_28, T#34_3) -> [T#34_29]
  Op#15 SOFTMAX(T#34_29) -> [T#34_30]
  Op#16 RESHAPE(T#34_30, T#34_6) -> [T#34_31]
  Op#17 RESHAPE(T#34_21, T#34_5) -> [T#34_32]
  Op#18 BATCH_MATMUL(T#34_31, T#34_32) -> [T#34_33]
  Op#19 RESHAPE(T#34_33, T#34_4) -> [T#34_34]
  Op#20 TRANSPOSE(T#34_34, T#34_14) -> [T#34_35]

Tensors of Subgraph#34
  T#34_0(odml.scaled_dot_product_attention.impl_31_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#34_1(odml.scaled_dot_product_attention.impl_31_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#34_2(odml.scaled_dot_product_attention.impl_31_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#34_3(odml.scaled_dot_product_attention.impl_31_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#34_4(arith.constant799) shape:[4], type:INT32
  T#34_5(arith.constant800) shape:[3], type:INT32
  T#34_6(arith.constant801) shape:[3], type:INT32
  T#34_7(arith.constant802) shape:[3], type:INT32
  T#34_8(arith.constant803) shape:[3], type:INT32
  T#34_9(arith.constant804) shape:[4], type:INT32
  T#34_10(arith.constant805) shape:[4], type:INT32
  T#34_11(arith.constant806) shape:[5], type:INT32
  T#34_12(arith.constant807) shape:[5], type:INT64
  T#34_13(arith.constant808) shape:[], type:FLOAT32
  T#34_14(arith.constant809) shape:[4], type:INT32
  T#34_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#34_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#34_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#34_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#34_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#34_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#34_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#34_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#34_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#34_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#34_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#34_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#34_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#34_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#34_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#34_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#34_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#34_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#34_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#34_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#34_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_17/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#35 odml.scaled_dot_product_attention.impl_32(T#35_0, T#35_1, T#35_2, T#35_3) -> [T#35_35]
  Op#0 TRANSPOSE(T#35_1, T#35_14) -> [T#35_15]
  Op#1 TRANSPOSE(T#35_2, T#35_14) -> [T#35_16]
  Op#2 RESHAPE(T#35_15, T#35_11) -> [T#35_17]
  Op#3 BROADCAST_TO(T#35_17, T#35_12) -> [T#35_18]
  Op#4 RESHAPE(T#35_18, T#35_10) -> [T#35_19]
  Op#5 RESHAPE(T#35_16, T#35_11) -> [T#35_20]
  Op#6 BROADCAST_TO(T#35_20, T#35_12) -> [T#35_21]
  Op#7 MUL(T#35_0, T#35_13) -> [T#35_22]
  Op#8 TRANSPOSE(T#35_22, T#35_14) -> [T#35_23]
  Op#9 MUL(T#35_19, T#35_13) -> [T#35_24]
  Op#10 TRANSPOSE(T#35_24, T#35_9) -> [T#35_25]
  Op#11 RESHAPE(T#35_23, T#35_8) -> [T#35_26]
  Op#12 RESHAPE(T#35_25, T#35_7) -> [T#35_27]
  Op#13 BATCH_MATMUL(T#35_26, T#35_27) -> [T#35_28]
  Op#14 ADD(T#35_28, T#35_3) -> [T#35_29]
  Op#15 SOFTMAX(T#35_29) -> [T#35_30]
  Op#16 RESHAPE(T#35_30, T#35_6) -> [T#35_31]
  Op#17 RESHAPE(T#35_21, T#35_5) -> [T#35_32]
  Op#18 BATCH_MATMUL(T#35_31, T#35_32) -> [T#35_33]
  Op#19 RESHAPE(T#35_33, T#35_4) -> [T#35_34]
  Op#20 TRANSPOSE(T#35_34, T#35_14) -> [T#35_35]

Tensors of Subgraph#35
  T#35_0(odml.scaled_dot_product_attention.impl_32_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#35_1(odml.scaled_dot_product_attention.impl_32_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#35_2(odml.scaled_dot_product_attention.impl_32_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#35_3(odml.scaled_dot_product_attention.impl_32_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#35_4(arith.constant810) shape:[4], type:INT32
  T#35_5(arith.constant811) shape:[3], type:INT32
  T#35_6(arith.constant812) shape:[3], type:INT32
  T#35_7(arith.constant813) shape:[3], type:INT32
  T#35_8(arith.constant814) shape:[3], type:INT32
  T#35_9(arith.constant815) shape:[4], type:INT32
  T#35_10(arith.constant816) shape:[4], type:INT32
  T#35_11(arith.constant817) shape:[5], type:INT32
  T#35_12(arith.constant818) shape:[5], type:INT64
  T#35_13(arith.constant819) shape:[], type:FLOAT32
  T#35_14(arith.constant820) shape:[4], type:INT32
  T#35_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#35_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#35_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#35_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#35_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#35_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#35_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#35_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#35_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#35_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#35_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#35_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#35_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#35_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#35_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#35_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#35_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#35_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#35_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#35_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#35_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_16/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#36 odml.scaled_dot_product_attention.impl_33(T#36_0, T#36_1, T#36_2, T#36_3) -> [T#36_35]
  Op#0 TRANSPOSE(T#36_1, T#36_14) -> [T#36_15]
  Op#1 TRANSPOSE(T#36_2, T#36_14) -> [T#36_16]
  Op#2 RESHAPE(T#36_15, T#36_11) -> [T#36_17]
  Op#3 BROADCAST_TO(T#36_17, T#36_12) -> [T#36_18]
  Op#4 RESHAPE(T#36_18, T#36_10) -> [T#36_19]
  Op#5 RESHAPE(T#36_16, T#36_11) -> [T#36_20]
  Op#6 BROADCAST_TO(T#36_20, T#36_12) -> [T#36_21]
  Op#7 MUL(T#36_0, T#36_13) -> [T#36_22]
  Op#8 TRANSPOSE(T#36_22, T#36_14) -> [T#36_23]
  Op#9 MUL(T#36_19, T#36_13) -> [T#36_24]
  Op#10 TRANSPOSE(T#36_24, T#36_9) -> [T#36_25]
  Op#11 RESHAPE(T#36_23, T#36_8) -> [T#36_26]
  Op#12 RESHAPE(T#36_25, T#36_7) -> [T#36_27]
  Op#13 BATCH_MATMUL(T#36_26, T#36_27) -> [T#36_28]
  Op#14 ADD(T#36_28, T#36_3) -> [T#36_29]
  Op#15 SOFTMAX(T#36_29) -> [T#36_30]
  Op#16 RESHAPE(T#36_30, T#36_6) -> [T#36_31]
  Op#17 RESHAPE(T#36_21, T#36_5) -> [T#36_32]
  Op#18 BATCH_MATMUL(T#36_31, T#36_32) -> [T#36_33]
  Op#19 RESHAPE(T#36_33, T#36_4) -> [T#36_34]
  Op#20 TRANSPOSE(T#36_34, T#36_14) -> [T#36_35]

Tensors of Subgraph#36
  T#36_0(odml.scaled_dot_product_attention.impl_33_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#36_1(odml.scaled_dot_product_attention.impl_33_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#36_2(odml.scaled_dot_product_attention.impl_33_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#36_3(odml.scaled_dot_product_attention.impl_33_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#36_4(arith.constant821) shape:[4], type:INT32
  T#36_5(arith.constant822) shape:[3], type:INT32
  T#36_6(arith.constant823) shape:[3], type:INT32
  T#36_7(arith.constant824) shape:[3], type:INT32
  T#36_8(arith.constant825) shape:[3], type:INT32
  T#36_9(arith.constant826) shape:[4], type:INT32
  T#36_10(arith.constant827) shape:[4], type:INT32
  T#36_11(arith.constant828) shape:[5], type:INT32
  T#36_12(arith.constant829) shape:[5], type:INT64
  T#36_13(arith.constant830) shape:[], type:FLOAT32
  T#36_14(arith.constant831) shape:[4], type:INT32
  T#36_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#36_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#36_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#36_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#36_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#36_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#36_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#36_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#36_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#36_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#36_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#36_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#36_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#36_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#36_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#36_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#36_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#36_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#36_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#36_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#36_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_15/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#37 odml.scaled_dot_product_attention.impl_34(T#37_0, T#37_1, T#37_2, T#37_3) -> [T#37_35]
  Op#0 TRANSPOSE(T#37_1, T#37_14) -> [T#37_15]
  Op#1 TRANSPOSE(T#37_2, T#37_14) -> [T#37_16]
  Op#2 RESHAPE(T#37_15, T#37_11) -> [T#37_17]
  Op#3 BROADCAST_TO(T#37_17, T#37_12) -> [T#37_18]
  Op#4 RESHAPE(T#37_18, T#37_10) -> [T#37_19]
  Op#5 RESHAPE(T#37_16, T#37_11) -> [T#37_20]
  Op#6 BROADCAST_TO(T#37_20, T#37_12) -> [T#37_21]
  Op#7 MUL(T#37_0, T#37_13) -> [T#37_22]
  Op#8 TRANSPOSE(T#37_22, T#37_14) -> [T#37_23]
  Op#9 MUL(T#37_19, T#37_13) -> [T#37_24]
  Op#10 TRANSPOSE(T#37_24, T#37_9) -> [T#37_25]
  Op#11 RESHAPE(T#37_23, T#37_8) -> [T#37_26]
  Op#12 RESHAPE(T#37_25, T#37_7) -> [T#37_27]
  Op#13 BATCH_MATMUL(T#37_26, T#37_27) -> [T#37_28]
  Op#14 ADD(T#37_28, T#37_3) -> [T#37_29]
  Op#15 SOFTMAX(T#37_29) -> [T#37_30]
  Op#16 RESHAPE(T#37_30, T#37_6) -> [T#37_31]
  Op#17 RESHAPE(T#37_21, T#37_5) -> [T#37_32]
  Op#18 BATCH_MATMUL(T#37_31, T#37_32) -> [T#37_33]
  Op#19 RESHAPE(T#37_33, T#37_4) -> [T#37_34]
  Op#20 TRANSPOSE(T#37_34, T#37_14) -> [T#37_35]

Tensors of Subgraph#37
  T#37_0(odml.scaled_dot_product_attention.impl_34_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#37_1(odml.scaled_dot_product_attention.impl_34_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#37_2(odml.scaled_dot_product_attention.impl_34_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#37_3(odml.scaled_dot_product_attention.impl_34_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#37_4(arith.constant832) shape:[4], type:INT32
  T#37_5(arith.constant833) shape:[3], type:INT32
  T#37_6(arith.constant834) shape:[3], type:INT32
  T#37_7(arith.constant835) shape:[3], type:INT32
  T#37_8(arith.constant836) shape:[3], type:INT32
  T#37_9(arith.constant837) shape:[4], type:INT32
  T#37_10(arith.constant838) shape:[4], type:INT32
  T#37_11(arith.constant839) shape:[5], type:INT32
  T#37_12(arith.constant840) shape:[5], type:INT64
  T#37_13(arith.constant841) shape:[], type:FLOAT32
  T#37_14(arith.constant842) shape:[4], type:INT32
  T#37_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#37_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#37_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#37_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#37_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#37_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#37_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#37_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#37_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#37_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#37_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#37_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#37_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#37_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#37_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#37_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#37_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#37_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#37_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#37_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#37_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_12/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#38 odml.scaled_dot_product_attention.impl_35(T#38_0, T#38_1, T#38_2, T#38_3) -> [T#38_35]
  Op#0 TRANSPOSE(T#38_1, T#38_14) -> [T#38_15]
  Op#1 TRANSPOSE(T#38_2, T#38_14) -> [T#38_16]
  Op#2 RESHAPE(T#38_15, T#38_11) -> [T#38_17]
  Op#3 BROADCAST_TO(T#38_17, T#38_12) -> [T#38_18]
  Op#4 RESHAPE(T#38_18, T#38_10) -> [T#38_19]
  Op#5 RESHAPE(T#38_16, T#38_11) -> [T#38_20]
  Op#6 BROADCAST_TO(T#38_20, T#38_12) -> [T#38_21]
  Op#7 MUL(T#38_0, T#38_13) -> [T#38_22]
  Op#8 TRANSPOSE(T#38_22, T#38_14) -> [T#38_23]
  Op#9 MUL(T#38_19, T#38_13) -> [T#38_24]
  Op#10 TRANSPOSE(T#38_24, T#38_9) -> [T#38_25]
  Op#11 RESHAPE(T#38_23, T#38_8) -> [T#38_26]
  Op#12 RESHAPE(T#38_25, T#38_7) -> [T#38_27]
  Op#13 BATCH_MATMUL(T#38_26, T#38_27) -> [T#38_28]
  Op#14 ADD(T#38_28, T#38_3) -> [T#38_29]
  Op#15 SOFTMAX(T#38_29) -> [T#38_30]
  Op#16 RESHAPE(T#38_30, T#38_6) -> [T#38_31]
  Op#17 RESHAPE(T#38_21, T#38_5) -> [T#38_32]
  Op#18 BATCH_MATMUL(T#38_31, T#38_32) -> [T#38_33]
  Op#19 RESHAPE(T#38_33, T#38_4) -> [T#38_34]
  Op#20 TRANSPOSE(T#38_34, T#38_14) -> [T#38_35]

Tensors of Subgraph#38
  T#38_0(odml.scaled_dot_product_attention.impl_35_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#38_1(odml.scaled_dot_product_attention.impl_35_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#38_2(odml.scaled_dot_product_attention.impl_35_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#38_3(odml.scaled_dot_product_attention.impl_35_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#38_4(arith.constant843) shape:[4], type:INT32
  T#38_5(arith.constant844) shape:[3], type:INT32
  T#38_6(arith.constant845) shape:[3], type:INT32
  T#38_7(arith.constant846) shape:[3], type:INT32
  T#38_8(arith.constant847) shape:[3], type:INT32
  T#38_9(arith.constant848) shape:[4], type:INT32
  T#38_10(arith.constant849) shape:[4], type:INT32
  T#38_11(arith.constant850) shape:[5], type:INT32
  T#38_12(arith.constant851) shape:[5], type:INT64
  T#38_13(arith.constant852) shape:[], type:FLOAT32
  T#38_14(arith.constant853) shape:[4], type:INT32
  T#38_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#38_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#38_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#38_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#38_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#38_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#38_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#38_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#38_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#38_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#38_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#38_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#38_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#38_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#38_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#38_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#38_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#38_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#38_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#38_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#38_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_23/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#39 odml.scaled_dot_product_attention.impl_36(T#39_0, T#39_1, T#39_2, T#39_3) -> [T#39_35]
  Op#0 TRANSPOSE(T#39_1, T#39_14) -> [T#39_15]
  Op#1 TRANSPOSE(T#39_2, T#39_14) -> [T#39_16]
  Op#2 RESHAPE(T#39_15, T#39_11) -> [T#39_17]
  Op#3 BROADCAST_TO(T#39_17, T#39_12) -> [T#39_18]
  Op#4 RESHAPE(T#39_18, T#39_10) -> [T#39_19]
  Op#5 RESHAPE(T#39_16, T#39_11) -> [T#39_20]
  Op#6 BROADCAST_TO(T#39_20, T#39_12) -> [T#39_21]
  Op#7 MUL(T#39_0, T#39_13) -> [T#39_22]
  Op#8 TRANSPOSE(T#39_22, T#39_14) -> [T#39_23]
  Op#9 MUL(T#39_19, T#39_13) -> [T#39_24]
  Op#10 TRANSPOSE(T#39_24, T#39_9) -> [T#39_25]
  Op#11 RESHAPE(T#39_23, T#39_8) -> [T#39_26]
  Op#12 RESHAPE(T#39_25, T#39_7) -> [T#39_27]
  Op#13 BATCH_MATMUL(T#39_26, T#39_27) -> [T#39_28]
  Op#14 ADD(T#39_28, T#39_3) -> [T#39_29]
  Op#15 SOFTMAX(T#39_29) -> [T#39_30]
  Op#16 RESHAPE(T#39_30, T#39_6) -> [T#39_31]
  Op#17 RESHAPE(T#39_21, T#39_5) -> [T#39_32]
  Op#18 BATCH_MATMUL(T#39_31, T#39_32) -> [T#39_33]
  Op#19 RESHAPE(T#39_33, T#39_4) -> [T#39_34]
  Op#20 TRANSPOSE(T#39_34, T#39_14) -> [T#39_35]

Tensors of Subgraph#39
  T#39_0(odml.scaled_dot_product_attention.impl_36_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#39_1(odml.scaled_dot_product_attention.impl_36_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#39_2(odml.scaled_dot_product_attention.impl_36_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#39_3(odml.scaled_dot_product_attention.impl_36_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#39_4(arith.constant854) shape:[4], type:INT32
  T#39_5(arith.constant855) shape:[3], type:INT32
  T#39_6(arith.constant856) shape:[3], type:INT32
  T#39_7(arith.constant857) shape:[3], type:INT32
  T#39_8(arith.constant858) shape:[3], type:INT32
  T#39_9(arith.constant859) shape:[4], type:INT32
  T#39_10(arith.constant860) shape:[4], type:INT32
  T#39_11(arith.constant861) shape:[5], type:INT32
  T#39_12(arith.constant862) shape:[5], type:INT64
  T#39_13(arith.constant863) shape:[], type:FLOAT32
  T#39_14(arith.constant864) shape:[4], type:INT32
  T#39_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 8, 1280, 128], type:FLOAT32
  T#39_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 8, 1280, 128], type:FLOAT32
  T#39_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;77) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#39_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;78) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#39_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;79) shape:[1, 24, 1280, 128], type:FLOAT32
  T#39_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;80) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#39_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;81) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#39_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;82) shape:[1, 1024, 24, 128], type:FLOAT32
  T#39_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;83) shape:[1, 24, 1024, 128], type:FLOAT32
  T#39_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;84) shape:[1, 24, 1280, 128], type:FLOAT32
  T#39_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;85) shape:[1, 24, 128, 1280], type:FLOAT32
  T#39_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;86) shape:[24, 1024, 128], type:FLOAT32
  T#39_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;87) shape:[24, 128, 1280], type:FLOAT32
  T#39_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;88) shape:[24, 1024, 1280], type:FLOAT32
  T#39_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;89) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#39_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;90) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#39_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;91) shape:[24, 1024, 1280], type:FLOAT32
  T#39_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;92) shape:[24, 1280, 128], type:FLOAT32
  T#39_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;93) shape:[24, 1024, 128], type:FLOAT32
  T#39_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;94) shape:[1, 24, 1024, 128], type:FLOAT32
  T#39_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_0/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;95) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#40 odml.scaled_dot_product_attention.impl_37(T#40_0, T#40_1, T#40_2, T#40_3) -> [T#40_35]
  Op#0 TRANSPOSE(T#40_1, T#40_14) -> [T#40_15]
  Op#1 TRANSPOSE(T#40_2, T#40_14) -> [T#40_16]
  Op#2 RESHAPE(T#40_15, T#40_11) -> [T#40_17]
  Op#3 BROADCAST_TO(T#40_17, T#40_12) -> [T#40_18]
  Op#4 RESHAPE(T#40_18, T#40_10) -> [T#40_19]
  Op#5 RESHAPE(T#40_16, T#40_11) -> [T#40_20]
  Op#6 BROADCAST_TO(T#40_20, T#40_12) -> [T#40_21]
  Op#7 MUL(T#40_0, T#40_13) -> [T#40_22]
  Op#8 TRANSPOSE(T#40_22, T#40_14) -> [T#40_23]
  Op#9 MUL(T#40_19, T#40_13) -> [T#40_24]
  Op#10 TRANSPOSE(T#40_24, T#40_9) -> [T#40_25]
  Op#11 RESHAPE(T#40_23, T#40_8) -> [T#40_26]
  Op#12 RESHAPE(T#40_25, T#40_7) -> [T#40_27]
  Op#13 BATCH_MATMUL(T#40_26, T#40_27) -> [T#40_28]
  Op#14 ADD(T#40_28, T#40_3) -> [T#40_29]
  Op#15 SOFTMAX(T#40_29) -> [T#40_30]
  Op#16 RESHAPE(T#40_30, T#40_6) -> [T#40_31]
  Op#17 RESHAPE(T#40_21, T#40_5) -> [T#40_32]
  Op#18 BATCH_MATMUL(T#40_31, T#40_32) -> [T#40_33]
  Op#19 RESHAPE(T#40_33, T#40_4) -> [T#40_34]
  Op#20 TRANSPOSE(T#40_34, T#40_14) -> [T#40_35]

Tensors of Subgraph#40
  T#40_0(odml.scaled_dot_product_attention.impl_37_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#40_1(odml.scaled_dot_product_attention.impl_37_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#40_2(odml.scaled_dot_product_attention.impl_37_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#40_3(odml.scaled_dot_product_attention.impl_37_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#40_4(arith.constant865) shape:[4], type:INT32
  T#40_5(arith.constant866) shape:[3], type:INT32
  T#40_6(arith.constant867) shape:[3], type:INT32
  T#40_7(arith.constant868) shape:[3], type:INT32
  T#40_8(arith.constant869) shape:[3], type:INT32
  T#40_9(arith.constant870) shape:[4], type:INT32
  T#40_10(arith.constant871) shape:[4], type:INT32
  T#40_11(arith.constant872) shape:[5], type:INT32
  T#40_12(arith.constant873) shape:[5], type:INT64
  T#40_13(arith.constant874) shape:[], type:FLOAT32
  T#40_14(arith.constant875) shape:[4], type:INT32
  T#40_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#40_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#40_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#40_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#40_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#40_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#40_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#40_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#40_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#40_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#40_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#40_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#40_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#40_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#40_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#40_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#40_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#40_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#40_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#40_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#40_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_20/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#41 odml.scaled_dot_product_attention.impl_38(T#41_0, T#41_1, T#41_2, T#41_3) -> [T#41_35]
  Op#0 TRANSPOSE(T#41_1, T#41_14) -> [T#41_15]
  Op#1 TRANSPOSE(T#41_2, T#41_14) -> [T#41_16]
  Op#2 RESHAPE(T#41_15, T#41_11) -> [T#41_17]
  Op#3 BROADCAST_TO(T#41_17, T#41_12) -> [T#41_18]
  Op#4 RESHAPE(T#41_18, T#41_10) -> [T#41_19]
  Op#5 RESHAPE(T#41_16, T#41_11) -> [T#41_20]
  Op#6 BROADCAST_TO(T#41_20, T#41_12) -> [T#41_21]
  Op#7 MUL(T#41_0, T#41_13) -> [T#41_22]
  Op#8 TRANSPOSE(T#41_22, T#41_14) -> [T#41_23]
  Op#9 MUL(T#41_19, T#41_13) -> [T#41_24]
  Op#10 TRANSPOSE(T#41_24, T#41_9) -> [T#41_25]
  Op#11 RESHAPE(T#41_23, T#41_8) -> [T#41_26]
  Op#12 RESHAPE(T#41_25, T#41_7) -> [T#41_27]
  Op#13 BATCH_MATMUL(T#41_26, T#41_27) -> [T#41_28]
  Op#14 ADD(T#41_28, T#41_3) -> [T#41_29]
  Op#15 SOFTMAX(T#41_29) -> [T#41_30]
  Op#16 RESHAPE(T#41_30, T#41_6) -> [T#41_31]
  Op#17 RESHAPE(T#41_21, T#41_5) -> [T#41_32]
  Op#18 BATCH_MATMUL(T#41_31, T#41_32) -> [T#41_33]
  Op#19 RESHAPE(T#41_33, T#41_4) -> [T#41_34]
  Op#20 TRANSPOSE(T#41_34, T#41_14) -> [T#41_35]

Tensors of Subgraph#41
  T#41_0(odml.scaled_dot_product_attention.impl_38_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#41_1(odml.scaled_dot_product_attention.impl_38_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#41_2(odml.scaled_dot_product_attention.impl_38_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#41_3(odml.scaled_dot_product_attention.impl_38_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#41_4(arith.constant876) shape:[4], type:INT32
  T#41_5(arith.constant877) shape:[3], type:INT32
  T#41_6(arith.constant878) shape:[3], type:INT32
  T#41_7(arith.constant879) shape:[3], type:INT32
  T#41_8(arith.constant880) shape:[3], type:INT32
  T#41_9(arith.constant881) shape:[4], type:INT32
  T#41_10(arith.constant882) shape:[4], type:INT32
  T#41_11(arith.constant883) shape:[5], type:INT32
  T#41_12(arith.constant884) shape:[5], type:INT64
  T#41_13(arith.constant885) shape:[], type:FLOAT32
  T#41_14(arith.constant886) shape:[4], type:INT32
  T#41_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#41_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#41_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#41_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#41_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#41_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#41_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#41_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#41_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#41_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#41_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#41_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#41_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#41_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#41_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#41_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#41_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#41_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#41_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#41_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#41_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_1/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#42 odml.scaled_dot_product_attention.impl_39(T#42_0, T#42_1, T#42_2, T#42_3) -> [T#42_35]
  Op#0 TRANSPOSE(T#42_1, T#42_14) -> [T#42_15]
  Op#1 TRANSPOSE(T#42_2, T#42_14) -> [T#42_16]
  Op#2 RESHAPE(T#42_15, T#42_11) -> [T#42_17]
  Op#3 BROADCAST_TO(T#42_17, T#42_12) -> [T#42_18]
  Op#4 RESHAPE(T#42_18, T#42_10) -> [T#42_19]
  Op#5 RESHAPE(T#42_16, T#42_11) -> [T#42_20]
  Op#6 BROADCAST_TO(T#42_20, T#42_12) -> [T#42_21]
  Op#7 MUL(T#42_0, T#42_13) -> [T#42_22]
  Op#8 TRANSPOSE(T#42_22, T#42_14) -> [T#42_23]
  Op#9 MUL(T#42_19, T#42_13) -> [T#42_24]
  Op#10 TRANSPOSE(T#42_24, T#42_9) -> [T#42_25]
  Op#11 RESHAPE(T#42_23, T#42_8) -> [T#42_26]
  Op#12 RESHAPE(T#42_25, T#42_7) -> [T#42_27]
  Op#13 BATCH_MATMUL(T#42_26, T#42_27) -> [T#42_28]
  Op#14 ADD(T#42_28, T#42_3) -> [T#42_29]
  Op#15 SOFTMAX(T#42_29) -> [T#42_30]
  Op#16 RESHAPE(T#42_30, T#42_6) -> [T#42_31]
  Op#17 RESHAPE(T#42_21, T#42_5) -> [T#42_32]
  Op#18 BATCH_MATMUL(T#42_31, T#42_32) -> [T#42_33]
  Op#19 RESHAPE(T#42_33, T#42_4) -> [T#42_34]
  Op#20 TRANSPOSE(T#42_34, T#42_14) -> [T#42_35]

Tensors of Subgraph#42
  T#42_0(odml.scaled_dot_product_attention.impl_39_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#42_1(odml.scaled_dot_product_attention.impl_39_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#42_2(odml.scaled_dot_product_attention.impl_39_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#42_3(odml.scaled_dot_product_attention.impl_39_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#42_4(arith.constant887) shape:[4], type:INT32
  T#42_5(arith.constant888) shape:[3], type:INT32
  T#42_6(arith.constant889) shape:[3], type:INT32
  T#42_7(arith.constant890) shape:[3], type:INT32
  T#42_8(arith.constant891) shape:[3], type:INT32
  T#42_9(arith.constant892) shape:[4], type:INT32
  T#42_10(arith.constant893) shape:[4], type:INT32
  T#42_11(arith.constant894) shape:[5], type:INT32
  T#42_12(arith.constant895) shape:[5], type:INT64
  T#42_13(arith.constant896) shape:[], type:FLOAT32
  T#42_14(arith.constant897) shape:[4], type:INT32
  T#42_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#42_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#42_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#42_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#42_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#42_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#42_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#42_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#42_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#42_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#42_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#42_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#42_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#42_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#42_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#42_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#42_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#42_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#42_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#42_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#42_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_26/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#43 odml.scaled_dot_product_attention.impl_40(T#43_0, T#43_1, T#43_2, T#43_3) -> [T#43_35]
  Op#0 TRANSPOSE(T#43_1, T#43_14) -> [T#43_15]
  Op#1 TRANSPOSE(T#43_2, T#43_14) -> [T#43_16]
  Op#2 RESHAPE(T#43_15, T#43_11) -> [T#43_17]
  Op#3 BROADCAST_TO(T#43_17, T#43_12) -> [T#43_18]
  Op#4 RESHAPE(T#43_18, T#43_10) -> [T#43_19]
  Op#5 RESHAPE(T#43_16, T#43_11) -> [T#43_20]
  Op#6 BROADCAST_TO(T#43_20, T#43_12) -> [T#43_21]
  Op#7 MUL(T#43_0, T#43_13) -> [T#43_22]
  Op#8 TRANSPOSE(T#43_22, T#43_14) -> [T#43_23]
  Op#9 MUL(T#43_19, T#43_13) -> [T#43_24]
  Op#10 TRANSPOSE(T#43_24, T#43_9) -> [T#43_25]
  Op#11 RESHAPE(T#43_23, T#43_8) -> [T#43_26]
  Op#12 RESHAPE(T#43_25, T#43_7) -> [T#43_27]
  Op#13 BATCH_MATMUL(T#43_26, T#43_27) -> [T#43_28]
  Op#14 ADD(T#43_28, T#43_3) -> [T#43_29]
  Op#15 SOFTMAX(T#43_29) -> [T#43_30]
  Op#16 RESHAPE(T#43_30, T#43_6) -> [T#43_31]
  Op#17 RESHAPE(T#43_21, T#43_5) -> [T#43_32]
  Op#18 BATCH_MATMUL(T#43_31, T#43_32) -> [T#43_33]
  Op#19 RESHAPE(T#43_33, T#43_4) -> [T#43_34]
  Op#20 TRANSPOSE(T#43_34, T#43_14) -> [T#43_35]

Tensors of Subgraph#43
  T#43_0(odml.scaled_dot_product_attention.impl_40_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#43_1(odml.scaled_dot_product_attention.impl_40_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#43_2(odml.scaled_dot_product_attention.impl_40_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#43_3(odml.scaled_dot_product_attention.impl_40_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#43_4(arith.constant898) shape:[4], type:INT32
  T#43_5(arith.constant899) shape:[3], type:INT32
  T#43_6(arith.constant900) shape:[3], type:INT32
  T#43_7(arith.constant901) shape:[3], type:INT32
  T#43_8(arith.constant902) shape:[3], type:INT32
  T#43_9(arith.constant903) shape:[4], type:INT32
  T#43_10(arith.constant904) shape:[4], type:INT32
  T#43_11(arith.constant905) shape:[5], type:INT32
  T#43_12(arith.constant906) shape:[5], type:INT64
  T#43_13(arith.constant907) shape:[], type:FLOAT32
  T#43_14(arith.constant908) shape:[4], type:INT32
  T#43_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#43_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#43_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#43_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#43_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#43_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#43_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#43_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#43_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#43_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#43_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#43_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#43_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#43_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#43_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#43_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#43_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#43_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#43_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#43_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#43_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_2/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#44 odml.scaled_dot_product_attention.impl_41(T#44_0, T#44_1, T#44_2, T#44_3) -> [T#44_35]
  Op#0 TRANSPOSE(T#44_1, T#44_14) -> [T#44_15]
  Op#1 TRANSPOSE(T#44_2, T#44_14) -> [T#44_16]
  Op#2 RESHAPE(T#44_15, T#44_11) -> [T#44_17]
  Op#3 BROADCAST_TO(T#44_17, T#44_12) -> [T#44_18]
  Op#4 RESHAPE(T#44_18, T#44_10) -> [T#44_19]
  Op#5 RESHAPE(T#44_16, T#44_11) -> [T#44_20]
  Op#6 BROADCAST_TO(T#44_20, T#44_12) -> [T#44_21]
  Op#7 MUL(T#44_0, T#44_13) -> [T#44_22]
  Op#8 TRANSPOSE(T#44_22, T#44_14) -> [T#44_23]
  Op#9 MUL(T#44_19, T#44_13) -> [T#44_24]
  Op#10 TRANSPOSE(T#44_24, T#44_9) -> [T#44_25]
  Op#11 RESHAPE(T#44_23, T#44_8) -> [T#44_26]
  Op#12 RESHAPE(T#44_25, T#44_7) -> [T#44_27]
  Op#13 BATCH_MATMUL(T#44_26, T#44_27) -> [T#44_28]
  Op#14 ADD(T#44_28, T#44_3) -> [T#44_29]
  Op#15 SOFTMAX(T#44_29) -> [T#44_30]
  Op#16 RESHAPE(T#44_30, T#44_6) -> [T#44_31]
  Op#17 RESHAPE(T#44_21, T#44_5) -> [T#44_32]
  Op#18 BATCH_MATMUL(T#44_31, T#44_32) -> [T#44_33]
  Op#19 RESHAPE(T#44_33, T#44_4) -> [T#44_34]
  Op#20 TRANSPOSE(T#44_34, T#44_14) -> [T#44_35]

Tensors of Subgraph#44
  T#44_0(odml.scaled_dot_product_attention.impl_41_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#44_1(odml.scaled_dot_product_attention.impl_41_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#44_2(odml.scaled_dot_product_attention.impl_41_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#44_3(odml.scaled_dot_product_attention.impl_41_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#44_4(arith.constant909) shape:[4], type:INT32
  T#44_5(arith.constant910) shape:[3], type:INT32
  T#44_6(arith.constant911) shape:[3], type:INT32
  T#44_7(arith.constant912) shape:[3], type:INT32
  T#44_8(arith.constant913) shape:[3], type:INT32
  T#44_9(arith.constant914) shape:[4], type:INT32
  T#44_10(arith.constant915) shape:[4], type:INT32
  T#44_11(arith.constant916) shape:[5], type:INT32
  T#44_12(arith.constant917) shape:[5], type:INT64
  T#44_13(arith.constant918) shape:[], type:FLOAT32
  T#44_14(arith.constant919) shape:[4], type:INT32
  T#44_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#44_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#44_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#44_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#44_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#44_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#44_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#44_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#44_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#44_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#44_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#44_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#44_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#44_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#44_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#44_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#44_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#44_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#44_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#44_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#44_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_21/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#45 odml.scaled_dot_product_attention.impl_42(T#45_0, T#45_1, T#45_2, T#45_3) -> [T#45_35]
  Op#0 TRANSPOSE(T#45_1, T#45_14) -> [T#45_15]
  Op#1 TRANSPOSE(T#45_2, T#45_14) -> [T#45_16]
  Op#2 RESHAPE(T#45_15, T#45_11) -> [T#45_17]
  Op#3 BROADCAST_TO(T#45_17, T#45_12) -> [T#45_18]
  Op#4 RESHAPE(T#45_18, T#45_10) -> [T#45_19]
  Op#5 RESHAPE(T#45_16, T#45_11) -> [T#45_20]
  Op#6 BROADCAST_TO(T#45_20, T#45_12) -> [T#45_21]
  Op#7 MUL(T#45_0, T#45_13) -> [T#45_22]
  Op#8 TRANSPOSE(T#45_22, T#45_14) -> [T#45_23]
  Op#9 MUL(T#45_19, T#45_13) -> [T#45_24]
  Op#10 TRANSPOSE(T#45_24, T#45_9) -> [T#45_25]
  Op#11 RESHAPE(T#45_23, T#45_8) -> [T#45_26]
  Op#12 RESHAPE(T#45_25, T#45_7) -> [T#45_27]
  Op#13 BATCH_MATMUL(T#45_26, T#45_27) -> [T#45_28]
  Op#14 ADD(T#45_28, T#45_3) -> [T#45_29]
  Op#15 SOFTMAX(T#45_29) -> [T#45_30]
  Op#16 RESHAPE(T#45_30, T#45_6) -> [T#45_31]
  Op#17 RESHAPE(T#45_21, T#45_5) -> [T#45_32]
  Op#18 BATCH_MATMUL(T#45_31, T#45_32) -> [T#45_33]
  Op#19 RESHAPE(T#45_33, T#45_4) -> [T#45_34]
  Op#20 TRANSPOSE(T#45_34, T#45_14) -> [T#45_35]

Tensors of Subgraph#45
  T#45_0(odml.scaled_dot_product_attention.impl_42_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#45_1(odml.scaled_dot_product_attention.impl_42_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#45_2(odml.scaled_dot_product_attention.impl_42_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#45_3(odml.scaled_dot_product_attention.impl_42_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#45_4(arith.constant920) shape:[4], type:INT32
  T#45_5(arith.constant921) shape:[3], type:INT32
  T#45_6(arith.constant922) shape:[3], type:INT32
  T#45_7(arith.constant923) shape:[3], type:INT32
  T#45_8(arith.constant924) shape:[3], type:INT32
  T#45_9(arith.constant925) shape:[4], type:INT32
  T#45_10(arith.constant926) shape:[4], type:INT32
  T#45_11(arith.constant927) shape:[5], type:INT32
  T#45_12(arith.constant928) shape:[5], type:INT64
  T#45_13(arith.constant929) shape:[], type:FLOAT32
  T#45_14(arith.constant930) shape:[4], type:INT32
  T#45_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#45_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#45_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#45_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#45_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#45_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#45_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#45_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#45_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#45_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#45_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#45_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#45_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#45_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#45_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#45_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#45_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#45_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#45_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#45_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#45_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_3/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#46 odml.scaled_dot_product_attention.impl_43(T#46_0, T#46_1, T#46_2, T#46_3) -> [T#46_35]
  Op#0 TRANSPOSE(T#46_1, T#46_14) -> [T#46_15]
  Op#1 TRANSPOSE(T#46_2, T#46_14) -> [T#46_16]
  Op#2 RESHAPE(T#46_15, T#46_11) -> [T#46_17]
  Op#3 BROADCAST_TO(T#46_17, T#46_12) -> [T#46_18]
  Op#4 RESHAPE(T#46_18, T#46_10) -> [T#46_19]
  Op#5 RESHAPE(T#46_16, T#46_11) -> [T#46_20]
  Op#6 BROADCAST_TO(T#46_20, T#46_12) -> [T#46_21]
  Op#7 MUL(T#46_0, T#46_13) -> [T#46_22]
  Op#8 TRANSPOSE(T#46_22, T#46_14) -> [T#46_23]
  Op#9 MUL(T#46_19, T#46_13) -> [T#46_24]
  Op#10 TRANSPOSE(T#46_24, T#46_9) -> [T#46_25]
  Op#11 RESHAPE(T#46_23, T#46_8) -> [T#46_26]
  Op#12 RESHAPE(T#46_25, T#46_7) -> [T#46_27]
  Op#13 BATCH_MATMUL(T#46_26, T#46_27) -> [T#46_28]
  Op#14 ADD(T#46_28, T#46_3) -> [T#46_29]
  Op#15 SOFTMAX(T#46_29) -> [T#46_30]
  Op#16 RESHAPE(T#46_30, T#46_6) -> [T#46_31]
  Op#17 RESHAPE(T#46_21, T#46_5) -> [T#46_32]
  Op#18 BATCH_MATMUL(T#46_31, T#46_32) -> [T#46_33]
  Op#19 RESHAPE(T#46_33, T#46_4) -> [T#46_34]
  Op#20 TRANSPOSE(T#46_34, T#46_14) -> [T#46_35]

Tensors of Subgraph#46
  T#46_0(odml.scaled_dot_product_attention.impl_43_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#46_1(odml.scaled_dot_product_attention.impl_43_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#46_2(odml.scaled_dot_product_attention.impl_43_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#46_3(odml.scaled_dot_product_attention.impl_43_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#46_4(arith.constant931) shape:[4], type:INT32
  T#46_5(arith.constant932) shape:[3], type:INT32
  T#46_6(arith.constant933) shape:[3], type:INT32
  T#46_7(arith.constant934) shape:[3], type:INT32
  T#46_8(arith.constant935) shape:[3], type:INT32
  T#46_9(arith.constant936) shape:[4], type:INT32
  T#46_10(arith.constant937) shape:[4], type:INT32
  T#46_11(arith.constant938) shape:[5], type:INT32
  T#46_12(arith.constant939) shape:[5], type:INT64
  T#46_13(arith.constant940) shape:[], type:FLOAT32
  T#46_14(arith.constant941) shape:[4], type:INT32
  T#46_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#46_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#46_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#46_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#46_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#46_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#46_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#46_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#46_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#46_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#46_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#46_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#46_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#46_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#46_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#46_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#46_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#46_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#46_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#46_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#46_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_4/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#47 odml.scaled_dot_product_attention.impl_44(T#47_0, T#47_1, T#47_2, T#47_3) -> [T#47_35]
  Op#0 TRANSPOSE(T#47_1, T#47_14) -> [T#47_15]
  Op#1 TRANSPOSE(T#47_2, T#47_14) -> [T#47_16]
  Op#2 RESHAPE(T#47_15, T#47_11) -> [T#47_17]
  Op#3 BROADCAST_TO(T#47_17, T#47_12) -> [T#47_18]
  Op#4 RESHAPE(T#47_18, T#47_10) -> [T#47_19]
  Op#5 RESHAPE(T#47_16, T#47_11) -> [T#47_20]
  Op#6 BROADCAST_TO(T#47_20, T#47_12) -> [T#47_21]
  Op#7 MUL(T#47_0, T#47_13) -> [T#47_22]
  Op#8 TRANSPOSE(T#47_22, T#47_14) -> [T#47_23]
  Op#9 MUL(T#47_19, T#47_13) -> [T#47_24]
  Op#10 TRANSPOSE(T#47_24, T#47_9) -> [T#47_25]
  Op#11 RESHAPE(T#47_23, T#47_8) -> [T#47_26]
  Op#12 RESHAPE(T#47_25, T#47_7) -> [T#47_27]
  Op#13 BATCH_MATMUL(T#47_26, T#47_27) -> [T#47_28]
  Op#14 ADD(T#47_28, T#47_3) -> [T#47_29]
  Op#15 SOFTMAX(T#47_29) -> [T#47_30]
  Op#16 RESHAPE(T#47_30, T#47_6) -> [T#47_31]
  Op#17 RESHAPE(T#47_21, T#47_5) -> [T#47_32]
  Op#18 BATCH_MATMUL(T#47_31, T#47_32) -> [T#47_33]
  Op#19 RESHAPE(T#47_33, T#47_4) -> [T#47_34]
  Op#20 TRANSPOSE(T#47_34, T#47_14) -> [T#47_35]

Tensors of Subgraph#47
  T#47_0(odml.scaled_dot_product_attention.impl_44_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#47_1(odml.scaled_dot_product_attention.impl_44_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#47_2(odml.scaled_dot_product_attention.impl_44_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#47_3(odml.scaled_dot_product_attention.impl_44_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#47_4(arith.constant942) shape:[4], type:INT32
  T#47_5(arith.constant943) shape:[3], type:INT32
  T#47_6(arith.constant944) shape:[3], type:INT32
  T#47_7(arith.constant945) shape:[3], type:INT32
  T#47_8(arith.constant946) shape:[3], type:INT32
  T#47_9(arith.constant947) shape:[4], type:INT32
  T#47_10(arith.constant948) shape:[4], type:INT32
  T#47_11(arith.constant949) shape:[5], type:INT32
  T#47_12(arith.constant950) shape:[5], type:INT64
  T#47_13(arith.constant951) shape:[], type:FLOAT32
  T#47_14(arith.constant952) shape:[4], type:INT32
  T#47_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#47_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#47_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#47_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#47_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#47_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#47_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#47_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#47_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#47_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#47_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#47_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#47_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#47_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#47_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#47_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#47_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#47_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#47_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#47_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#47_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_19/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#48 odml.scaled_dot_product_attention.impl_45(T#48_0, T#48_1, T#48_2, T#48_3) -> [T#48_35]
  Op#0 TRANSPOSE(T#48_1, T#48_14) -> [T#48_15]
  Op#1 TRANSPOSE(T#48_2, T#48_14) -> [T#48_16]
  Op#2 RESHAPE(T#48_15, T#48_11) -> [T#48_17]
  Op#3 BROADCAST_TO(T#48_17, T#48_12) -> [T#48_18]
  Op#4 RESHAPE(T#48_18, T#48_10) -> [T#48_19]
  Op#5 RESHAPE(T#48_16, T#48_11) -> [T#48_20]
  Op#6 BROADCAST_TO(T#48_20, T#48_12) -> [T#48_21]
  Op#7 MUL(T#48_0, T#48_13) -> [T#48_22]
  Op#8 TRANSPOSE(T#48_22, T#48_14) -> [T#48_23]
  Op#9 MUL(T#48_19, T#48_13) -> [T#48_24]
  Op#10 TRANSPOSE(T#48_24, T#48_9) -> [T#48_25]
  Op#11 RESHAPE(T#48_23, T#48_8) -> [T#48_26]
  Op#12 RESHAPE(T#48_25, T#48_7) -> [T#48_27]
  Op#13 BATCH_MATMUL(T#48_26, T#48_27) -> [T#48_28]
  Op#14 ADD(T#48_28, T#48_3) -> [T#48_29]
  Op#15 SOFTMAX(T#48_29) -> [T#48_30]
  Op#16 RESHAPE(T#48_30, T#48_6) -> [T#48_31]
  Op#17 RESHAPE(T#48_21, T#48_5) -> [T#48_32]
  Op#18 BATCH_MATMUL(T#48_31, T#48_32) -> [T#48_33]
  Op#19 RESHAPE(T#48_33, T#48_4) -> [T#48_34]
  Op#20 TRANSPOSE(T#48_34, T#48_14) -> [T#48_35]

Tensors of Subgraph#48
  T#48_0(odml.scaled_dot_product_attention.impl_45_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#48_1(odml.scaled_dot_product_attention.impl_45_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#48_2(odml.scaled_dot_product_attention.impl_45_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#48_3(odml.scaled_dot_product_attention.impl_45_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#48_4(arith.constant953) shape:[4], type:INT32
  T#48_5(arith.constant954) shape:[3], type:INT32
  T#48_6(arith.constant955) shape:[3], type:INT32
  T#48_7(arith.constant956) shape:[3], type:INT32
  T#48_8(arith.constant957) shape:[3], type:INT32
  T#48_9(arith.constant958) shape:[4], type:INT32
  T#48_10(arith.constant959) shape:[4], type:INT32
  T#48_11(arith.constant960) shape:[5], type:INT32
  T#48_12(arith.constant961) shape:[5], type:INT64
  T#48_13(arith.constant962) shape:[], type:FLOAT32
  T#48_14(arith.constant963) shape:[4], type:INT32
  T#48_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#48_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#48_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#48_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#48_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#48_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#48_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#48_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#48_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#48_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#48_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#48_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#48_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#48_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#48_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#48_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#48_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#48_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#48_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#48_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#48_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_5/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#49 odml.scaled_dot_product_attention.impl_46(T#49_0, T#49_1, T#49_2, T#49_3) -> [T#49_35]
  Op#0 TRANSPOSE(T#49_1, T#49_14) -> [T#49_15]
  Op#1 TRANSPOSE(T#49_2, T#49_14) -> [T#49_16]
  Op#2 RESHAPE(T#49_15, T#49_11) -> [T#49_17]
  Op#3 BROADCAST_TO(T#49_17, T#49_12) -> [T#49_18]
  Op#4 RESHAPE(T#49_18, T#49_10) -> [T#49_19]
  Op#5 RESHAPE(T#49_16, T#49_11) -> [T#49_20]
  Op#6 BROADCAST_TO(T#49_20, T#49_12) -> [T#49_21]
  Op#7 MUL(T#49_0, T#49_13) -> [T#49_22]
  Op#8 TRANSPOSE(T#49_22, T#49_14) -> [T#49_23]
  Op#9 MUL(T#49_19, T#49_13) -> [T#49_24]
  Op#10 TRANSPOSE(T#49_24, T#49_9) -> [T#49_25]
  Op#11 RESHAPE(T#49_23, T#49_8) -> [T#49_26]
  Op#12 RESHAPE(T#49_25, T#49_7) -> [T#49_27]
  Op#13 BATCH_MATMUL(T#49_26, T#49_27) -> [T#49_28]
  Op#14 ADD(T#49_28, T#49_3) -> [T#49_29]
  Op#15 SOFTMAX(T#49_29) -> [T#49_30]
  Op#16 RESHAPE(T#49_30, T#49_6) -> [T#49_31]
  Op#17 RESHAPE(T#49_21, T#49_5) -> [T#49_32]
  Op#18 BATCH_MATMUL(T#49_31, T#49_32) -> [T#49_33]
  Op#19 RESHAPE(T#49_33, T#49_4) -> [T#49_34]
  Op#20 TRANSPOSE(T#49_34, T#49_14) -> [T#49_35]

Tensors of Subgraph#49
  T#49_0(odml.scaled_dot_product_attention.impl_46_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#49_1(odml.scaled_dot_product_attention.impl_46_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#49_2(odml.scaled_dot_product_attention.impl_46_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#49_3(odml.scaled_dot_product_attention.impl_46_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#49_4(arith.constant964) shape:[4], type:INT32
  T#49_5(arith.constant965) shape:[3], type:INT32
  T#49_6(arith.constant966) shape:[3], type:INT32
  T#49_7(arith.constant967) shape:[3], type:INT32
  T#49_8(arith.constant968) shape:[3], type:INT32
  T#49_9(arith.constant969) shape:[4], type:INT32
  T#49_10(arith.constant970) shape:[4], type:INT32
  T#49_11(arith.constant971) shape:[5], type:INT32
  T#49_12(arith.constant972) shape:[5], type:INT64
  T#49_13(arith.constant973) shape:[], type:FLOAT32
  T#49_14(arith.constant974) shape:[4], type:INT32
  T#49_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#49_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#49_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#49_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#49_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#49_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#49_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#49_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#49_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#49_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#49_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#49_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#49_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#49_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#49_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#49_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#49_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#49_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#49_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#49_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#49_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_9/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#50 odml.scaled_dot_product_attention.impl_47(T#50_0, T#50_1, T#50_2, T#50_3) -> [T#50_35]
  Op#0 TRANSPOSE(T#50_1, T#50_14) -> [T#50_15]
  Op#1 TRANSPOSE(T#50_2, T#50_14) -> [T#50_16]
  Op#2 RESHAPE(T#50_15, T#50_11) -> [T#50_17]
  Op#3 BROADCAST_TO(T#50_17, T#50_12) -> [T#50_18]
  Op#4 RESHAPE(T#50_18, T#50_10) -> [T#50_19]
  Op#5 RESHAPE(T#50_16, T#50_11) -> [T#50_20]
  Op#6 BROADCAST_TO(T#50_20, T#50_12) -> [T#50_21]
  Op#7 MUL(T#50_0, T#50_13) -> [T#50_22]
  Op#8 TRANSPOSE(T#50_22, T#50_14) -> [T#50_23]
  Op#9 MUL(T#50_19, T#50_13) -> [T#50_24]
  Op#10 TRANSPOSE(T#50_24, T#50_9) -> [T#50_25]
  Op#11 RESHAPE(T#50_23, T#50_8) -> [T#50_26]
  Op#12 RESHAPE(T#50_25, T#50_7) -> [T#50_27]
  Op#13 BATCH_MATMUL(T#50_26, T#50_27) -> [T#50_28]
  Op#14 ADD(T#50_28, T#50_3) -> [T#50_29]
  Op#15 SOFTMAX(T#50_29) -> [T#50_30]
  Op#16 RESHAPE(T#50_30, T#50_6) -> [T#50_31]
  Op#17 RESHAPE(T#50_21, T#50_5) -> [T#50_32]
  Op#18 BATCH_MATMUL(T#50_31, T#50_32) -> [T#50_33]
  Op#19 RESHAPE(T#50_33, T#50_4) -> [T#50_34]
  Op#20 TRANSPOSE(T#50_34, T#50_14) -> [T#50_35]

Tensors of Subgraph#50
  T#50_0(odml.scaled_dot_product_attention.impl_47_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#50_1(odml.scaled_dot_product_attention.impl_47_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#50_2(odml.scaled_dot_product_attention.impl_47_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#50_3(odml.scaled_dot_product_attention.impl_47_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#50_4(arith.constant975) shape:[4], type:INT32
  T#50_5(arith.constant976) shape:[3], type:INT32
  T#50_6(arith.constant977) shape:[3], type:INT32
  T#50_7(arith.constant978) shape:[3], type:INT32
  T#50_8(arith.constant979) shape:[3], type:INT32
  T#50_9(arith.constant980) shape:[4], type:INT32
  T#50_10(arith.constant981) shape:[4], type:INT32
  T#50_11(arith.constant982) shape:[5], type:INT32
  T#50_12(arith.constant983) shape:[5], type:INT64
  T#50_13(arith.constant984) shape:[], type:FLOAT32
  T#50_14(arith.constant985) shape:[4], type:INT32
  T#50_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#50_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#50_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#50_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#50_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#50_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#50_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#50_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#50_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#50_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#50_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#50_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#50_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#50_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#50_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#50_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#50_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#50_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#50_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#50_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#50_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_14/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#51 odml.scaled_dot_product_attention.impl_48(T#51_0, T#51_1, T#51_2, T#51_3) -> [T#51_35]
  Op#0 TRANSPOSE(T#51_1, T#51_14) -> [T#51_15]
  Op#1 TRANSPOSE(T#51_2, T#51_14) -> [T#51_16]
  Op#2 RESHAPE(T#51_15, T#51_11) -> [T#51_17]
  Op#3 BROADCAST_TO(T#51_17, T#51_12) -> [T#51_18]
  Op#4 RESHAPE(T#51_18, T#51_10) -> [T#51_19]
  Op#5 RESHAPE(T#51_16, T#51_11) -> [T#51_20]
  Op#6 BROADCAST_TO(T#51_20, T#51_12) -> [T#51_21]
  Op#7 MUL(T#51_0, T#51_13) -> [T#51_22]
  Op#8 TRANSPOSE(T#51_22, T#51_14) -> [T#51_23]
  Op#9 MUL(T#51_19, T#51_13) -> [T#51_24]
  Op#10 TRANSPOSE(T#51_24, T#51_9) -> [T#51_25]
  Op#11 RESHAPE(T#51_23, T#51_8) -> [T#51_26]
  Op#12 RESHAPE(T#51_25, T#51_7) -> [T#51_27]
  Op#13 BATCH_MATMUL(T#51_26, T#51_27) -> [T#51_28]
  Op#14 ADD(T#51_28, T#51_3) -> [T#51_29]
  Op#15 SOFTMAX(T#51_29) -> [T#51_30]
  Op#16 RESHAPE(T#51_30, T#51_6) -> [T#51_31]
  Op#17 RESHAPE(T#51_21, T#51_5) -> [T#51_32]
  Op#18 BATCH_MATMUL(T#51_31, T#51_32) -> [T#51_33]
  Op#19 RESHAPE(T#51_33, T#51_4) -> [T#51_34]
  Op#20 TRANSPOSE(T#51_34, T#51_14) -> [T#51_35]

Tensors of Subgraph#51
  T#51_0(odml.scaled_dot_product_attention.impl_48_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#51_1(odml.scaled_dot_product_attention.impl_48_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#51_2(odml.scaled_dot_product_attention.impl_48_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#51_3(odml.scaled_dot_product_attention.impl_48_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#51_4(arith.constant986) shape:[4], type:INT32
  T#51_5(arith.constant987) shape:[3], type:INT32
  T#51_6(arith.constant988) shape:[3], type:INT32
  T#51_7(arith.constant989) shape:[3], type:INT32
  T#51_8(arith.constant990) shape:[3], type:INT32
  T#51_9(arith.constant991) shape:[4], type:INT32
  T#51_10(arith.constant992) shape:[4], type:INT32
  T#51_11(arith.constant993) shape:[5], type:INT32
  T#51_12(arith.constant994) shape:[5], type:INT64
  T#51_13(arith.constant995) shape:[], type:FLOAT32
  T#51_14(arith.constant996) shape:[4], type:INT32
  T#51_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#51_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#51_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#51_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#51_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#51_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#51_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#51_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#51_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#51_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#51_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#51_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#51_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#51_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#51_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#51_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#51_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#51_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#51_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#51_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#51_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_6/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#52 odml.scaled_dot_product_attention.impl_49(T#52_0, T#52_1, T#52_2, T#52_3) -> [T#52_35]
  Op#0 TRANSPOSE(T#52_1, T#52_14) -> [T#52_15]
  Op#1 TRANSPOSE(T#52_2, T#52_14) -> [T#52_16]
  Op#2 RESHAPE(T#52_15, T#52_11) -> [T#52_17]
  Op#3 BROADCAST_TO(T#52_17, T#52_12) -> [T#52_18]
  Op#4 RESHAPE(T#52_18, T#52_10) -> [T#52_19]
  Op#5 RESHAPE(T#52_16, T#52_11) -> [T#52_20]
  Op#6 BROADCAST_TO(T#52_20, T#52_12) -> [T#52_21]
  Op#7 MUL(T#52_0, T#52_13) -> [T#52_22]
  Op#8 TRANSPOSE(T#52_22, T#52_14) -> [T#52_23]
  Op#9 MUL(T#52_19, T#52_13) -> [T#52_24]
  Op#10 TRANSPOSE(T#52_24, T#52_9) -> [T#52_25]
  Op#11 RESHAPE(T#52_23, T#52_8) -> [T#52_26]
  Op#12 RESHAPE(T#52_25, T#52_7) -> [T#52_27]
  Op#13 BATCH_MATMUL(T#52_26, T#52_27) -> [T#52_28]
  Op#14 ADD(T#52_28, T#52_3) -> [T#52_29]
  Op#15 SOFTMAX(T#52_29) -> [T#52_30]
  Op#16 RESHAPE(T#52_30, T#52_6) -> [T#52_31]
  Op#17 RESHAPE(T#52_21, T#52_5) -> [T#52_32]
  Op#18 BATCH_MATMUL(T#52_31, T#52_32) -> [T#52_33]
  Op#19 RESHAPE(T#52_33, T#52_4) -> [T#52_34]
  Op#20 TRANSPOSE(T#52_34, T#52_14) -> [T#52_35]

Tensors of Subgraph#52
  T#52_0(odml.scaled_dot_product_attention.impl_49_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#52_1(odml.scaled_dot_product_attention.impl_49_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#52_2(odml.scaled_dot_product_attention.impl_49_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#52_3(odml.scaled_dot_product_attention.impl_49_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#52_4(arith.constant997) shape:[4], type:INT32
  T#52_5(arith.constant998) shape:[3], type:INT32
  T#52_6(arith.constant999) shape:[3], type:INT32
  T#52_7(arith.constant1000) shape:[3], type:INT32
  T#52_8(arith.constant1001) shape:[3], type:INT32
  T#52_9(arith.constant1002) shape:[4], type:INT32
  T#52_10(arith.constant1003) shape:[4], type:INT32
  T#52_11(arith.constant1004) shape:[5], type:INT32
  T#52_12(arith.constant1005) shape:[5], type:INT64
  T#52_13(arith.constant1006) shape:[], type:FLOAT32
  T#52_14(arith.constant1007) shape:[4], type:INT32
  T#52_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#52_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#52_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#52_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#52_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#52_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#52_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#52_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#52_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#52_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#52_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#52_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#52_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#52_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#52_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#52_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#52_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#52_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#52_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#52_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#52_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_13/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#53 odml.scaled_dot_product_attention.impl_50(T#53_0, T#53_1, T#53_2, T#53_3) -> [T#53_35]
  Op#0 TRANSPOSE(T#53_1, T#53_14) -> [T#53_15]
  Op#1 TRANSPOSE(T#53_2, T#53_14) -> [T#53_16]
  Op#2 RESHAPE(T#53_15, T#53_11) -> [T#53_17]
  Op#3 BROADCAST_TO(T#53_17, T#53_12) -> [T#53_18]
  Op#4 RESHAPE(T#53_18, T#53_10) -> [T#53_19]
  Op#5 RESHAPE(T#53_16, T#53_11) -> [T#53_20]
  Op#6 BROADCAST_TO(T#53_20, T#53_12) -> [T#53_21]
  Op#7 MUL(T#53_0, T#53_13) -> [T#53_22]
  Op#8 TRANSPOSE(T#53_22, T#53_14) -> [T#53_23]
  Op#9 MUL(T#53_19, T#53_13) -> [T#53_24]
  Op#10 TRANSPOSE(T#53_24, T#53_9) -> [T#53_25]
  Op#11 RESHAPE(T#53_23, T#53_8) -> [T#53_26]
  Op#12 RESHAPE(T#53_25, T#53_7) -> [T#53_27]
  Op#13 BATCH_MATMUL(T#53_26, T#53_27) -> [T#53_28]
  Op#14 ADD(T#53_28, T#53_3) -> [T#53_29]
  Op#15 SOFTMAX(T#53_29) -> [T#53_30]
  Op#16 RESHAPE(T#53_30, T#53_6) -> [T#53_31]
  Op#17 RESHAPE(T#53_21, T#53_5) -> [T#53_32]
  Op#18 BATCH_MATMUL(T#53_31, T#53_32) -> [T#53_33]
  Op#19 RESHAPE(T#53_33, T#53_4) -> [T#53_34]
  Op#20 TRANSPOSE(T#53_34, T#53_14) -> [T#53_35]

Tensors of Subgraph#53
  T#53_0(odml.scaled_dot_product_attention.impl_50_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#53_1(odml.scaled_dot_product_attention.impl_50_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#53_2(odml.scaled_dot_product_attention.impl_50_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#53_3(odml.scaled_dot_product_attention.impl_50_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#53_4(arith.constant1008) shape:[4], type:INT32
  T#53_5(arith.constant1009) shape:[3], type:INT32
  T#53_6(arith.constant1010) shape:[3], type:INT32
  T#53_7(arith.constant1011) shape:[3], type:INT32
  T#53_8(arith.constant1012) shape:[3], type:INT32
  T#53_9(arith.constant1013) shape:[4], type:INT32
  T#53_10(arith.constant1014) shape:[4], type:INT32
  T#53_11(arith.constant1015) shape:[5], type:INT32
  T#53_12(arith.constant1016) shape:[5], type:INT64
  T#53_13(arith.constant1017) shape:[], type:FLOAT32
  T#53_14(arith.constant1018) shape:[4], type:INT32
  T#53_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#53_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#53_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#53_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#53_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#53_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#53_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#53_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#53_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#53_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#53_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#53_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#53_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#53_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#53_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#53_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#53_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#53_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#53_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#53_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#53_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_7/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#54 odml.scaled_dot_product_attention.impl_51(T#54_0, T#54_1, T#54_2, T#54_3) -> [T#54_35]
  Op#0 TRANSPOSE(T#54_1, T#54_14) -> [T#54_15]
  Op#1 TRANSPOSE(T#54_2, T#54_14) -> [T#54_16]
  Op#2 RESHAPE(T#54_15, T#54_11) -> [T#54_17]
  Op#3 BROADCAST_TO(T#54_17, T#54_12) -> [T#54_18]
  Op#4 RESHAPE(T#54_18, T#54_10) -> [T#54_19]
  Op#5 RESHAPE(T#54_16, T#54_11) -> [T#54_20]
  Op#6 BROADCAST_TO(T#54_20, T#54_12) -> [T#54_21]
  Op#7 MUL(T#54_0, T#54_13) -> [T#54_22]
  Op#8 TRANSPOSE(T#54_22, T#54_14) -> [T#54_23]
  Op#9 MUL(T#54_19, T#54_13) -> [T#54_24]
  Op#10 TRANSPOSE(T#54_24, T#54_9) -> [T#54_25]
  Op#11 RESHAPE(T#54_23, T#54_8) -> [T#54_26]
  Op#12 RESHAPE(T#54_25, T#54_7) -> [T#54_27]
  Op#13 BATCH_MATMUL(T#54_26, T#54_27) -> [T#54_28]
  Op#14 ADD(T#54_28, T#54_3) -> [T#54_29]
  Op#15 SOFTMAX(T#54_29) -> [T#54_30]
  Op#16 RESHAPE(T#54_30, T#54_6) -> [T#54_31]
  Op#17 RESHAPE(T#54_21, T#54_5) -> [T#54_32]
  Op#18 BATCH_MATMUL(T#54_31, T#54_32) -> [T#54_33]
  Op#19 RESHAPE(T#54_33, T#54_4) -> [T#54_34]
  Op#20 TRANSPOSE(T#54_34, T#54_14) -> [T#54_35]

Tensors of Subgraph#54
  T#54_0(odml.scaled_dot_product_attention.impl_51_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#54_1(odml.scaled_dot_product_attention.impl_51_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#54_2(odml.scaled_dot_product_attention.impl_51_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#54_3(odml.scaled_dot_product_attention.impl_51_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#54_4(arith.constant1019) shape:[4], type:INT32
  T#54_5(arith.constant1020) shape:[3], type:INT32
  T#54_6(arith.constant1021) shape:[3], type:INT32
  T#54_7(arith.constant1022) shape:[3], type:INT32
  T#54_8(arith.constant1023) shape:[3], type:INT32
  T#54_9(arith.constant1024) shape:[4], type:INT32
  T#54_10(arith.constant1025) shape:[4], type:INT32
  T#54_11(arith.constant1026) shape:[5], type:INT32
  T#54_12(arith.constant1027) shape:[5], type:INT64
  T#54_13(arith.constant1028) shape:[], type:FLOAT32
  T#54_14(arith.constant1029) shape:[4], type:INT32
  T#54_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#54_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#54_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#54_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#54_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#54_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#54_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#54_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#54_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#54_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#54_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#54_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#54_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#54_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#54_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#54_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#54_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#54_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#54_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#54_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#54_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_8/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#55 odml.scaled_dot_product_attention.impl_52(T#55_0, T#55_1, T#55_2, T#55_3) -> [T#55_35]
  Op#0 TRANSPOSE(T#55_1, T#55_14) -> [T#55_15]
  Op#1 TRANSPOSE(T#55_2, T#55_14) -> [T#55_16]
  Op#2 RESHAPE(T#55_15, T#55_11) -> [T#55_17]
  Op#3 BROADCAST_TO(T#55_17, T#55_12) -> [T#55_18]
  Op#4 RESHAPE(T#55_18, T#55_10) -> [T#55_19]
  Op#5 RESHAPE(T#55_16, T#55_11) -> [T#55_20]
  Op#6 BROADCAST_TO(T#55_20, T#55_12) -> [T#55_21]
  Op#7 MUL(T#55_0, T#55_13) -> [T#55_22]
  Op#8 TRANSPOSE(T#55_22, T#55_14) -> [T#55_23]
  Op#9 MUL(T#55_19, T#55_13) -> [T#55_24]
  Op#10 TRANSPOSE(T#55_24, T#55_9) -> [T#55_25]
  Op#11 RESHAPE(T#55_23, T#55_8) -> [T#55_26]
  Op#12 RESHAPE(T#55_25, T#55_7) -> [T#55_27]
  Op#13 BATCH_MATMUL(T#55_26, T#55_27) -> [T#55_28]
  Op#14 ADD(T#55_28, T#55_3) -> [T#55_29]
  Op#15 SOFTMAX(T#55_29) -> [T#55_30]
  Op#16 RESHAPE(T#55_30, T#55_6) -> [T#55_31]
  Op#17 RESHAPE(T#55_21, T#55_5) -> [T#55_32]
  Op#18 BATCH_MATMUL(T#55_31, T#55_32) -> [T#55_33]
  Op#19 RESHAPE(T#55_33, T#55_4) -> [T#55_34]
  Op#20 TRANSPOSE(T#55_34, T#55_14) -> [T#55_35]

Tensors of Subgraph#55
  T#55_0(odml.scaled_dot_product_attention.impl_52_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#55_1(odml.scaled_dot_product_attention.impl_52_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#55_2(odml.scaled_dot_product_attention.impl_52_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#55_3(odml.scaled_dot_product_attention.impl_52_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#55_4(arith.constant1030) shape:[4], type:INT32
  T#55_5(arith.constant1031) shape:[3], type:INT32
  T#55_6(arith.constant1032) shape:[3], type:INT32
  T#55_7(arith.constant1033) shape:[3], type:INT32
  T#55_8(arith.constant1034) shape:[3], type:INT32
  T#55_9(arith.constant1035) shape:[4], type:INT32
  T#55_10(arith.constant1036) shape:[4], type:INT32
  T#55_11(arith.constant1037) shape:[5], type:INT32
  T#55_12(arith.constant1038) shape:[5], type:INT64
  T#55_13(arith.constant1039) shape:[], type:FLOAT32
  T#55_14(arith.constant1040) shape:[4], type:INT32
  T#55_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#55_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#55_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#55_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#55_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#55_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#55_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#55_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#55_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#55_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#55_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#55_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#55_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#55_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#55_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#55_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#55_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#55_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#55_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#55_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#55_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_10/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

Subgraph#56 odml.scaled_dot_product_attention.impl_53(T#56_0, T#56_1, T#56_2, T#56_3) -> [T#56_35]
  Op#0 TRANSPOSE(T#56_1, T#56_14) -> [T#56_15]
  Op#1 TRANSPOSE(T#56_2, T#56_14) -> [T#56_16]
  Op#2 RESHAPE(T#56_15, T#56_11) -> [T#56_17]
  Op#3 BROADCAST_TO(T#56_17, T#56_12) -> [T#56_18]
  Op#4 RESHAPE(T#56_18, T#56_10) -> [T#56_19]
  Op#5 RESHAPE(T#56_16, T#56_11) -> [T#56_20]
  Op#6 BROADCAST_TO(T#56_20, T#56_12) -> [T#56_21]
  Op#7 MUL(T#56_0, T#56_13) -> [T#56_22]
  Op#8 TRANSPOSE(T#56_22, T#56_14) -> [T#56_23]
  Op#9 MUL(T#56_19, T#56_13) -> [T#56_24]
  Op#10 TRANSPOSE(T#56_24, T#56_9) -> [T#56_25]
  Op#11 RESHAPE(T#56_23, T#56_8) -> [T#56_26]
  Op#12 RESHAPE(T#56_25, T#56_7) -> [T#56_27]
  Op#13 BATCH_MATMUL(T#56_26, T#56_27) -> [T#56_28]
  Op#14 ADD(T#56_28, T#56_3) -> [T#56_29]
  Op#15 SOFTMAX(T#56_29) -> [T#56_30]
  Op#16 RESHAPE(T#56_30, T#56_6) -> [T#56_31]
  Op#17 RESHAPE(T#56_21, T#56_5) -> [T#56_32]
  Op#18 BATCH_MATMUL(T#56_31, T#56_32) -> [T#56_33]
  Op#19 RESHAPE(T#56_33, T#56_4) -> [T#56_34]
  Op#20 TRANSPOSE(T#56_34, T#56_14) -> [T#56_35]

Tensors of Subgraph#56
  T#56_0(odml.scaled_dot_product_attention.impl_53_arg0) shape:[1, 1024, 24, 128], type:FLOAT32
  T#56_1(odml.scaled_dot_product_attention.impl_53_arg1) shape:[1, 1280, 8, 128], type:FLOAT32
  T#56_2(odml.scaled_dot_product_attention.impl_53_arg2) shape:[1, 1280, 8, 128], type:FLOAT32
  T#56_3(odml.scaled_dot_product_attention.impl_53_arg3) shape:[1, 1, 1024, 1280], type:FLOAT32
  T#56_4(arith.constant1041) shape:[4], type:INT32
  T#56_5(arith.constant1042) shape:[3], type:INT32
  T#56_6(arith.constant1043) shape:[3], type:INT32
  T#56_7(arith.constant1044) shape:[3], type:INT32
  T#56_8(arith.constant1045) shape:[3], type:INT32
  T#56_9(arith.constant1046) shape:[4], type:INT32
  T#56_10(arith.constant1047) shape:[4], type:INT32
  T#56_11(arith.constant1048) shape:[5], type:INT32
  T#56_12(arith.constant1049) shape:[5], type:INT64
  T#56_13(arith.constant1050) shape:[], type:FLOAT32
  T#56_14(arith.constant1051) shape:[4], type:INT32
  T#56_15(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;56) shape:[1, 8, 1280, 128], type:FLOAT32
  T#56_16(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;57) shape:[1, 8, 1280, 128], type:FLOAT32
  T#56_17(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;58) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#56_18(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;59) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#56_19(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;60) shape:[1, 24, 1280, 128], type:FLOAT32
  T#56_20(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;61) shape:[1, 8, 1, 1280, 128], type:FLOAT32
  T#56_21(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;62) shape:[1, 8, 3, 1280, 128], type:FLOAT32
  T#56_22(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;63) shape:[1, 1024, 24, 128], type:FLOAT32
  T#56_23(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;64) shape:[1, 24, 1024, 128], type:FLOAT32
  T#56_24(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;65) shape:[1, 24, 1280, 128], type:FLOAT32
  T#56_25(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;66) shape:[1, 24, 128, 1280], type:FLOAT32
  T#56_26(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;67) shape:[24, 1024, 128], type:FLOAT32
  T#56_27(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;68) shape:[24, 128, 1280], type:FLOAT32
  T#56_28(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;69) shape:[24, 1024, 1280], type:FLOAT32
  T#56_29(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;70) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#56_30(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;71) shape:[1, 24, 1024, 1280], type:FLOAT32
  T#56_31(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;72) shape:[24, 1024, 1280], type:FLOAT32
  T#56_32(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;73) shape:[24, 1280, 128], type:FLOAT32
  T#56_33(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;74) shape:[24, 1024, 128], type:FLOAT32
  T#56_34(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;75) shape:[1, 24, 1024, 128], type:FLOAT32
  T#56_35(ai_edge_torch.generative.utilities.converter.ExportableModule/ai_edge_torch.generative.examples.llama.llama.Llama_module/ai_edge_torch.generative.layers.attention.TransformerBlock_11/ai_edge_torch.generative.layers.attention.CausalSelfAttention_atten_func;76) shape:[1, 1024, 24, 128], type:FLOAT32

---------------------------------------------------------------
Your TFLite model has '2' signature_def(s).

Signature#0 key: 'decode'
- Subgraph: Subgraph#0
- Inputs: 
    'input_pos' : T#5
    'kv_cache_k_0' : T#42
    'kv_cache_k_1' : T#48
    'kv_cache_k_10' : T#8
    'kv_cache_k_11' : T#52
    'kv_cache_k_12' : T#20
    'kv_cache_k_13' : T#35
    'kv_cache_k_14' : T#33
    'kv_cache_k_15' : T#45
    'kv_cache_k_16' : T#3
    'kv_cache_k_17' : T#10
    'kv_cache_k_18' : T#25
    'kv_cache_k_19' : T#53
    'kv_cache_k_2' : T#2
    'kv_cache_k_20' : T#15
    'kv_cache_k_21' : T#43
    'kv_cache_k_22' : T#13
    'kv_cache_k_23' : T#32
    'kv_cache_k_24' : T#9
    'kv_cache_k_25' : T#50
    'kv_cache_k_26' : T#26
    'kv_cache_k_27' : T#56
    'kv_cache_k_3' : T#18
    'kv_cache_k_4' : T#14
    'kv_cache_k_5' : T#54
    'kv_cache_k_6' : T#24
    'kv_cache_k_7' : T#28
    'kv_cache_k_8' : T#4
    'kv_cache_k_9' : T#16
    'kv_cache_v_0' : T#19
    'kv_cache_v_1' : T#38
    'kv_cache_v_10' : T#29
    'kv_cache_v_11' : T#22
    'kv_cache_v_12' : T#31
    'kv_cache_v_13' : T#41
    'kv_cache_v_14' : T#57
    'kv_cache_v_15' : T#34
    'kv_cache_v_16' : T#30
    'kv_cache_v_17' : T#40
    'kv_cache_v_18' : T#21
    'kv_cache_v_19' : T#27
    'kv_cache_v_2' : T#47
    'kv_cache_v_20' : T#55
    'kv_cache_v_21' : T#37
    'kv_cache_v_22' : T#7
    'kv_cache_v_23' : T#17
    'kv_cache_v_24' : T#44
    'kv_cache_v_25' : T#6
    'kv_cache_v_26' : T#0
    'kv_cache_v_27' : T#12
    'kv_cache_v_3' : T#1
    'kv_cache_v_4' : T#51
    'kv_cache_v_5' : T#23
    'kv_cache_v_6' : T#11
    'kv_cache_v_7' : T#36
    'kv_cache_v_8' : T#49
    'kv_cache_v_9' : T#46
    'tokens' : T#39
- Outputs: 
    'kv_cache_k_0' : T#340
    'kv_cache_k_1' : T#394
    'kv_cache_k_10' : T#880
    'kv_cache_k_11' : T#934
    'kv_cache_k_12' : T#988
    'kv_cache_k_13' : T#1042
    'kv_cache_k_14' : T#1096
    'kv_cache_k_15' : T#1150
    'kv_cache_k_16' : T#1204
    'kv_cache_k_17' : T#1258
    'kv_cache_k_18' : T#1312
    'kv_cache_k_19' : T#1366
    'kv_cache_k_2' : T#448
    'kv_cache_k_20' : T#1420
    'kv_cache_k_21' : T#1474
    'kv_cache_k_22' : T#1528
    'kv_cache_k_23' : T#1582
    'kv_cache_k_24' : T#1636
    'kv_cache_k_25' : T#1690
    'kv_cache_k_26' : T#1744
    'kv_cache_k_27' : T#1798
    'kv_cache_k_3' : T#502
    'kv_cache_k_4' : T#556
    'kv_cache_k_5' : T#610
    'kv_cache_k_6' : T#664
    'kv_cache_k_7' : T#718
    'kv_cache_k_8' : T#772
    'kv_cache_k_9' : T#826
    'kv_cache_v_0' : T#341
    'kv_cache_v_1' : T#395
    'kv_cache_v_10' : T#881
    'kv_cache_v_11' : T#935
    'kv_cache_v_12' : T#989
    'kv_cache_v_13' : T#1043
    'kv_cache_v_14' : T#1097
    'kv_cache_v_15' : T#1151
    'kv_cache_v_16' : T#1205
    'kv_cache_v_17' : T#1259
    'kv_cache_v_18' : T#1313
    'kv_cache_v_19' : T#1367
    'kv_cache_v_2' : T#449
    'kv_cache_v_20' : T#1421
    'kv_cache_v_21' : T#1475
    'kv_cache_v_22' : T#1529
    'kv_cache_v_23' : T#1583
    'kv_cache_v_24' : T#1637
    'kv_cache_v_25' : T#1691
    'kv_cache_v_26' : T#1745
    'kv_cache_v_27' : T#1799
    'kv_cache_v_3' : T#503
    'kv_cache_v_4' : T#557
    'kv_cache_v_5' : T#611
    'kv_cache_v_6' : T#665
    'kv_cache_v_7' : T#719
    'kv_cache_v_8' : T#773
    'kv_cache_v_9' : T#827
    'logits' : T#1825

Signature#1 key: 'prefill_1024'
- Subgraph: Subgraph#1
- Inputs: 
    'input_pos' : T#1_5
    'kv_cache_k_0' : T#1_42
    'kv_cache_k_1' : T#1_48
    'kv_cache_k_10' : T#1_8
    'kv_cache_k_11' : T#1_52
    'kv_cache_k_12' : T#1_20
    'kv_cache_k_13' : T#1_35
    'kv_cache_k_14' : T#1_33
    'kv_cache_k_15' : T#1_45
    'kv_cache_k_16' : T#1_3
    'kv_cache_k_17' : T#1_10
    'kv_cache_k_18' : T#1_25
    'kv_cache_k_19' : T#1_53
    'kv_cache_k_2' : T#1_2
    'kv_cache_k_20' : T#1_15
    'kv_cache_k_21' : T#1_43
    'kv_cache_k_22' : T#1_13
    'kv_cache_k_23' : T#1_32
    'kv_cache_k_24' : T#1_9
    'kv_cache_k_25' : T#1_50
    'kv_cache_k_26' : T#1_26
    'kv_cache_k_27' : T#1_56
    'kv_cache_k_3' : T#1_18
    'kv_cache_k_4' : T#1_14
    'kv_cache_k_5' : T#1_54
    'kv_cache_k_6' : T#1_24
    'kv_cache_k_7' : T#1_28
    'kv_cache_k_8' : T#1_4
    'kv_cache_k_9' : T#1_16
    'kv_cache_v_0' : T#1_19
    'kv_cache_v_1' : T#1_38
    'kv_cache_v_10' : T#1_29
    'kv_cache_v_11' : T#1_22
    'kv_cache_v_12' : T#1_31
    'kv_cache_v_13' : T#1_41
    'kv_cache_v_14' : T#1_57
    'kv_cache_v_15' : T#1_34
    'kv_cache_v_16' : T#1_30
    'kv_cache_v_17' : T#1_40
    'kv_cache_v_18' : T#1_21
    'kv_cache_v_19' : T#1_27
    'kv_cache_v_2' : T#1_47
    'kv_cache_v_20' : T#1_55
    'kv_cache_v_21' : T#1_37
    'kv_cache_v_22' : T#1_7
    'kv_cache_v_23' : T#1_17
    'kv_cache_v_24' : T#1_44
    'kv_cache_v_25' : T#1_6
    'kv_cache_v_26' : T#1_0
    'kv_cache_v_27' : T#1_12
    'kv_cache_v_3' : T#1_1
    'kv_cache_v_4' : T#1_51
    'kv_cache_v_5' : T#1_23
    'kv_cache_v_6' : T#1_11
    'kv_cache_v_7' : T#1_36
    'kv_cache_v_8' : T#1_49
    'kv_cache_v_9' : T#1_46
    'tokens' : T#1_39
- Outputs: 
    'kv_cache_k_0' : T#1_340
    'kv_cache_k_1' : T#1_397
    'kv_cache_k_10' : T#1_910
    'kv_cache_k_11' : T#1_967
    'kv_cache_k_12' : T#1_1024
    'kv_cache_k_13' : T#1_1081
    'kv_cache_k_14' : T#1_1138
    'kv_cache_k_15' : T#1_1195
    'kv_cache_k_16' : T#1_1252
    'kv_cache_k_17' : T#1_1309
    'kv_cache_k_18' : T#1_1366
    'kv_cache_k_19' : T#1_1423
    'kv_cache_k_2' : T#1_454
    'kv_cache_k_20' : T#1_1480
    'kv_cache_k_21' : T#1_1537
    'kv_cache_k_22' : T#1_1594
    'kv_cache_k_23' : T#1_1651
    'kv_cache_k_24' : T#1_1708
    'kv_cache_k_25' : T#1_1765
    'kv_cache_k_26' : T#1_1822
    'kv_cache_k_27' : T#1_1866
    'kv_cache_k_3' : T#1_511
    'kv_cache_k_4' : T#1_568
    'kv_cache_k_5' : T#1_625
    'kv_cache_k_6' : T#1_682
    'kv_cache_k_7' : T#1_739
    'kv_cache_k_8' : T#1_796
    'kv_cache_k_9' : T#1_853
    'kv_cache_v_0' : T#1_341
    'kv_cache_v_1' : T#1_398
    'kv_cache_v_10' : T#1_911
    'kv_cache_v_11' : T#1_968
    'kv_cache_v_12' : T#1_1025
    'kv_cache_v_13' : T#1_1082
    'kv_cache_v_14' : T#1_1139
    'kv_cache_v_15' : T#1_1196
    'kv_cache_v_16' : T#1_1253
    'kv_cache_v_17' : T#1_1310
    'kv_cache_v_18' : T#1_1367
    'kv_cache_v_19' : T#1_1424
    'kv_cache_v_2' : T#1_455
    'kv_cache_v_20' : T#1_1481
    'kv_cache_v_21' : T#1_1538
    'kv_cache_v_22' : T#1_1595
    'kv_cache_v_23' : T#1_1652
    'kv_cache_v_24' : T#1_1709
    'kv_cache_v_25' : T#1_1766
    'kv_cache_v_26' : T#1_1823
    'kv_cache_v_27' : T#1_1867
    'kv_cache_v_3' : T#1_512
    'kv_cache_v_4' : T#1_569
    'kv_cache_v_5' : T#1_626
    'kv_cache_v_6' : T#1_683
    'kv_cache_v_7' : T#1_740
    'kv_cache_v_8' : T#1_797
    'kv_cache_v_9' : T#1_854

---------------------------------------------------------------
              Model size: 12859360704 bytes
    Non-data buffer size: 12859360669 bytes (100.00 %)
  Total data buffer size:         35 bytes (00.00 %)
          - Subgraph#0  :          0 bytes (00.00 %)
          - Subgraph#1  :          0 bytes (00.00 %)
          - Subgraph#2  :          0 bytes (00.00 %)
          - Subgraph#3  :          0 bytes (00.00 %)
          - Subgraph#4  :          0 bytes (00.00 %)
          - Subgraph#5  :          0 bytes (00.00 %)
          - Subgraph#6  :          0 bytes (00.00 %)
          - Subgraph#7  :          0 bytes (00.00 %)
          - Subgraph#8  :          0 bytes (00.00 %)
          - Subgraph#9  :          0 bytes (00.00 %)
          - Subgraph#10 :          0 bytes (00.00 %)
          - Subgraph#11 :          0 bytes (00.00 %)
          - Subgraph#12 :          0 bytes (00.00 %)
          - Subgraph#13 :          0 bytes (00.00 %)
          - Subgraph#14 :          0 bytes (00.00 %)
          - Subgraph#15 :          0 bytes (00.00 %)
          - Subgraph#16 :          0 bytes (00.00 %)
          - Subgraph#17 :          0 bytes (00.00 %)
          - Subgraph#18 :          0 bytes (00.00 %)
          - Subgraph#19 :          0 bytes (00.00 %)
          - Subgraph#20 :          0 bytes (00.00 %)
          - Subgraph#21 :          0 bytes (00.00 %)
          - Subgraph#22 :          0 bytes (00.00 %)
          - Subgraph#23 :          0 bytes (00.00 %)
          - Subgraph#24 :          0 bytes (00.00 %)
          - Subgraph#25 :          0 bytes (00.00 %)
          - Subgraph#26 :          0 bytes (00.00 %)
          - Subgraph#27 :          0 bytes (00.00 %)
          - Subgraph#28 :          0 bytes (00.00 %)
          - Subgraph#29 :          0 bytes (00.00 %)
          - Subgraph#30 :          0 bytes (00.00 %)
          - Subgraph#31 :          0 bytes (00.00 %)
          - Subgraph#32 :          0 bytes (00.00 %)
          - Subgraph#33 :          0 bytes (00.00 %)
          - Subgraph#34 :          0 bytes (00.00 %)
          - Subgraph#35 :          0 bytes (00.00 %)
          - Subgraph#36 :          0 bytes (00.00 %)
          - Subgraph#37 :          0 bytes (00.00 %)
          - Subgraph#38 :          0 bytes (00.00 %)
          - Subgraph#39 :          0 bytes (00.00 %)
          - Subgraph#40 :          0 bytes (00.00 %)
          - Subgraph#41 :          0 bytes (00.00 %)
          - Subgraph#42 :          0 bytes (00.00 %)
          - Subgraph#43 :          0 bytes (00.00 %)
          - Subgraph#44 :          0 bytes (00.00 %)
          - Subgraph#45 :          0 bytes (00.00 %)
          - Subgraph#46 :          0 bytes (00.00 %)
          - Subgraph#47 :          0 bytes (00.00 %)
          - Subgraph#48 :          0 bytes (00.00 %)
          - Subgraph#49 :          0 bytes (00.00 %)
          - Subgraph#50 :          0 bytes (00.00 %)
          - Subgraph#51 :          0 bytes (00.00 %)
          - Subgraph#52 :          0 bytes (00.00 %)
          - Subgraph#53 :          0 bytes (00.00 %)
          - Subgraph#54 :          0 bytes (00.00 %)
          - Subgraph#55 :          0 bytes (00.00 %)
          - Subgraph#56 :          0 bytes (00.00 %)
    (Zero value buffers):          0 bytes (00.00 %)

* Buffers of TFLite model are mostly used for constant tensors.
  And zero value buffers are buffers filled with zeros.
  Non-data buffers area are used to store operators, subgraphs and etc.
  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs

